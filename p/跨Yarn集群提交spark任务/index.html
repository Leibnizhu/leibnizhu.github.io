<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="背景 之前写过一篇 Spark动态加载hive配置的方案 ，当时是为了spark应用的fat-jar里面已经有Hadoop相关xml配置文件的情况下，将数据输出到不是该配置的Hadoop集群的方案。
现在这个需求有点类似，没有走spark-submit提交任务，而是在spark应用里面通过创建SparkContext的形式提交任务，而spark应用的fat-jar里面已经有Hadoop相关xml配置文件，在此情况下，想将Spakr任务提交到外部的Yarn集群（不是fat-jar里面配置文件对应的yarn集群）。
思考一个问题 先思考一个问题，如果Spark应用的fat-jar里面有外部Yarn集群对应的配置文件(core-site.xml，hdfs-site.xml，yarn-site.xml等)，此时Spark应用代码里面创建SparkContext，是不是就一定能提交到那个集群里？
可以做个实验，但实验不一定会cover到所有情况。
直接给结论吧，不一定能提交过去，但自己做实验的话很可能还是能直接提交过去的，还是直接看代码吧（以yarn-client模式为例）。
Spark Yarn-client 默认提交任务简析 通过代码创建SparkContext后，其动态代码块会根据启动模式创建SchedulerBackend和TaskScheduler并启动：
// org.apache.spark.SparkContext #501  // Create and start the scheduler  val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)  _schedulerBackend = sched  _taskScheduler = ts  _dagScheduler = new DAGScheduler(this)  // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&amp;#39;s  // constructor  _taskScheduler.start() 其中 TaskScheduler 是通过 org.apache.spark.scheduler.cluster.YarnClusterManager#createTaskScheduler 创建的，对应 yarn-client 创建的是YarnScheduler（继承了TaskSchedulerImpl），start()方法调用到SchedulerBackend的start方法，后者就会创建yarn模式下的Client客户端（org.apache.spark.deploy.yarn.Client，不是yarn自己那个client），并调用其submitApplication方法提交任务到Yarn：
//org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend#start  override def start() {  val driverHost = conf."><title>跨Yarn集群提交spark任务</title><link rel=canonical href=https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/><link rel=stylesheet href=/scss/style.min.f6e1f9fc3ff66a58601c08d5ce96493eafb5f1b936be2f7b2eacd3e7784f2f38.css><meta property="og:title" content="跨Yarn集群提交spark任务"><meta property="og:description" content="背景 之前写过一篇 Spark动态加载hive配置的方案 ，当时是为了spark应用的fat-jar里面已经有Hadoop相关xml配置文件的情况下，将数据输出到不是该配置的Hadoop集群的方案。
现在这个需求有点类似，没有走spark-submit提交任务，而是在spark应用里面通过创建SparkContext的形式提交任务，而spark应用的fat-jar里面已经有Hadoop相关xml配置文件，在此情况下，想将Spakr任务提交到外部的Yarn集群（不是fat-jar里面配置文件对应的yarn集群）。
思考一个问题 先思考一个问题，如果Spark应用的fat-jar里面有外部Yarn集群对应的配置文件(core-site.xml，hdfs-site.xml，yarn-site.xml等)，此时Spark应用代码里面创建SparkContext，是不是就一定能提交到那个集群里？
可以做个实验，但实验不一定会cover到所有情况。
直接给结论吧，不一定能提交过去，但自己做实验的话很可能还是能直接提交过去的，还是直接看代码吧（以yarn-client模式为例）。
Spark Yarn-client 默认提交任务简析 通过代码创建SparkContext后，其动态代码块会根据启动模式创建SchedulerBackend和TaskScheduler并启动：
// org.apache.spark.SparkContext #501  // Create and start the scheduler  val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)  _schedulerBackend = sched  _taskScheduler = ts  _dagScheduler = new DAGScheduler(this)  // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&amp;#39;s  // constructor  _taskScheduler.start() 其中 TaskScheduler 是通过 org.apache.spark.scheduler.cluster.YarnClusterManager#createTaskScheduler 创建的，对应 yarn-client 创建的是YarnScheduler（继承了TaskSchedulerImpl），start()方法调用到SchedulerBackend的start方法，后者就会创建yarn模式下的Client客户端（org.apache.spark.deploy.yarn.Client，不是yarn自己那个client），并调用其submitApplication方法提交任务到Yarn：
//org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend#start  override def start() {  val driverHost = conf."><meta property="og:url" content="https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/"><meta property="og:site_name" content="Heaven's Door"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Spark"><meta property="article:tag" content="Hadoop"><meta property="article:tag" content="Yarn"><meta property="article:tag" content="跨集群"><meta property="article:published_time" content="2021-12-04T18:17:55+08:00"><meta property="article:modified_time" content="2021-12-04T18:17:55+08:00"><meta property="og:image" content="https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/93990522.jpg"><meta name=twitter:title content="跨Yarn集群提交spark任务"><meta name=twitter:description content="背景 之前写过一篇 Spark动态加载hive配置的方案 ，当时是为了spark应用的fat-jar里面已经有Hadoop相关xml配置文件的情况下，将数据输出到不是该配置的Hadoop集群的方案。
现在这个需求有点类似，没有走spark-submit提交任务，而是在spark应用里面通过创建SparkContext的形式提交任务，而spark应用的fat-jar里面已经有Hadoop相关xml配置文件，在此情况下，想将Spakr任务提交到外部的Yarn集群（不是fat-jar里面配置文件对应的yarn集群）。
思考一个问题 先思考一个问题，如果Spark应用的fat-jar里面有外部Yarn集群对应的配置文件(core-site.xml，hdfs-site.xml，yarn-site.xml等)，此时Spark应用代码里面创建SparkContext，是不是就一定能提交到那个集群里？
可以做个实验，但实验不一定会cover到所有情况。
直接给结论吧，不一定能提交过去，但自己做实验的话很可能还是能直接提交过去的，还是直接看代码吧（以yarn-client模式为例）。
Spark Yarn-client 默认提交任务简析 通过代码创建SparkContext后，其动态代码块会根据启动模式创建SchedulerBackend和TaskScheduler并启动：
// org.apache.spark.SparkContext #501  // Create and start the scheduler  val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)  _schedulerBackend = sched  _taskScheduler = ts  _dagScheduler = new DAGScheduler(this)  // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&amp;#39;s  // constructor  _taskScheduler.start() 其中 TaskScheduler 是通过 org.apache.spark.scheduler.cluster.YarnClusterManager#createTaskScheduler 创建的，对应 yarn-client 创建的是YarnScheduler（继承了TaskSchedulerImpl），start()方法调用到SchedulerBackend的start方法，后者就会创建yarn模式下的Client客户端（org.apache.spark.deploy.yarn.Client，不是yarn自己那个client），并调用其submitApplication方法提交任务到Yarn：
//org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend#start  override def start() {  val driverHost = conf."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/93990522.jpg"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu4af873b90feb2f44d14cf5fcf42d034c_7593_300x0_resize_q75_box.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Heaven's Door</a></h1><h2 class=site-description>That cold black cloud is comin' down, Feels like I'm knockin' on heaven's door…</h2></div></header><ol class=social-menu><li><a href=https://leibnizhu.github.io title=Home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg></a></li><li><a href=https://github.com/leibnizhu target=_blank title=GitHub><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:leibnizhu@gmail.com target=_blank title=Email><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-gmail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M16 20h3a1 1 0 001-1V5a1 1 0 00-1-1h-3v16z"/><path d="M5 20h3V4H5A1 1 0 004 5v14a1 1 0 001 1z"/><path d="M16 4l-4 4-4-4"/><path d="M4 6.5l8 7.5 8-7.5"/></svg></a></li><li><a href=https://leibnizhu.github.io/running target=_blank title=Running><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-run" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="13" cy="4" r="1"/><path d="M4 17l5 1 .75-1.5"/><path d="M15 21v-4l-4-3 1-6"/><path d="M7 12V9l5-1 3 3 3 1"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/About/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg><span>About</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/Links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/><img src=/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/93990522_hu6aa0acda77cb3b4cc5d51f0a61452bcb_184592_800x0_resize_q75_box.jpg srcset="/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/93990522_hu6aa0acda77cb3b4cc5d51f0a61452bcb_184592_800x0_resize_q75_box.jpg 800w, /p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/93990522_hu6aa0acda77cb3b4cc5d51f0a61452bcb_184592_1600x0_resize_q75_box.jpg 1600w" width=800 height=304 loading=lazy alt="Featured image of post 跨Yarn集群提交spark任务"></a></div><div class=article-details><header class=article-category><a href=/categories/Spark/ style=background-color:#2a9d8f;color:#fff>Spark</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/>跨Yarn集群提交spark任务</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Dec 04, 2021</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><h2 id=背景>背景</h2><p>之前写过一篇 <a class=link href=/2020/05/06/%e5%8a%a8%e6%80%81%e5%8a%a0%e8%bd%bdhive%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e7%9a%84%e6%96%b9%e6%a1%88/>Spark动态加载hive配置的方案</a> ，当时是为了spark应用的fat-jar里面已经有Hadoop相关xml配置文件的情况下，将数据输出到不是该配置的Hadoop集群的方案。<br>现在这个需求有点类似，没有走spark-submit提交任务，而是在spark应用里面通过创建<code>SparkContext</code>的形式提交任务，而spark应用的fat-jar里面已经有Hadoop相关xml配置文件，在此情况下，想将Spakr任务提交到外部的Yarn集群（不是fat-jar里面配置文件对应的yarn集群）。</p><h2 id=思考一个问题>思考一个问题</h2><p>先思考一个问题，如果Spark应用的fat-jar里面有外部Yarn集群对应的配置文件(<code>core-site.xml</code>，<code>hdfs-site.xml</code>，<code>yarn-site.xml</code>等)，此时Spark应用代码里面创建<code>SparkContext</code>，是不是就一定能提交到那个集群里？<br>可以做个实验，但实验不一定会cover到所有情况。<br>直接给结论吧，不一定能提交过去，但自己做实验的话很可能还是能直接提交过去的，还是直接看代码吧（以<code>yarn-client</code>模式为例）。</p><h3 id=spark-yarn-client-默认提交任务简析>Spark Yarn-client 默认提交任务简析</h3><p>通过代码创建<code>SparkContext</code>后，其动态代码块会根据启动模式创建<code>SchedulerBackend</code>和<code>TaskScheduler</code>并启动：</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#776e71>// org.apache.spark.SparkContext #501
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// Create and start the scheduler
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#815ba4>val</span> <span style=color:#5bc4bf>(</span>sched<span style=color:#5bc4bf>,</span> ts<span style=color:#5bc4bf>)</span> <span style=color:#815ba4>=</span> <span style=color:#fec418>SparkContext</span><span style=color:#5bc4bf>.</span>createTaskScheduler<span style=color:#5bc4bf>(</span><span style=color:#815ba4>this</span><span style=color:#5bc4bf>,</span> master<span style=color:#5bc4bf>,</span> deployMode<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    <span style=color:#fec418>_schedulerBackend</span> <span style=color:#815ba4>=</span> sched
</span></span><span style=display:flex><span>    <span style=color:#fec418>_taskScheduler</span> <span style=color:#815ba4>=</span> ts
</span></span><span style=display:flex><span>    <span style=color:#fec418>_dagScheduler</span> <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>DAGScheduler</span><span style=color:#5bc4bf>(</span><span style=color:#815ba4>this</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    <span style=color:#776e71>// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&#39;s
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// constructor
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#fec418>_taskScheduler</span><span style=color:#5bc4bf>.</span>start<span style=color:#5bc4bf>()</span>
</span></span></code></pre></div><p>其中 <code>TaskScheduler</code> 是通过 <code>org.apache.spark.scheduler.cluster.YarnClusterManager#createTaskScheduler</code> 创建的，对应 yarn-client 创建的是<code>YarnScheduler</code>（继承了<code>TaskSchedulerImpl</code>），start()方法调用到<code>SchedulerBackend</code>的<code>start</code>方法，后者就会创建yarn模式下的Client客户端（<code>org.apache.spark.deploy.yarn.Client</code>，不是yarn自己那个client），并调用其<code>submitApplication</code>方法提交任务到Yarn：</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#776e71>//org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend#start
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>  <span style=color:#815ba4>override</span> <span style=color:#815ba4>def</span> start<span style=color:#5bc4bf>()</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> driverHost <span style=color:#815ba4>=</span> conf<span style=color:#5bc4bf>.</span>get<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;spark.driver.host&#34;</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> driverPort <span style=color:#815ba4>=</span> conf<span style=color:#5bc4bf>.</span>get<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;spark.driver.port&#34;</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> hostport <span style=color:#815ba4>=</span> driverHost <span style=color:#5bc4bf>+</span> <span style=color:#48b685>&#34;:&#34;</span> <span style=color:#5bc4bf>+</span> driverPort
</span></span><span style=display:flex><span>    sc<span style=color:#5bc4bf>.</span>ui<span style=color:#5bc4bf>.</span>foreach <span style=color:#5bc4bf>{</span> ui <span style=color:#815ba4>=&gt;</span> conf<span style=color:#5bc4bf>.</span>set<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;spark.driver.appUIAddress&#34;</span><span style=color:#5bc4bf>,</span> ui<span style=color:#5bc4bf>.</span>webUrl<span style=color:#5bc4bf>)</span> <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> argsArrayBuf <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>ArrayBuffer</span><span style=color:#5bc4bf>[</span><span style=color:#fec418>String</span><span style=color:#5bc4bf>]()</span>
</span></span><span style=display:flex><span>    argsArrayBuf <span style=color:#5bc4bf>+=</span> <span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;--arg&#34;</span><span style=color:#5bc4bf>,</span> hostport<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logDebug<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;ClientArguments called with: &#34;</span> <span style=color:#5bc4bf>+</span> argsArrayBuf<span style=color:#5bc4bf>.</span>mkString<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34; &#34;</span><span style=color:#5bc4bf>))</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> args <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>ClientArguments</span><span style=color:#5bc4bf>(</span>argsArrayBuf<span style=color:#5bc4bf>.</span>toArray<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    totalExpectedExecutors <span style=color:#815ba4>=</span> <span style=color:#fec418>YarnSparkHadoopUtil</span><span style=color:#5bc4bf>.</span>getInitialTargetExecutorNumber<span style=color:#5bc4bf>(</span>conf<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    client <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>Client</span><span style=color:#5bc4bf>(</span>args<span style=color:#5bc4bf>,</span> conf<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    bindToYarn<span style=color:#5bc4bf>(</span>client<span style=color:#5bc4bf>.</span>submitApplication<span style=color:#5bc4bf>(),</span> <span style=color:#fec418>None</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    <span style=color:#776e71>//………………
</span></span></span><span style=display:flex><span><span style=color:#776e71></span><span style=color:#5bc4bf>}</span>
</span></span></code></pre></div><p>初始化<code>Client</code>的时候，会创建YarnConfiguration，此时就会读取到Configuration里面配置的默认资源，包括<code>yarn-site.xml</code>等；如果fatjar里面放的是外部集群的配置文件，那么对应的<code>YarnClient</code>就可以连接到外部Yarn集群。</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#776e71>//org.apache.spark.deploy.yarn.Client
</span></span></span><span style=display:flex><span><span style=color:#776e71></span><span style=color:#815ba4>private</span><span style=color:#5bc4bf>[</span><span style=color:#fec418>spark</span><span style=color:#5bc4bf>]</span> <span style=color:#815ba4>class</span> <span style=color:#fec418>Client</span><span style=color:#5bc4bf>(</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> args<span style=color:#815ba4>:</span> <span style=color:#fec418>ClientArguments</span><span style=color:#5bc4bf>,</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> hadoopConf<span style=color:#815ba4>:</span> <span style=color:#fec418>Configuration</span><span style=color:#5bc4bf>,</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> sparkConf<span style=color:#815ba4>:</span> <span style=color:#fec418>SparkConf</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>  <span style=color:#815ba4>extends</span> <span style=color:#fec418>Logging</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>    <span style=color:#776e71>//………………
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>  <span style=color:#815ba4>private</span> <span style=color:#815ba4>val</span> yarnClient <span style=color:#815ba4>=</span> <span style=color:#fec418>YarnClient</span><span style=color:#5bc4bf>.</span>createYarnClient
</span></span><span style=display:flex><span>  <span style=color:#815ba4>private</span> <span style=color:#815ba4>val</span> yarnConf <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>YarnConfiguration</span><span style=color:#5bc4bf>(</span>hadoopConf<span style=color:#5bc4bf>)</span>
</span></span></code></pre></div><p>接着刚才说到<code>SchedulerBackend</code>调用<code>Client</code>的<code>submitApplication</code>方法:</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#776e71>//org.apache.spark.deploy.yarn.Client#submitApplication
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>  <span style=color:#815ba4>def</span> submitApplication<span style=color:#5bc4bf>()</span><span style=color:#815ba4>:</span> <span style=color:#fec418>ApplicationId</span> <span style=color:#5bc4bf>=</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>var</span> appId<span style=color:#815ba4>:</span> <span style=color:#fec418>ApplicationId</span> <span style=color:#5bc4bf>=</span> <span style=color:#815ba4>null</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>try</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>      launcherBackend<span style=color:#5bc4bf>.</span>connect<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>// Setup the credentials before doing anything else,
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      <span style=color:#776e71>// so we have don&#39;t have issues at any point.
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      setupCredentials<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>      yarnClient<span style=color:#5bc4bf>.</span>init<span style=color:#5bc4bf>(</span>yarnConf<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>      yarnClient<span style=color:#5bc4bf>.</span>start<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      logInfo<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;Requesting a new application from cluster with %d NodeManagers&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#5bc4bf>.</span>format<span style=color:#5bc4bf>(</span>yarnClient<span style=color:#5bc4bf>.</span>getYarnClusterMetrics<span style=color:#5bc4bf>.</span>getNumNodeManagers<span style=color:#5bc4bf>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>// Get a new application from our RM
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      <span style=color:#776e71>//新建一个Application
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      <span style=color:#815ba4>val</span> newApp <span style=color:#815ba4>=</span> yarnClient<span style=color:#5bc4bf>.</span>createApplication<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>      <span style=color:#815ba4>val</span> newAppResponse <span style=color:#815ba4>=</span> newApp<span style=color:#5bc4bf>.</span>getNewApplicationResponse<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>      appId <span style=color:#815ba4>=</span> newAppResponse<span style=color:#5bc4bf>.</span>getApplicationId<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#815ba4>new</span> <span style=color:#fec418>CallerContext</span><span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;CLIENT&#34;</span><span style=color:#5bc4bf>,</span> sparkConf<span style=color:#5bc4bf>.</span>get<span style=color:#5bc4bf>(</span><span style=color:#fec418>APP_CALLER_CONTEXT</span><span style=color:#5bc4bf>),</span>
</span></span><span style=display:flex><span>        <span style=color:#fec418>Option</span><span style=color:#5bc4bf>(</span>appId<span style=color:#5bc4bf>.</span>toString<span style=color:#5bc4bf>)).</span>setCurrentContext<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>// Verify whether the cluster has enough resources for our AM
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      verifyClusterResources<span style=color:#5bc4bf>(</span>newAppResponse<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>// Set up the appropriate contexts to launch our AM
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      <span style=color:#776e71>//创建environment, java options以及启动AM的命令
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      <span style=color:#815ba4>val</span> containerContext <span style=color:#815ba4>=</span> createContainerLaunchContext<span style=color:#5bc4bf>(</span>newAppResponse<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>//创建提交AM的Context，包括名字、队列、类型、内存、CPU及参数
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      <span style=color:#815ba4>val</span> appContext <span style=color:#815ba4>=</span> createApplicationSubmissionContext<span style=color:#5bc4bf>(</span>newApp<span style=color:#5bc4bf>,</span> containerContext<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>// Finally, submit and monitor the application
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      logInfo<span style=color:#5bc4bf>(</span><span style=color:#48b685>s&#34;Submitting application </span><span style=color:#f99b15>$appId</span><span style=color:#48b685> to ResourceManager&#34;</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>      <span style=color:#776e71>//向Yarn提交Application
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>      yarnClient<span style=color:#5bc4bf>.</span>submitApplication<span style=color:#5bc4bf>(</span>appContext<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>      launcherBackend<span style=color:#5bc4bf>.</span>setAppId<span style=color:#5bc4bf>(</span>appId<span style=color:#5bc4bf>.</span>toString<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>      reportLauncherState<span style=color:#5bc4bf>(</span><span style=color:#fec418>SparkAppHandle</span><span style=color:#5bc4bf>.</span><span style=color:#fec418>State</span><span style=color:#5bc4bf>.</span><span style=color:#fec418>SUBMITTED</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      appId
</span></span><span style=display:flex><span>    <span style=color:#5bc4bf>}</span> <span style=color:#815ba4>catch</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>      <span style=color:#815ba4>case</span> e<span style=color:#815ba4>:</span> <span style=color:#fec418>Throwable</span> <span style=color:#5bc4bf>=&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#815ba4>if</span> <span style=color:#5bc4bf>(</span>appId <span style=color:#5bc4bf>!=</span> <span style=color:#815ba4>null</span><span style=color:#5bc4bf>)</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>          cleanupStagingDir<span style=color:#5bc4bf>(</span>appId<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>        <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>        <span style=color:#815ba4>throw</span> e
</span></span><span style=display:flex><span>    <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>  <span style=color:#5bc4bf>}</span>
</span></span></code></pre></div><p>其中 createContainerLaunchContext 会创建environment, java options以及启动AM的命令等，也会收集本地资源（<code>prepareLocalResources</code>方法），其中包括<code>__spark_conf__.zip</code>，在<code>createConfArchive</code>方法中处理，压缩了本地的一些配置文件：</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>  <span style=color:#815ba4>private</span> <span style=color:#815ba4>def</span> createConfArchive<span style=color:#5bc4bf>()</span><span style=color:#815ba4>:</span> <span style=color:#fec418>File</span> <span style=color:#5bc4bf>=</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> hadoopConfFiles <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>HashMap</span><span style=color:#5bc4bf>[</span><span style=color:#fec418>String</span>, <span style=color:#fec418>File</span><span style=color:#5bc4bf>]()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#776e71>// Uploading $SPARK_CONF_DIR/log4j.properties file to the distributed cache to make sure that
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// the executors will use the latest configurations instead of the default values. This is
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// required when user changes log4j.properties directly to set the log configurations. If
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// configuration file is provided through --files then executors will be taking configurations
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// from --files instead of $SPARK_CONF_DIR/log4j.properties.
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>
</span></span><span style=display:flex><span>    <span style=color:#776e71>// Also uploading metrics.properties to distributed cache if exists in classpath.
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// If user specify this file using --files then executors will use the one
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#776e71>// from --files instead.
</span></span></span><span style=display:flex><span><span style=color:#776e71></span>    <span style=color:#815ba4>for</span> <span style=color:#5bc4bf>{</span> prop <span style=color:#815ba4>&lt;-</span> <span style=color:#fec418>Seq</span><span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;log4j.properties&#34;</span><span style=color:#5bc4bf>,</span> <span style=color:#48b685>&#34;metrics.properties&#34;</span><span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>          url <span style=color:#815ba4>&lt;-</span> <span style=color:#fec418>Option</span><span style=color:#5bc4bf>(</span><span style=color:#fec418>Utils</span><span style=color:#5bc4bf>.</span>getContextOrSparkClassLoader<span style=color:#5bc4bf>.</span>getResource<span style=color:#5bc4bf>(</span>prop<span style=color:#5bc4bf>))</span>
</span></span><span style=display:flex><span>          <span style=color:#815ba4>if</span> url<span style=color:#5bc4bf>.</span>getProtocol <span style=color:#5bc4bf>==</span> <span style=color:#48b685>&#34;file&#34;</span> <span style=color:#5bc4bf>}</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>      hadoopConfFiles<span style=color:#5bc4bf>(</span>prop<span style=color:#5bc4bf>)</span> <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>File</span><span style=color:#5bc4bf>(</span>url<span style=color:#5bc4bf>.</span>getPath<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>    <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#fec418>Seq</span><span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;HADOOP_CONF_DIR&#34;</span><span style=color:#5bc4bf>,</span> <span style=color:#48b685>&#34;YARN_CONF_DIR&#34;</span><span style=color:#5bc4bf>).</span>foreach <span style=color:#5bc4bf>{</span> envKey <span style=color:#815ba4>=&gt;</span>
</span></span><span style=display:flex><span>      sys<span style=color:#5bc4bf>.</span>env<span style=color:#5bc4bf>.</span>get<span style=color:#5bc4bf>(</span>envKey<span style=color:#5bc4bf>).</span>foreach <span style=color:#5bc4bf>{</span> path <span style=color:#815ba4>=&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#815ba4>val</span> dir <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>File</span><span style=color:#5bc4bf>(</span>path<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>        <span style=color:#815ba4>if</span> <span style=color:#5bc4bf>(</span>dir<span style=color:#5bc4bf>.</span>isDirectory<span style=color:#5bc4bf>())</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>          <span style=color:#815ba4>val</span> files <span style=color:#815ba4>=</span> dir<span style=color:#5bc4bf>.</span>listFiles<span style=color:#5bc4bf>()</span>
</span></span><span style=display:flex><span>          <span style=color:#815ba4>if</span> <span style=color:#5bc4bf>(</span>files <span style=color:#5bc4bf>==</span> <span style=color:#815ba4>null</span><span style=color:#5bc4bf>)</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>            logWarning<span style=color:#5bc4bf>(</span><span style=color:#48b685>&#34;Failed to list files under directory &#34;</span> <span style=color:#5bc4bf>+</span> dir<span style=color:#5bc4bf>)</span>
</span></span><span style=display:flex><span>          <span style=color:#5bc4bf>}</span> <span style=color:#815ba4>else</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>            files<span style=color:#5bc4bf>.</span>foreach <span style=color:#5bc4bf>{</span> file <span style=color:#815ba4>=&gt;</span>
</span></span><span style=display:flex><span>              <span style=color:#815ba4>if</span> <span style=color:#5bc4bf>(</span>file<span style=color:#5bc4bf>.</span>isFile <span style=color:#5bc4bf>&amp;&amp;</span> <span style=color:#5bc4bf>!</span>hadoopConfFiles<span style=color:#5bc4bf>.</span>contains<span style=color:#5bc4bf>(</span>file<span style=color:#5bc4bf>.</span>getName<span style=color:#5bc4bf>()))</span> <span style=color:#5bc4bf>{</span>
</span></span><span style=display:flex><span>                hadoopConfFiles<span style=color:#5bc4bf>(</span>file<span style=color:#5bc4bf>.</span>getName<span style=color:#5bc4bf>())</span> <span style=color:#815ba4>=</span> file
</span></span><span style=display:flex><span>              <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>            <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>          <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>        <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>      <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>    <span style=color:#5bc4bf>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> confArchive <span style=color:#815ba4>=</span> <span style=color:#fec418>File</span><span style=color:#5bc4bf>.</span>createTempFile<span style=color:#5bc4bf>(</span><span style=color:#fec418>LOCALIZED_CONF_DIR</span><span style=color:#5bc4bf>,</span> <span style=color:#48b685>&#34;.zip&#34;</span><span style=color:#5bc4bf>,</span>
</span></span><span style=display:flex><span>      <span style=color:#815ba4>new</span> <span style=color:#fec418>File</span><span style=color:#5bc4bf>(</span><span style=color:#fec418>Utils</span><span style=color:#5bc4bf>.</span>getLocalDir<span style=color:#5bc4bf>(</span>sparkConf<span style=color:#5bc4bf>)))</span>
</span></span><span style=display:flex><span>    <span style=color:#815ba4>val</span> confStream <span style=color:#815ba4>=</span> <span style=color:#815ba4>new</span> <span style=color:#fec418>ZipOutputStream</span><span style=color:#5bc4bf>(</span><span style=color:#815ba4>new</span> <span style=color:#fec418>FileOutputStream</span><span style=color:#5bc4bf>(</span>confArchive<span style=color:#5bc4bf>))</span>
</span></span><span style=display:flex><span>    <span style=color:#776e71>//后面就是把这些文件写入到zip包的代码，略
</span></span></span><span style=display:flex><span><span style=color:#776e71></span><span style=color:#5bc4bf>}</span>
</span></span></code></pre></div><p>可以看到，除了本地的<code>log4j.properties</code>和<code>metrics.properties</code>配置文件以外，还会读取<code>HADOOP_CONF_DIR</code>和<code>YARN_CONF_DIR</code>环境变量，读取对应目录下的文件放入<code>hadoopConfFiles</code>这个<code>HashMap</code>中，而这里面的文件都会压缩到<code>__spark_conf__.zip</code>中。<br>再后续的代码就不分析了，可以参考网上其他文章。</p><h3 id=提交外部yarn集群的障碍>提交外部Yarn集群的障碍</h3><p>所以，如果执行spark应用程序的机器中配置了 <em>HADOOP_CONF_DIR</em> 或 <em>YARN_CONF_DIR</em> 环境变量（如HDP的节点安装了对应客户端都会配置上），在Spark提交任务到外部yarn集群的时候，就会将里面的配置文件压缩传输到外部集群的Executor节点，这样Executor的各种操作都会使用原集群的配置，连接不到正确的Yarn服务，最后也就导致任务执行失败。</p><h2 id=解决方案>解决方案</h2><p>所以解决整个提交外部集群的问题，有两个问题要处理：</p><ol><li>Spark应用代码使用外部集群的配置文件进行任务提交<ol><li>一种方案是启动Spark应用后，创建<code>SparkContext</code>之前，将外部集群的配置写入当前classpath的前面（如classpath是<code>.:xxx.jar</code>，那么放在当前目录就可以）</li><li>另一种方案是启动Spark应用前，将外部集群的配置写入当前目录，并通过<code>jar uvf</code>打入jar包中；当然只是针对当前问题的话，无需打入jar包</li></ol></li><li>Spark准备executor的资源时，使用外部集群配置文件<ol><li>一种方案是，创建<code>SparkContext</code>之前，将<code>HADOOP_CONF_DIR</code>和<code>YARN_CONF_DIR</code>环境变量删除，提交任务后再恢复环境变量；这样不会把集群配置传给Executor，Executor使用的是fatjar包里面的配置文件，需要提前替换。</li><li>另一种方案是，将外部集群的配置写入一个目录，并在创建<code>SparkContext</code>之前，将<code>HADOOP_CONF_DIR</code>和<code>YARN_CONF_DIR</code>环境变量改为那个目录；这样正确的配置会传给Executor使用。</li></ol></li></ol><p>结合起来最终的方案：</p><ol><li>外部集群的配置文件统一一个地方存储，可以直接存储在RDB。</li><li>启动Spark应用的时候，检查需要提交到的Yarn集群，如果是外部集群，那么：<ol><li>下载外部集群的配置文件到当前目录，同时复制到一个子目录里面</li><li>将<code>HADOOP_CONF_DIR</code>和<code>YARN_CONF_DIR</code>环境变量改为那个子目录（不能用当前目录，因为当前目录包含fat-jar，根据代码jar包也会打包过去Executor）</li><li>正常创建<code>SparkContext</code></li><li>恢复环境变量</li></ol></li></ol><p>具体实现不外乎一些黑魔法（环境变量在JVM里面修改不了，但可以修改JVM用到的那个环境变量Map），再考虑下要不要放上来吧，反正这个最主要是思路和里面的坑。</p></section><footer class=article-footer><section class=article-tags><a href=/tags/Spark/>Spark</a>
<a href=/tags/Hadoop/>Hadoop</a>
<a href=/tags/Yarn/>Yarn</a>
<a href=/tags/%E8%B7%A8%E9%9B%86%E7%BE%A4/>跨集群</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under Apache License 2.0</span></section></footer></article><aside class=related-contents--wrapper><h2 class=section-title>Related contents</h2><div class=related-contents><div class="flex article-list--tile"><article class=has-image><a href=/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1%E4%B9%8BSpark2.4%E5%9D%91/><div class=article-image><img src=/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1%E4%B9%8BSpark2.4%E5%9D%91/blackcat.2d80e3ae4410f3b70a66a85a9b3bfab6_hu988cee6e86777a2a931b84d96a10c67f_110902_250x150_fill_q75_box_smart1.jpeg width=250 height=150 loading=lazy alt="Featured image of post 跨Yarn集群提交spark任务——之Spark2.4坑" data-hash="md5-LYDjrkQQ87cKZqhamzv6tg=="></div><div class=article-details><h2 class=article-title>跨Yarn集群提交spark任务——之Spark2.4坑</h2></div></a></article><article class=has-image><a href=/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E6%9F%A5%E8%AF%A2%E6%9D%A1%E4%BB%B6%E5%B8%A6emoji%E6%97%B6%E7%9A%84JsonGenerationException/><div class=article-image><img src=/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E6%9F%A5%E8%AF%A2%E6%9D%A1%E4%BB%B6%E5%B8%A6emoji%E6%97%B6%E7%9A%84JsonGenerationException/bg1.9ed074639915eb593a7d4b504a93b1e3_hu45c6181425d2bec34f45ee6e4ec4a6e2_647451_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post 修复Elasticsearch-hadoop查询条件带emoji时的JsonGenerationException" data-hash="md5-ntB0Y5kV61k6fUtQSpOx4w=="></div><div class=article-details><h2 class=article-title>修复Elasticsearch-hadoop查询条件带emoji时的JsonGenerationException</h2></div></a></article><article class=has-image><a href=/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E8%AF%BB%E5%8F%96%E7%89%B9%E6%AE%8A%E6%95%B0%E5%AD%97%E5%8F%96%E5%80%BC%E6%97%B6%E7%9A%84NumberFormatException/><div class=article-image><img src=/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E8%AF%BB%E5%8F%96%E7%89%B9%E6%AE%8A%E6%95%B0%E5%AD%97%E5%8F%96%E5%80%BC%E6%97%B6%E7%9A%84NumberFormatException/bg2.1cec89e82f674ddee278bdb45efae3e9_hu581b7b12baca6a4e8f95bd7dff860ba9_184982_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post 修复Elasticsearch-hadoop读取特殊数字取值时的NumberFormatException" data-hash="md5-HOyJ6C9nTd7ieL20Xvrj6Q=="></div><div class=article-details><h2 class=article-title>修复Elasticsearch-hadoop读取特殊数字取值时的NumberFormatException</h2></div></a></article><article class=has-image><a href=/p/Spark%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BDhive%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%A1%88/><div class=article-image><img src=/p/Spark%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BDhive%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%A1%88/bg3.21f41c6e14fb98426d8d634cdcff4e48_hu04adce3b52cc8e582e668512c3886fe4_174126_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post Spark动态加载hive配置的方案" data-hash="md5-IfQcbhT7mEJtjWNM3P9OSA=="></div><div class=article-details><h2 class=article-title>Spark动态加载hive配置的方案</h2></div></a></article><article class=has-image><a href=/p/Spark%E5%86%99Mongodb%E7%9A%84%E5%B0%8F%E5%9D%91/><div class=article-image><img src=/p/Spark%E5%86%99Mongodb%E7%9A%84%E5%B0%8F%E5%9D%91/zelda.232f7f0a17ebcc514cf6da9d6a07c114_hu62caf65ed048351c8a298072e6e03294_171791_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post Spark写Mongodb的小坑" data-hash="md5-Iy9/ChfrzFFM9tqdagfBFA=="></div><div class=article-details><h2 class=article-title>Spark写Mongodb的小坑</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//leibnizhu.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{DISQUS&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2016 -
2022 Heaven's Door</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.11.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous></main><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#背景>背景</a></li><li><a href=#思考一个问题>思考一个问题</a><ol><li><a href=#spark-yarn-client-默认提交任务简析>Spark Yarn-client 默认提交任务简析</a></li><li><a href=#提交外部yarn集群的障碍>提交外部Yarn集群的障碍</a></li></ol></li><li><a href=#解决方案>解决方案</a></li></ol></nav></div></section></aside></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>