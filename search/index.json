[{"content":"又到年底了，今年的总结写到博客里吧。\n工作 除了日常需求、优化和支撑，今年还搞了：\nGitOps改造及推广 一个老的分布式DAG调度服务全面迁移到K8s，逐步推广摆脱虚拟机部署（任务实例执行部分） Helm Chart发布改造 MDS(Modern Data Stack)探索与实践、推广 Quarkus传教 基本是一个现代化的过程吧。\n其中 MDS 目前国内基本还没别的公司在用，我司用到的一些技术栈的Youtube可以关注下：\ndbt Airbyte，更新频率挺高 Dagster，质量不错 Cube.js 开源社区 今年参与了贡献的一些项目：\nmicrosoft/api-guidelines quarkusio/cn.quarkus.io apache/skywalking-java quarkiverse/quarkus-openapi-generator airbytehq/airbyte 总体来说比较水。\n今年Vertx中文社区基本没怎么弄了，文档翻译更新不多也不怎么参与了，还是顺手打个广告吧： Vert.x 中文站 。\n跑步 11、12月连100km都没守住，结合时代背景，原因很显然了：\n11月初天天下雨没怎么跑，到月中开始广州（事实上）封城了，跑不了 12月阳了，且康复期也大幅减量了 不过平均下来每个月还是保住了100km。\n2022年跑了4次半马（或以上），路线基本还是那些，小区绕圈，天河公园，珠江沿岸（3桥、5桥）：\n静息心率基本在 38-42左右徘徊。\n跑鞋基本在用 Altra Escalante 2.0，到10月份基本损耗完了，里程1000km了，鞋底也磨得差不多了，就用回一段时间老 Vibram FiveFinger V-Run；其实这双 V-Run 的里程也是接近1000km了。终于撑到双11，搞了 Hoka one one Rincon 3 和 Saucony Kinvara 13 。\nSaucony的熊猫背心也是挺有意思的，穿着也舒服，很干爽：\n其他 2022看的书（有记录的完整看完的）：\n强风吹拂 凤凰架构 Reactive in Java 独角兽项目 6月份换了 Macbook Pro 14\u0026quot; M1。\n书包换了 Osprey Rayline ，挺适合通勤的。\n折叠车换了 闪电Power Arc Expert，超舒服的。\n","date":"2022-12-31T20:45:49+08:00","image":"https://leibnizhu.github.io/p/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/thumbnail_huf793f0de47b176cbfbd36e3ddf64c42c_678818_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/2022%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","title":"2022年终总结"},{"content":"dagster是 MDS 中推荐使用的调度组件。Dagster的官方文档已经挺完善挺人性化的了，但为了公司内推广，还是写一篇快速入门的文档吧。\n除了文档，官方youtube也可以看看，比如 Dagster Day 2022 对dagster有整体的介绍。\n准备工作 环境准备 首先，Dagster需要Python环境和pip：\nPython下载安装 ：MacOS已自带。请确保 Python3.7及以上版本 。 pip安装 使用pip安装：\npip install dagster 创建第一个项目 执行以下命令创建一个简单的dagster项目：\ndagster project scaffold --name my-dagster-project 也可以使用官方例子创建项目，请参见： Create a New Project 。\n随后执行以下命令安装依赖：\ncd my-dagster-project pip install -e \u0026#34;.[dev]\u0026#34; 最后执行下述命令启动一个dagster服务：\ndagit 可以看到控制台打印出类似：\nTo persist information across sessions, set the environment variable DAGSTER_HOME to a directory to use. 0it [00:00, ?it/s] 0it [00:00, ?it/s] 2022-09-20 15:39:59 +0800 - dagit - INFO - Serving dagit on http://127.0.0.1:3000 in process 37014 即可在浏览器打开 http://127.0.0.1:3000 进入dagster页面。\n快速理解dagster基本概念 命令 dagster提供了两个命令：\ndagster: 核心CLI程序，可以用于执行单个job、查询asset、debug等，具体可通过 dagster -h 查询。 dagit: dagster的UI服务，前面小节已经使用到了。 Asset 在传统的工作流/DAG调度工具里，我们面向执行的任务编写(代码)/(通过UI)编辑工作流，关注的是一个个任务的流转。\n而这里定义的任务，对于数据处理而言，一般是读取一个数据源，经过处理后，写入另一个数据源。\n我们知道一个任务处理了哪个数据源、输出了什么数据源，只能通过任务命名、或阅读任务的代码/注释/文档。\n如果想知道这些用到的数据源之间的血缘关系，则需要调度工具支持+任务中声明输入输出的数据，或者在任务中调用第三方血缘关系管理的服务。\n对于单纯的执行而言，这无疑是直观的。但如果从数仓建设的角度来看，这是很不人性化的：我们只能看到一步步做了什么，不能直观地看到数仓每一层的数据流转、依赖。\n而 Dagster，提供了调度工作流的另一个视角： 数据资产视角 ，去审视数仓的数据流，从而声明式定义工作流。\n也就是说，在dagster里面，不再关注需要写什么任务，而是捋清到底有哪些数据资产，以及这些数据资产之间的关系。\n然后在代码里声明：\n数据资产：@asset 注解 这个数据资产依赖什么上游的数据资产：@asset 注解的 ins 或 non_argument_deps 属性 如何利用上游的数据资产产生当前数据资产：在 @asset 声明的方法里通过py代码实现，比如依赖的DataFrame，在方法代码里用DataFrame API，编写业务逻辑，定义return 一个新的DataFrame 就可以了。\n当你把数据资产都定义完，可以通过 define_asset_job 方法，将选定的数据资产，按依赖关系自动构建出DAG工作流（dagster称之为 job），然后就可以执行了。\n关于 asset 的定义和使用更多信息，请参考官方文档：\nA First Asset Building Graphs of Assets Assets without Arguments and Return Values Testing Assets Software-Defined Assets Asset Materializations Asset Observations Multi-Assets 等。\n说到这里，好像缺了什么？\nDataFrame是定义完了，DataFrame保存到哪里？又是从哪里读的？\nIO Manager Asset只定义了数据资产的来源依赖与自身定义，关注的是数据的业务逻辑。\n在传统的ETL工具或工作流工具里，数据的读写和处理逻辑是在同一个任务/工作流里定义的，而在Dagster中，数据的读写和处理逻辑是解耦的，处理逻辑在Asset定义了，而读写在 IO Manager 中定义。\nIO Manager有一些官方的实现，也可以自己实现。\n具体来说是继承 dagster.IOManager，实现 handle_output（数据输出） 和 load_input（数据读取） 方法。\n而每个Asset使用哪个IO Manager，则是在 @asset 注解的 io_manager_key 属性中设置，如：\nclass MyIOManager(IOManager): def handle_output(self, context, obj): pass def load_input(self, context): pass @io_manager def my_io_manager(init_context): return MyIOManager() @asset(io_manager_key=\u0026#34;my_io_manager\u0026#34;) def my_op(): pass 具体请参见官方文档：\nIO Managers Unconnected Inputs Op \u0026amp;\u0026amp; Graph \u0026amp;\u0026amp; Job 概念简介 Op（不是那个OP）是dagster最基础的计算单元，包括asset物化的时候，dagster也是包装成op进行执行的。\nOp也定义了（一个或多个）输入和输出，也可通过IO Manager做存储管理，也可以使用job级别的 resource 定义。\n还可以通过 @op 注解的 config_schema 定义执行时需要的op_config配置。\n关于 Op 的文档：\nWriting, Executing a Single-Op Job Connecting Ops In Jobs Testing Ops and Jobs Ops Op Events and Exceptions Op Hooks Op Retries Graph 可以将多个op或Graph组成DAG————\u0026ldquo;或Graph\u0026rdquo;，也就是说支持graph的嵌套。\ngraph的定义是通过python代码的入参依赖构建的，如官方的例子：\nfrom dagster import graph, op @op def return_one(context) -\u0026gt; int: return 1 @op def add_one(context, number: int) -\u0026gt; int: return number + 1 @graph def linear(): add_one(add_one(add_one(return_one()))) 关于 Graph 的文档：\nGraphs Nesting Graphs Dynamic Graphs Job 是Dagster的执行和监控单元，Job由Graph或Op（通过Python代码）连接而成。类比到传统调度服务，就是整个工作流了。\nJob的定义方式与Graph类似，也是在Python代码中通过入参或注解实现依赖。\n关于 Job 的文档：\nConnecting Ops In Jobs Testing Ops and Jobs Jobs Job Execution Job Metadata \u0026amp; Run Tags 几者转换 \u0026amp; 常用方法 其实也不是互相转换，主要是转成Job的。我们在代码里定义了asset、op、graph这些，是可以直接定义为一个job的。具体：\ndagster.load_assets_from_modules ：可以从指定的Python module中加载所有定义的asset，可以简化 repository 的配置。类似的还有 load_assets_from_current_module、load_assets_from_package_module、load_assets_from_package_name 等方法。 dagster.define_asset_job ： 可以将一系列的asset的物化动作作为一个job，可以通过 selection 属性选择所需的asset。 dagster.GraphDefinition.to_job ：可以将Graph定义（用Graph的方法名调用）转换为一个Job dagster_dbt.load_assets_from_dbt_project ：从dbt项目的模型定义加载asset，需要依赖 dagster-dbt dagster_dbt.dbt_run_op ：利用resource里面定义的dbt资源，产生一个 dbt run 的op。 Schedule \u0026amp; Sensor image.png 是定时执行Job，可以简单地配置每小时、每天、每周等，也可以通过cron表达式配置。详见 Schedules。需要注意的是时区的配置，通过 ScheduleDefinition 的 execution_timezone 属性配置；以及可以通过 environment_vars 属性定义执行Job时的环境变量。\nSensor 定义在Job运行结束或asset物化结束后的操作，可以根据执行结果做任何自定义的操作，包括但不限于：\n发送企业微信Bot消息通知 获取物化结果的OSS下载地址，发送邮件给客户 清理过时的分区数据 ……………… 详见 Sensors。\nSchedule和Sensor的运行都需要 dagster-daemon 进程。\n在k8s中，dagster-daemon可以与dagit分别做成一个pod里的两个容器。\nRepository \u0026amp; Workspace Repository 包含一个项目的所有 Asset、Op、Graph、Job、Schedule、Sensor 等资源。dagster UI左侧栏同一时间只显示一个Repository。通过 @repository 注解定义。\n而 Workspace 是Dagit实例级别的工作区，可以包含多个Repository。通过 workspace.yaml 文件配置、里面从哪里加载Repository定义。\n可以指定Python文件、Python的package、或gRpc服务（可通过 dagster 命令启动gRpc服务）。详见 Workspace。\ndagster实例 在环境变量 DAGSTER_HOME 配置的目录下的 dagster.yaml 文件进行配置，该文件定义了Dagit实例的一些存储位置、日志等等配置。\n详见 Dagster Instance。\n","date":"2022-08-26T15:21:08+08:00","image":"https://leibnizhu.github.io/p/Dagster%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/thumbnail_hu63b30981a5f882541819e7af87283ab0_1582041_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Dagster%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","title":"Dagster快速入门"},{"content":"Delta Lake Delta Lake是Databricks推出的流批一体存储层，分为开源版和商业版，深度绑定自家Spark。\nLakehouse论文 请先快速阅读Databricks的论文 Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics。\n这篇论文主要讲了传统的数仓和数据湖模型的缺陷，在此基础上提出湖仓一体的LakeHouse模型，并阐述了LakeHouse的一些设计细节。下面挑一些要点来讲讲。\nLakeHouse是Databricks提出的一种将在未来几年取代传统数仓结构的架构。传统数仓将业务数据库数据收集到集中式的数仓，在写入时针对下游BI消费对schema进行优化；然而传统数仓将计算与存储耦合，且对非结构化数据支持不佳。于是出现了数据湖，以开放格式（如Parquet、ORC）存储在低成本的存储系统（如S3），湖中的数据ETL到下游数仓，再提供给BI使用。但这样的两层结构带来了一下问题：\n可靠性：难以维持数据湖与下游数仓的数据一致性 数据不及时：需要两次ETL才到BI，对于传统数仓而言，甚至是开历史倒车 对高级分析支持不佳：机器学习Tensorflow、PyTorch等，需要通过非SQL处理大量数据，无法通过JDBC/ODBC高效运行 总成本高 于是噔噔瞪瞪，LakeHouse出来了：基于开放文件格式（如ORC/Parquet）的低成本存储，同时提供数仓的管理功能、传统DBMS的管理特性（如ACID事务、版本控制、审计、索引、缓存等）、及for高级分析（如机器学习）的高速I/O，的高性能系统。同时在设计上适应可存储计算隔离的云计算环境。LakeHouse解决了以下关键问题：\n可靠的数据管理：传统数据湖只是管理了一堆半结构化文件，在事务、回滚等方面的能力太差。 支持机器学习：支持DataFrame API；而ML系统可以直接读取数据湖的格式，很多也支持DataFrame SQL查询性能：归功于过去对ORC/Parquet做SQL查询的性能优化经验 下面是这三代数仓/数据湖结构对比：\n下面是LakeHouse架构和实现要点。\n元数据层：\nLakeHouse设计理念是使用标准文件格式以低成本存储，在存储的顶层设计实现传统的元数据层，可提高抽象级别、实现ACID事务、版本控制等特性。DeltaLake 以Parquet格式在数据湖本身的存储上存放事务日志 ，可以存储哪些对象属于表的信息：\n实践证明这套设计在性能类似或更优于原始Parquet/ORC的同时，提供了事务等管理功能 元数据层同时可以保证数据质量，如强制schema（schema enforcement）可以保障新增数据必须匹配已有schema，并提供了API允许配置数据约束（如某列数据需要满足枚举要求）。 元数据层可实施ACL、日志审计等功能 SQL性能优化：\n缓存：热数据缓存到更快的存储设备，如SSD、RAM 辅助数据：Lakehouse虽然使用了公开的文件格式（Parquet/ORC），但可以通过辅助数据帮助优化查询速度，如构建索引、维护统计信息。DeltaLake在上面提到的事务日志中维护列的最大最小值信息，方便查询时跳过；并实现了基于布龙过滤器的索引 数据布局优化：在Parquet格式不变的基础上，还是有优化空间，如记录排序，让数据聚拢便于读取。DeltaLake使用Z-order和Hilbert曲线对记录排序以在读取时更好地进行文件修剪。 针对高级分析的优化：\n通过声明式的DataFrame API加速高级分析。DataFrame API在DeltaLake的位置：\n下图是Spark MLlib中执行声明式的DataFrame API。用户的DataFrame调用是lazy的，执行时Spark引擎获取执行计划，传给DeltaLake的库，后者读取元数据层，根据缓存、索引、统计信息等进行优化查询，最后执行。\nDelta Lake白皮书 请先快速阅读Databricks的论文 Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores 。\n纯Parquet/ORC的缺陷：\n多对象更新不是原子的，所以查询之间没有隔离；回滚写操作也很困难：如果更新查询崩溃，则表处于损坏状态。 元数据操作的成本很高 大多数企业数据集是不断更新的，因此它们需要一个原子性的写入解决方案 Delta Lake应运而生，它是一个云对象存储上的ACID表存储层。核心思想：\n用ACID方式在云对象中存储日志（write-ahead），以纪录哪些对象是Delta表的一部分 日志还包含元数据，例如每个数据文件的最小/最大统计信息，用于查询优化。 将所有元数据都存在底层对象存储中，并且使用针对对象存储的乐观并发协议实现事务，意味着不需要运行服务器来维护Delta表的状态 特性：\nTime travel UPSERT, DELETE and MERGE operations Efficient streamingI/O Caching Data layout optimization Schema evolution Audit logging 使用DeltaLake的例子：\n旧世界的桎梏：\n对象存储API：云对象存储的API太慢太麻烦云云 一致性保证：云对象存储为每个键提供了最终的一致性，并且没有跨键的一致性保证；对现有对象的更新可能不会立即对其他客户端可见 性能问题 目前有三种表存储方式： 文件目录：将表存储为对象的集合，基于一个或多个属性将记录“分区”到目录中。比如Hive。主要问题是存在性能和一致性问题 自定义存储引擎：在一个单独的、高度一致的服务中管理元数据本身。比如Snowflake。需要运行一个高可用性的服务来管理元数据，这可能很开销很大；而且表的所有I/O操作都需要联系元数据服务，这会增加其资源成本，降低性能和可用性；现有计算引擎的连接器需要更多的开发对接工作。 噔噔瞪瞪，对象存储中的元数据 ：DeltaLake将事务日志和元数据直接存储在云对象存储中，并在对象存储操作上使用一组协议来实现序列化 后面是具体的存储格式和访问协议，篇幅太长，不展开了。后面再补吧。\nDeltaLake基本概念与使用 官方文档\n注意到上文所说的事务日志放在对象存储的同一位置，就是在表的目录下，有个 _delta_log 子目录，一个版本对应里面一个json，记录了事务操作，Time Travel显然也是基于此。\nIceberg 设计理念 Meetup视频： Data Science DC Nov 2021 Meetup: Apache Iceberg - An Architectural Look Under the Covers，也可以看 演讲稿。\nWhat is Iceberg ✅ Iceberg 是…… ❌ Iceberg不是…… Table Format定义 存储引擎 遵循Iceberg Table Format定义与表进行交互的一套API及库 执行引擎 一套服务 Table Format? A good way to define a table format is a way to organize a dataset’s files to present them as a single “table”. Another somewhat simpler definition from a user’s perspective is a way to answer the question “what data is in this table?”. A single answer to that question allows multiple people, groups, and tools to interact with data in the table at the same time, whether they’re writing to the table or reading from the table.\nTable Format 是表的抽象，将数据集文件组合起来，以单个“表”的形式呈现，允许人和工具与表数据高效交互。\nTable Format简史 Hadoop刀耕火种：直撸HDFS，用户必须清楚如何将问题转化为MR模型并用java编码，数据也没有schema，用户必须熟知他处理的文件的实际schema。\nHive：针对Hadoop第一个问题，用SQL替代之；针对Hadoop第二个问题，用metastore记录schema。Hive从此成为了事实上的Table Format标准。Hive的Table Format我们比较熟知，就不细述了，更多优点也不说了，看视频有。说下缺点：\n修改数据太低效，尤其分区很大，修改里面的少量数据、或频繁更改 无法安全地更改多个分区的数据，Hive只保证单个分区的事务一致性 在实际操作中，多个作业同时修改一个数据集是不安全的 对于大型表，即便只是列出所有目录也要耗费很多时间 为了利用分区优化查询，用户必须知道物理的分区布局（比如按年、月、日分区） Hive表的统计信息往往是过时的，可能导致优化器选择效率低的计划 在云存储中性能不佳。Hive表一个分区中的所有数据都具有相同的目录前缀，利用不到云存储不同前缀的分流 Iceberg：Netflix发现，解决Hive Table Format问题的关键是放弃在文件夹级别跟踪表数据，改为在文件级别跟踪。修改之后，还能做到：\n保证表视图的正确性和一致性 实现更快的查询计划和执行 用户无需了解数据的物理布局（如分区） 实现更好、更安全的表进化（table evolution，即表schema变更） Iceberg Table Format定义 Iceberg Table Format架构：\n分了三层：\nIceberg catalog ：记录各个表当前元数据指针的位置。Iceberg catalog 要求元数据指针的实现支持原子性的更新操作。实际的存储方式取决于具体使用的存储： HDFS：元数据文件夹中有一个名为 version-hint.text 的文件，其内容是当前元数据文件的版本号 Hive metastore：metastore的表对象用表属性记录当前元数据文件的位置 Nessie：Nessie保存了当前元数据文件的位置 metadata layer，具体又包括： metadata file：存储表的元信息，包括表的schema、分区信息、所有快照信息、及当前快照等等,json格式。查询过程：按 current-snapshot-id 在 snapshots 找到对应快照，读取其 manifest-list 属性，打开 manifest lists。 manifest list：manifest files 的列表，每一行是构成该快照的一个 manifest file 的信息，包括 manifest file 位置、所属分区、分区列的最大最小值等。查询时可以在此阶段进行一些优化，例如使用行计数或使用分区信息过滤数据。 manifest file：记录数据文件以及有关每个文件的其他详细信息和统计信息，可用于优化查询，而且这些信息是在写入操作期间更新的，比Hive记录的统计信息的更准确、更新。Iceberg 与文件格式无关，因此清单文件还指定了数据文件的文件格式，例如 Parquet、ORC 或 Avro data layer ：实际存储的文件 日常表操作在Iceberg Table Format中的体现：\nCREATE TABLE xxx：创建metadata file，里面有快照s0（但不指向任何manifest list），表的当前元数据指针指向该metadata file INSERT INTO xxx： 创建数据文件（Parquet或其他格式） 创建manifest file，指向该数据文件 创建manifest list，指向该manifest file 创建新的metadata file，带有快照s1和s0，current-snapshot-id 是s1 表的当前元数据指针指向新的metadata file MERGE INTO xxx USING ... ON ... WHEN MATCHED THEN UPDATE ... WHEN NOT MATCHED THEN INSERT...：upsert的操作，步骤比较多： 找出满足 USING ... ON ... 条件的数据，读取查询引擎，执行 UPDATE 子句的更新，写入新的数据文件（不满足条件的也写入了，也就是 copy-on-write 策略；此外还有merge-on-read 策略） 读取不满足 ON 条件的外部表数据，写入另一个数据文件 创建manifest file，指向新建的两个数据文件 创建manifest list，指向该manifest file 创建新的metadata file，带有快照s2、s1和s0，current-snapshot-id 是s2 表的当前元数据指针指向新的metadata file SELECT * FROM xxx： 查询Iceberg catalog，获取表的当前云数据指针 读取当前元数据指针指向的metadata file，获取当前快照的manifest list地址 读取manifest list，获取manifest file地址 读取manifest file，获取所有数据文件的地址 读取所有数据文件、返回 隐式分区查询 ：前面说到的，Hive的分区查询经常需要用户熟知分区物理布局，Iceberg对此进行了优化，实际操作是：a)读取 metadata file，获取要查询的snapshot的分区信息（有哪些字段，当前查询的是否包含在内），b)读取 manifest file，读取里面记录的所有数据文件的分区字段信息，与用户的查询条件做对比，找出要读取的数据文件，c)最后只读取这些数据文件 Time Travel：其实就是表级版本控制，语法是 SELECT * FROM xxx AS OF '日期'，实际操作是：打开当前 metadata file，在 snapshots 数组查找满足查询版本日期的snapshot，后面就是正常流程（manifest list -\u0026gt; manifest file -\u0026gt; 数据文件）。 注意 Time Travel，Iceberg保留了旧版本的数据，也提供了异步后台进程去清理这些旧snapshot（垃圾回收GC）。用户可以配置GC策略，这里就是存储空间的衡量了。\n小文件合并（Compaction） Compaction是后台的异步进程，负责将一组小文件合并压缩成少量大文件，可以平衡写入和读取性能的权衡：低延迟写入需要在获取数据后尽快写入，意味着生产更多小文件；而高吞吐量读取需要单个文件保存尽量多的文件。合并操作的输入输出可以是不同的文件格式。\n但Iceberg只是提供了File Format及其API和库，实际的Compaction操作由集成了Iceberg的其他工具/引擎负责调度、执行。\nIceberg Table Format优势 事务的快照隔离：乐观锁 更快的执行计划和执行速度：写入数据时更新统计信息、文件级跟踪统计信息 对物理布局做抽象，对用户暴露逻辑视图 所有引擎都能立即看到变化：写入时更新 事件监听器：允许在Iceberg表上发生事件时通知其他服务 有效地对数据集进行较小的更新：文件级别跟踪数据 Iceberg基本使用 目前有 Spark、Flink、PrestoDB、Hive、Impala等集成。\nQuick Start\n官方文档\n从Hive迁移到Iceberg\nDelta Lake vs Iceberg 解决的痛点 从上面的介绍可以看出，DeltaLake和Iceberg的着重点不太一样。DataLake着眼于数仓/数据湖的架构优化、对流批一体的支持，而Iceberg站在了更高的抽象级别，定义TableFormat，制定标准和核心API，提供多种实现。\nDeltaLake Iceberg • 快速upsert/delete数据• Schema限制• ACID事务• 多版本控制• 小文件Compaction• 支持流批读写 • ACID事务• 多版本控制• 抽象、通用、优雅• 灵活的元数据管理• 元数据性能(文件粒度跟踪)• 隐式分区、分区演进 对比表 Iceberg Hudi Delta Lake ACID事务 √ √ √ 分区演进 分区演进 支持更改分区方案；分区转换 可对时间戳字段以年、月、日、小时粒度隐式分区 × Generated Columns可以实现类似隐式分区的功能，但目前是Public Preview状态 × Data Skipping可以实现类似隐式分区的功能 Schema演进 增/删/改/重命名/重排列 删/改名/重排只在Spark中支持 2.0.0之前只支持增加列、列重命名、更新列、重排序，2.0.0(2022-07-21发布)开始支持删除列 Time-Travel(表版本管理) √ 通过快照实现 √ √ 每个Delta文件都代表了表对上一版本的更改，默认保留30天 项目级别 Apache顶级项目 Apache顶级项目 Linux基金会项目（Databricks TSC） 社区活跃度(截至2022/03/28) 240贡献者2241已合并PR275未关闭PR 252贡献者2880已合并PR160未关闭PR 145贡献者16已合并PR43未关闭PR 兼容读取的工具 Apache Hive, Dremio Sonar, Apache Flink, Apache Spark, Presto, Trino, Athena, Snowflake, Databricks Spark, Apache Impala, Apache Drill Apache Hive, Apache Flink, Apache Spark, Presto, Trino, Athena, Databricks Spark, Redshift, Apache Impala, BigQuery Apache Hive, Dremio Sonar, Apache Flink, Databricks Spark, Apache Spark, Databricks SQL Analytics, Trino, Presto, Snowflake, Redshift, Apache Beam, Athena 兼容写入的工具 Apache Hive, Dremio Sonar, Apache Flink, Apache Spark, Trino, Athena, Databricks Spark, Debezium Apache Flink, Apache Spark, Databricks Spark, Debezium, Kafka Connect OSS Delta Lake: Trino, Apache Spark, Databricks Spark Apache Flink, Debezium.Databricks Delta Lake: Databricks Spark, Kafka Connect 文件格式支持 ParquetORCAvro ParquetORC Parquet 社区活跃度 此外值得注意的是，DeltaLake有81%的提交来自Databricks自己，而Iceberg的提交前5名提交是：Netfix 18.7%，Apple 17.1%，AWS 10.4%，Tabular 8.3%,Dremio 5.5%。\n另外，整个DeltaLake项目，无论是对Issue的反应处理/回答、提PR、RoadMap 的Issue，基本都是Databrick的员工，因此，DeltaLake相对而言没那么开放，不那么社区驱动：\n另一个值得注意的是DeltaLake跟Spark的绑定，及其社区版与商业版的区别（Databricks拥有自己的Delta Lake 专有分支，该分支具有仅在Databricks平台上可用的功能，Spark也是）。\n","date":"2022-07-20T10:03:41+08:00","image":"https://leibnizhu.github.io/p/%E6%95%B0%E6%8D%AE%E6%B9%96%E8%B0%83%E7%A0%94Delta-Lake-vs-Iceberg/magician_hud45400ab03b8bcc6bc10b4c1488a0b09_97675_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E6%95%B0%E6%8D%AE%E6%B9%96%E8%B0%83%E7%A0%94Delta-Lake-vs-Iceberg/","title":"数据湖调研：Delta Lake vs Iceberg"},{"content":"Quarkus对可观测性的支持情况 Quarkus在可观测性方面支持了 OpenTracing 和 OpenTelemetry 。如果要对使用Quarkus开发的服务接入Skywalking做链路跟踪：\nSkywalking支持OpenTelemetry，但只是在 metric部分 支持了。至于为什么，可以看看PR What does opentelemetry mean? 里面的讨(nu)论(chi)。 Skywalking兼容Opentracing协议。但Quarkus目前对OpenTracing的支持是针对Jaeger的，而Skywalking是用GRpc通信的，不能直接对接。 所以摆在面前的路有两条：\n模仿 quarkus-smallrye-opentracing （已deprecated），基于Provider，写一套用于拦截Quarkus请求打span等的插件。 使用Skywalking Java Agent 传统姿势，但要注意各个组件的支持情况。 第一条路工作量是未知的，可行性也未知。还是选简单点的第二条路吧。\nSkywalking对Quarkus常用组件的支持情况 先简单盘点一下Skywalking Java Agent对Quarkus做web服务常用组件的支持情况：\ncore: 使用了 Vert.X-core， 支持3.x 和 4.x。 Http server: 使用了 RestEasy， 已支持 3.x。Quarkus用的是 4.x Http client : RestClient，最终使用的是 Apache httpcomponent HttpClient ，已支持。 ORM: Hibernate：不支持。 MyBatis: 支持 3.4.x -\u0026gt; 3.5.x Redis: 使用了 vertx-redis，Vertx是自己用NetSocket实现了redis的协议，目前还不支持。 可以看到，目前redis client、http Server、Hibernate还不支持。\n注：详细请参考官方的 支持列表 。\n解决方案 RestEasy 4.x 这个我提了 PR，已 合入main分支，等8.12.0版本发布就行，或者先下载 main分支源码、自行打包用着。\n2022-09-10更新 8.12.0 版本已经release：Releases v8.12.0，可直接使用\n在实操中，请移除掉 RestEasy 3.x 的插件，以免冲突。如在sidecar中执行：\nrm /skywalking/agent/plugins/resteasy-server-3.x* Hibernate ORM虽然不支持 Hibernate，但支持具体的jdbc驱动（如Mysql、PostgreSql等），其实是可以追踪到数据库读写的，只是少了ORM这一层而已，关系不大。\nVert.X相关 什么？Vert.X-core不是支持了吗？\n正因为支持了才有问题！\nSkywalking如何支持Vert.X的 我们可以先看看Skywalking的 vertx-core-4.x-plugin 源码，跟踪原理很简单，它对 VertxBuilder 进行了增强，在VertxImpl实例化时注入了 SWVertxTracer 作为Vertx的tracer，也就是说利用了vertx的tracer机制。\n这样做的原因很简单，Vertx大量用到了EventLoop线程和Worker线程的切换，如果直接像其他的Skywalking插件一样，直接增强处理的方法，那么调用 ContextManager.getOrCreate() 的时候，由于线程切换，是拿不到上一个span使用的 TracerContext （保存在ThreadLocal里）的，也就是每个操作都成了独立的TracerContext，那么就无法将span串起来，也就无法实现追踪。所以直接利用Vert.X自己提供的tracer机制，Vert.X各个组件在触发一些事件（如Http服务接受到请求，Http客户端接受到响应）时，就会调用 VertxTracer 的对应方法，记录事件，在这个过程中，Vertx是允许带一个Payload的，对于 vertx-core-4.x-plugin 插件，这个Payload就是 Skywalking的 AbstractSpan ，这样就可以在Vert.X内部实现绕开ThreadLocal的线程限制、进行追踪了。\nSkywalking对Vert.X的支持、与Quarkus的关系 那么问题在哪呢？\n问题就在于，这样的机制决定了，只有在Vert.X体系里的才会被追踪到，看看前面列的那些组件，RestEasy、HttpClient、JDBC Driver这些都不在Vert.X控制下的，他们产生的Span和Vert.X是无法关联起来的。\n我们再看看目前 vertx-core-4.x-plugin 实际支持的Vert.X组件，从 源码 来看，实际支持了HTTP服务、HTTP客户端，以及EventBus（包括本地的和分布式的）。\n一个个看：\nHTTP服务，Quarkus在Vert.X-core之上用了 RestEasy 做路由，其实是无需再在Vert.X里面追踪的。 HTTP客户端，前面说了，目前用的是 Apache httpcomponent HttpClient ，也是不用管的。 Eventbus，Quarkus确实用了Vert.X-core的EventBus，这个好像没啥办法 Redis Client，目前 vertx-core-4.x-plugin 不支持，Quarkus用到，有兴趣的朋友可以考虑实现一个、提个PR 问题捋完了，方案就是个取舍的问题了。\n艰难的道路——全面支持Vert.X 应用里的RestClient改成Vert.X的Http client 扩展 vertx-core-4.x-plugin ，增加Redis请求响应的处理 考虑如何支持 JDBC Driver 的追踪 对于Quarkus其他组件，随缘了，或者想办法把VertxTracer和正常的基于TreadLocal的TracerContext关联起来？ 世上无难事、只要肯放弃之路 排除掉 vertx-core-4.x-plugin ，放弃EventBus和Redis Client的追踪 我选择放弃，在sidecar里面执行：\nrm /skywalking/agent/plugins/apm-vertx-core-* 总结 RestEasy 4.x 将在Skywalking 8.12.0里面支持，如有需要可提前自行编译（2022-09-10更新 2022-09-03已release）\n放弃EventBus和Redis Client的追踪，下次一定.jpg\nsidecar里面记得排除不必要的plugin：\nrm /skywalking/agent/plugins/apm-vertx-core-* /skywalking/agent/plugins/resteasy-server-3.x* ","date":"2022-07-18T12:58:35+08:00","image":"https://leibnizhu.github.io/p/Quarkus%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%85%A5Skywalking%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA/bus_hu060cc9712508966cf27d3aae1bcfec3e_109278_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Quarkus%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%85%A5Skywalking%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA/","title":"Quarkus服务接入Skywalking链路跟踪"},{"content":"Skywalking Java Agent的开发与测试在官网文档已经有详尽的介绍，包括（但不限于）：\n编译： Compiling Guidance ，及 How to build a project 编写testcase并运行： Plugin automatic test framework 发布： Apache SkyWalking release guide，及 Java Agent Release Guidance 但在M1芯片的MacOS中，还是有一些需要调整的地方，在此简单记录一下。\n编译过程需要clone其他仓库等一些网络操作，请先保障网络畅通；若直连网速不佳，请先备好梯子并配置到终端，如：\nexport ALL_PROXY=socks5://127.0.0.1:1080 git config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 clone完 skywalking-java 后，先初始化git子模块，官方文档 也有说到：\ngit submodule init git submodule update 编译过程会下载 protobuf， 而目前没有M1对应的版本，请在 test/plugin/agent-test-tools/bin/fetch-code.sh 增加 -Dos.detected.classifier=osx-x86_64 ，具体位置大约在32行：\n\u0026#34;$ROOT_DIR\u0026#34;/../../../../mvnw -B package -DskipTests -Dos.detected.classifier=osx-x86_64 启动Docker\n测试用到 docker-maven-plugin 起docker容器，这插件可能还没支持 macOS aarch64，无法使用 unix socket，导致在 M1 使用 docker-maven-plugin 构建镜像报错，如：\ncould not get native definition for type `POINTER`, original error message follows: java.lang.UnsatisfiedLinkError: Unable to execute or load jffi binary stub from `/var/folders/c0/xxxxxx/T/`. Set `TMPDIR` or Java property `java.io.tmpdir` to a read/write path that is not mounted \u0026#34;noexec\u0026#34;. [ERROR] /xxxxx/skywalking-java/xxxx.dylib: dlopen(/xxxxx/skywalking-java/xxxxx.dylib, 0x0001): tried: \u0026#39;/xxxxx/skywalking-java/xxxxx.dylib\u0026#39; (fat file, but missing compatible architecture (have \u0026#39;i386,x86_64\u0026#39;, need \u0026#39;arm64e\u0026#39;)) 需要通过 socat 来桥接，具体操作：\n# 安装socat brew install socat # 将 unix socket 代理到 tcp 端口 nohup socat TCP-LISTEN:2375,range=127.0.0.1/32,reuseaddr,fork UNIX-CLIENT:/var/run/docker.sock \u0026amp;\u0026gt; /dev/null \u0026amp; # 设置环境变量为socat桥接的tcp端口 export DOCKER_HOST=tcp://127.0.0.1:2375 执行测试拉取docker镜像时，tomcat没有m1的版本，参考 docker-maven-plugin 的配置文档 在 test/plugin/containers/jvm-container/pom.xml 加入 createImageOptions 参数（请注意，docker-maven-plugin 的0.39.0版本才有这个参数）：\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.fabric8\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;!-- 关键点1: 插件版本 --\u0026gt; \u0026lt;version\u0026gt;0.40.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;images\u0026gt; \u0026lt;image\u0026gt; \u0026lt;name\u0026gt;skywalking/agent-test-jvm:${container_image_version}\u0026lt;/name\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- 关键点2: 通过 createImageOptions 配置docker镜像选用的平台 --\u0026gt; \u0026lt;createImageOptions\u0026gt; \u0026lt;platform\u0026gt;linux/x86_64\u0026lt;/platform\u0026gt; \u0026lt;/createImageOptions\u0026gt; \u0026lt;from\u0026gt;${base_image_java}\u0026lt;/from\u0026gt; \u0026lt;workdir\u0026gt;/usr/local/skywalking/scenario\u0026lt;/workdir\u0026gt; \u0026lt;assembly\u0026gt; \u0026lt;!-- 以下省略 --\u0026gt; 到此为止，就可以使用：\nbash ./test/plugin/run.sh -f ${scenario_name}` 来执行测试了。\n","date":"2022-06-30T21:10:28+08:00","image":"https://leibnizhu.github.io/p/Skywalking-Java-Agent%E5%9C%A8M1%E8%8A%AF%E7%89%87MacOS%E4%B8%8B%E7%9A%84%E5%BC%80%E5%8F%91tips/platform_hu802e01a6fd24303ec74834630002e240_149920_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Skywalking-Java-Agent%E5%9C%A8M1%E8%8A%AF%E7%89%87MacOS%E4%B8%8B%E7%9A%84%E5%BC%80%E5%8F%91tips/","title":"Skywalking Java Agent在M1芯片MacOS下的开发tips"},{"content":"记录一下。\n2016款15寸MBP，购于2017年，定制加了内存、CPU、SSD。\n大概是去年年初的时候霍尔感应器坏了，感应不到开盖动作，开盖后按键盘可以恢复休眠，但屏幕亮不了————其实屏幕也是有显示的，但是没背光，打着手电勉强能看到是有显示的；要用磁铁在喇叭附近吸一下才能亮屏。\n到了去年4月就彻底罢工，磁铁不再管用，铷磁铁也不行，要亮屏必须强制重启才能亮屏。\nApple Store对这种问题的解决方案是换主板，几千块，修是不会给你修的。最后找人直接换了霍尔感应器，复活了。\n上周，5月26号开始发现电脑有在开着盖的前提下自己灭屏的情况，折腾下就又好了。次日上午，复现，反复开合盖又可以了。到下午，彻底亮不了屏，重启也不行，也是没背光，看起来像是背光直接坏了。借了个显示器外接凑合用了一下午。\n回家翻出XPS13装了一下软件凑合用着先，不得不说真是不习惯Windows，不过也是勉强能用了。\n所以有仨方案：\n修MBP，代价未知，预后未知；加上之前风扇也一直有问题（响声特别大，不是积灰那种声音，是准备起飞那种； 继续用XPS13，能给我用的时间大概还有两个月，过了一周适应期也能凑合用起来； 换新MBP，看了下官网，发货要到7月份了。 到29号的时候，忽然想到看看官翻，可以马上发货，但只有14寸丐中丐版本。14寸是我想要的，但16G内存有点……再思考了一下，其实16G也够平常上班写写代码了，24期免息走起。\n官翻发货是真的快，第二天发货，顺丰，再隔了一天就到了。\n开箱，开机，插上移动硬盘TimeMachine恢复，很快就恢复了生产力，嗷嗷的。\n说下体验：\n屏幕没得说，mbp屏幕一直很OK的，这次miniLED 120Hz的体验是更好了；其他的触摸板之类的也是，一如既往。 这代的mbp不用蝴蝶键盘了，不过我其实还挺喜欢蝴蝶二代的，放膝盖上敲特别有感觉；当然了，新的这个手感也还可以。嗯虽然手里有Filco Minila Air红轴和RealForce 87u十周年纪念静音版，但平时还是用笔记本自己键盘比较多。 续航和散热，杠杠的，看了下监控，其实很多时候都没开风扇，比如只是看看网页、vscode写写文档、代码，不编译的时候。 magesafe充电也回来了，好评；读卡器回归，不过我用不上。 编译速度肉眼可见快了很多（当然，只是跟我的老mbp比） 外放，用网抑云的时候偶尔会爆音，原因未明，网上也有类似的情况： 问：macbook pro 14寸 音响破音 ，网页看视频开外放则不会，奇怪。 外观：比2016款是丑了很多，不过也标志着mac团队从设计主导往技术/实用主导的变化吧 关于刘海：\n目前的系统导航栏图标太多被挤到刘海里面之后是显示不了的，只能通过command+拖拽图标进刘海里面，才能把刘海里面的图标挤回来。\n还有一种方案是装某些折叠导航栏的app，比如 bartender（收费软件），也可以用 Hidden Bar ，免费，前面链接是github地址。安装后只保留用得多的一些图标，其他统统折叠起来————对，只是为了保证要用的时候能找到，展开后要找刘海里的图标还是靠挤。\n","date":"2022-06-03T22:19:59+08:00","image":"https://leibnizhu.github.io/p/%E6%8D%A2%E7%94%B5%E8%84%91%E4%BA%86/thumbnail_hue48f0a63025b2818dda59cd081135c0a_685254_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E6%8D%A2%E7%94%B5%E8%84%91%E4%BA%86/","title":"换电脑了"},{"content":"最近把一个老项目迁到了Quarkus，想谈谈一些感想和想-法。\nWhy Quarkus??? 答案很简单，for 云原生。在 官网 也可以看到Quarkus的宣传点也是围绕云原生进行的。\n云原生的实现大概有这些要点：\n应用/架构设计遵循 12-Factor 服务容器化，最好框架层面就有支持 启动速度快，资源占用少，便于伸缩。这里又包括镜像大小、layer设计、应用本身启动速度等因素。 配合GitOps持续集成、持续交付 而Quarkus很好解决了这些问题。反观Spring，在启动速度、资源占用这块实在不尽人意，对k8s的支持也是很落后，对graalvm native- image也是起步落后。\n用不用Native?? 谈Quarkus绕不开native（native可执行文件的构建）。没办法，官方先来的，首页就是大大的native/jvm性能对比。\nnative实测是真的快（不到0.1s），内存占用也确实小（启动几十MB，跑一段时间稳定大概300MB附近，不过不同压力的不同类型服务也不能说明什么）。但有什么缺点？\n编译速度慢 ：一个比较简单的web服务，在我们CICD的机器上编译大概需要4min，如果是容器内multistage编译则时间更长，但直接编译对环境又有要求，所以还是最好有更强大的编译机器。 编译产物体积大 ：我们编译出来的二进制文件大约130-150MB，upx 压缩后40MB以内；单看最后的docker镜像，如果用最小的基础镜像，大约整个镜像可以做到100MB以内，是挺小的；但是每次构建变更的layer，都是整个二进制文件的变更，也就是没次编译多了一个40MB的layer出来，体积相当可观。同样的服务，如果打jar，真正变更的只有不到1MB，pod拉镜像的速度会更快，对docker仓库的压力也更小。 开发多了不少限制 ：你要注意新引入的依赖是不是支持native的；所以可能需要json序列化反序列化的类记得加上 @RegisterForReflection 注解；如果是外部依赖的实体类，要编写一个什么json文件声明要处理…………本文不是Quarkus教程，就不详细展开了，总之，如果要native，最好整个服务都只用quarkus生态里面的依赖，否则你不知道什么时候就编译不了了。虽然quarkus的生态已经比较完善，但总有没cover到的地方，比如hadoop生态的sdk。 没错，我们quarkus native上了生产一两个月，运行良好，充分体现了native的优越性；但是，现在要加入hadoop这些，没办法，只能放弃，要么自己实现一整套hdfs client之类的，成本很高。\n响应式要上吗? 官网说：\nUnifies imperative and reactive Combine both the familiar imperative code and the reactive style when developing applications.\n真的有那么Unify吗？未必哦。\n这里说的 Unifies imperative and reactive 是指：\nQuarkus core基于Vert.X实现，是响应式的，这也是它能unify命令式和响应式的关键原因，也是各个扩展能自行选择命令式实现和响应式实现的原因 Quarkus在Vert.X基础上做了一个路由层来兼容两种实现，像HTTP请求这种直接通过路由层、如果是响应式代码，直接在I/O线程上执行；命令式代码则丢到Worker线程执行（就要做上下文切换了）；至于Quarkus怎么知道是什么实现，就看方法注解以及签名了，具体不展开 也就是说，至少在http接口这一层面，开发者可以选择用命令式还是响应式编程实现，另外定时任务、Eventbus之类也是支持的 但到了ORM层只能有一个选择，是的， Hibernate 和 Hibernate-reactive 显然不能共存。 如果选了 Hibernate-reactive 那么意味你的dao都是返回Uni或Multi，这会一直传递出去（除非await结果）。\n另外，像 resteasy 用了reative的话，filter这些在I/O线程运行，不能阻塞，如果里面用到redis client之类也要改Reactive RedisClient。\n总而言之，选了Reactive，多少是有 传递性 的，很多地方要跟着改。\n在我们的项目里，也开了个分支彻底改了Reactive，发现在一些地方支持还不是很好，比如 ：\n@Scheduled 方法里面开Panache的多表事务会报错 EventBus的 @ConsumeEvent 注释的方法，同上 Hibernate-reactive 不支持 OneToMany ManyToOne 等的懒加载，必须在HQL用 join fetch 声明拉外表（就不是lazy了） ……………… 最后，基于以下考虑，放弃了reactive：\n老代码迁移，不可能dao全部改reactive，况且dao层还会往外传递reactive quarkus reactive 本身限制，比如上面列的一些点 团队接受、理解Reactive的成本 virtual thread 已经进入 jdk19 preview了，到下个LTS，jdk21，quarkus应该已经跟进支持，到时切过去就是。 从长远来看，趋势应该是开发者只要写同步代码，不用管实际执行，通过虚拟线程压榨线程性能，大部分需求不需要在代码层面通过类似reactive这样的pattern提高性能。\n当然不是说有了Loom、reactive就没用了，只是说在压榨线程性能这一点上，可以交给loom；而我们用reactive也不只是为了性能，具体就不展开了，可以看看 The Reactive Manifesto。\n有一说一，Quarkus 的 mutiny对比RxJava之类，好用不是一点两点，接口设计相当优秀，值得学习。\n其他 OpenAPI 回顾下使用OpenAPI的目的：\n接口标准 提供给其他业务方/前端访问文档 提供给其他业务方自己按需生成SDK使用 第1点是OpenAPI本身的特性，第2点Quarkus可以集成。\nQuarkus对OpenAPI的支持可以见 官网Guides ，说几个点：\n支持按代码生成OpenAPI定义 支持按代码生成Swagger-UI界面（后者默认是dev模式才有，可以通过 quarkus.swagger-ui.always-include=true 开启非dev模式下可用） 支持放一个自己写的OpenAPI定义文件，作为上面1、2点生成的一部分；但，Quarkus要求openapi定义文件必须放在固定路径下，如resources/META-INF/openapi.yml 等几个指定路径，具体参见 Using OpenAPI And Swagger UI 。 第3点会带来一些不便，比如，OpenAPI spec文件在另一个项目，deploy到maven仓库，Quarkus的项目去依赖的时候，就要求它按这个目录来，如果OpenAPI spec是外部的项目，就不好控制了。\nRestClient Quarkus的 RestClient 基本功能是完备的，配合 quarkus-smallrye-fault-tolerance 可以做一些重试啊、Fallback啊、熔断啊之类的操作。\n但有两个点要吐槽下：\nRestClient是可以通过 @RegisterClientHeaders 注册一个请求header的处理器的；但这个处理器的生命周期是restclient自己管的，给一个输入的header，你可以往里加header之类，实际上能操作的空间的很小，一般就是只能加一些固定的header。所以很多时候只能在接口方法加 @HeaderParam 的参数，如果又有一些固定的header处理逻辑，要么就写一层Facade，要么就在接口里做个default方法处理下。 接口的异常处理定义比较弱，基本只能通过Interceptor、或者异常捕获；比如想根据响应http status code有不同处理，获取响应header这种，就比较麻烦。 Multipart 有一点比较坑的，@MultipartForm 注解有俩：\norg.jboss.resteasy.annotations.providers.multipart.MultipartForm org.jboss.resteasy.reactive.MultipartForm 还都是resteasy下的，注意哈，用第一个才行的，resteasy-multipart-provider 里面的。\nRedisClient 好几个方法，如 io.quarkus.redis.client.RedisClient#del ，没用变长参数，用的是List接收参数。 算是有点不方便吧。\n","date":"2022-05-15T16:03:10+08:00","image":"https://leibnizhu.github.io/p/%E5%85%B3%E4%BA%8EQuarkus%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/thumbnail_hu89cc325cede3e18952eee73574be4390_272638_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E5%85%B3%E4%BA%8EQuarkus%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/","title":"关于Quarkus的碎碎念"},{"content":"背景 去年写过一篇 跨Yarn集群提交spark任务 ，是在Spark2.2基础上做的动态提交外部Yarn集群。这里“动态”指不事先将 *-site.xml 打入jar包，而是执行任务时根据配置按需提交到对应集群；而“外部”集群是相对jar包中（如果已有）的 *-site.xml 对应的集群以外的集群，也是在“动态”提交的context中定义的，可以理解为提交到任意网络互通的集群。\n简单回顾下，主要做了两件事情：\n创建SparkContext前，将外部集群的 *-site.xml 放入classpath，如 $PWD 。 创建SparkContext前，HADOOP_CONF_DIR 和 YARN_CONF_DIR 环境变量改为外部集群 *-site.xml 配置文件所在位置；由于启动java程序后不能直接修改环境变量，在实现上使用了黑魔法。 时隔半年终于重拾博客，显然又被坑了，没错，之前的方法在Spark2.4里行不通了。\n问题、原因分析、及解决方案 Spark2.4中的报错 在原来代码基础上，升级Spark为2.4.8，执行提交到外部集群的任务，提交到Yarn的AM报错如下：\nContainer id: container_e36_1650338235135_41710_02_000001 Exit code: 1 Container exited with a non-zero exit code 1. Error file: prelaunch.err. Last 4096 bytes of prelaunch.err : Last 4096 bytes of stderr : Exception in thread \u0026#34;main\u0026#34; java.lang.IllegalArgumentException: java.net.UnknownHostException: channel at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:374) at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:312) at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:178) at org.apache.hadoop.hdfs.DFSClient.\u0026lt;init\u0026gt;(DFSClient.java:665) at org.apache.hadoop.hdfs.DFSClient.\u0026lt;init\u0026gt;(DFSClient.java:601) at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:148) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2619) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2653) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2635) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$8$$anonfun$apply$3.apply(ApplicationMaster.scala:219) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$8$$anonfun$apply$3.apply(ApplicationMaster.scala:217) at scala.Option.foreach(Option.scala:257) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$8.apply(ApplicationMaster.scala:217) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$8.apply(ApplicationMaster.scala:182) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:780) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.spark.deploy.yarn.ApplicationMaster.doAsUser(ApplicationMaster.scala:779) at org.apache.spark.deploy.yarn.ApplicationMaster.\u0026lt;init\u0026gt;(ApplicationMaster.scala:182) at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:803) at org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:834) at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala) Caused by: java.net.UnknownHostException: xxx ... 25 more 其中 xxx 是外部集群的集群名（dfs.nameservices 配置）。\n直接原因分析 仔细观察异常的调用栈，调用到了 NameNodeProxies.createNonHAProxy ，而我们的集群是HA的，显然是读取到的配置不对了。\n看到这个类，阅读过hadoop源码的应该都知道，这是创建 DFSClient 的时候，会先读取 dfs.client.failover.proxy.provider.{hdfs路径对应host} 配置（取值是一个 FailoverProxyProvider 具体实现的全限定类名），反射出Class对象并实例化，然后创建对应的HAProxy；而如果配置为空，则认为NameNode没有开启HA，直接将hdfs路径当作普通host来进行读取，如果实际上这个host是一个HA的nameservices名，不存在这个host，则会报上面的错误。\n所以可以确定，是AM读取不到正确的hdfs配置导致的。那么是为什么呢？\n仔细观察AM的日志，launch_container.sh 里面：\n#……………… export HADOOP_YARN_HOME=${HADOOP_YARN_HOME:-\u0026#34;/usr/hdp/2.6.5.0-292/hadoop-yarn\u0026#34;} export CLASSPATH=\u0026#34;$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:$HADOOP_CONF_DIR:$PWD/__spark_conf__/__hadoop_conf__\u0026#34; export SPARK_CONF_DIR=\u0026#34;/opt/package/spark-2.4.8-bin-hadoop2.6/conf\u0026#34; #……………… 同时注意到 directory.info 记录的目录结构：\nls -l: total 32 -rw-r--r-- 1 yarn hadoop 71 May 12 21:18 container_tokens -rwx------ 1 yarn hadoop 712 May 12 21:18 default_container_executor_session.sh -rwx------ 1 yarn hadoop 766 May 12 21:18 default_container_executor.sh -rwx------ 1 yarn hadoop 5787 May 12 21:18 launch_container.sh lrwxrwxrwx 1 yarn hadoop 80 May 12 21:18 __spark_conf__ -\u0026gt; /path/to/filecache/29549/__spark_conf__.zip drwxr-xr-x 2 yarn hadoop 4096 May 12 21:18 __spark_libs__ drwx--x--- 2 yarn hadoop 4096 May 12 21:18 tmp find -L . -maxdepth 5 -ls: 204734730 4 drwx--x--- 4 yarn hadoop 4096 May 12 21:18 . 204734738 4 -rwx------ 1 yarn hadoop 766 May 12 21:18 ./default_container_executor.sh 204734734 8 -rwx------ 1 yarn hadoop 5787 May 12 21:18 ./launch_container.sh 204734733 4 -rw-r--r-- 1 yarn hadoop 12 May 12 21:18 ./.container_tokens.crc 204734741 4 drwxr-xr-x 2 yarn hadoop 4096 May 12 21:18 ./__spark_libs__ 105382561 555832 -r-xr-xr-x 1 yarn hadoop 569170427 May 12 21:18 ./__spark_libs__/mySparkApp.jar 204734731 4 drwx--x--- 2 yarn hadoop 4096 May 12 21:18 ./tmp 204734735 4 -rw-r--r-- 1 yarn hadoop 56 May 12 21:18 ./.launch_container.sh.crc 204734739 4 -rw-r--r-- 1 yarn hadoop 16 May 12 21:18 ./.default_container_executor.sh.crc 204734732 4 -rw-r--r-- 1 yarn hadoop 71 May 12 21:18 ./container_tokens 204734736 4 -rwx------ 1 yarn hadoop 712 May 12 21:18 ./default_container_executor_session.sh 204734737 4 -rw-r--r-- 1 yarn hadoop 16 May 12 21:18 ./.default_container_executor_session.sh.crc 105120101 4 drwx------ 3 yarn hadoop 4096 May 12 21:18 ./__spark_conf__ 105120108 4 -r-x------ 1 yarn hadoop 3063 May 12 21:18 ./__spark_conf__/__spark_conf__.properties 105120107 120 -r-x------ 1 yarn hadoop 120306 May 12 21:18 ./__spark_conf__/__spark_hadoop_conf__.xml 105120102 4 drwx------ 2 yarn hadoop 4096 May 12 21:18 ./__spark_conf__/__hadoop_conf__ 105120103 20 -r-x------ 1 yarn hadoop 19814 May 12 21:18 ./__spark_conf__/__hadoop_conf__/yarn-site.xml 105120104 8 -r-x------ 1 yarn hadoop 4282 May 12 21:18 ./__spark_conf__/__hadoop_conf__/core-site.xml 105120106 20 -r-x------ 1 yarn hadoop 19567 May 12 21:18 ./__spark_conf__/__hadoop_conf__/hive-site.xml 105120105 12 -r-x------ 1 yarn hadoop 8312 May 12 21:18 ./__spark_conf__/__hadoop_conf__/hdfs-site.xml broken symlinks(find -L . -maxdepth 5 -type l -ls): ./__spark_conf__/__hadoop_conf__/ 里面是外部集群配置文件，而 ./__spark_libs__/mySparkApp.jar 是spark应用的jar，里面已经有原集群的配置文件。按 CLASSPATH 定义的顺序，Configuration 读取默认资源 core-site.xml 、 hdfs-site.xml （由HdfsConfiguration静态代码块加入）的时候，优先从 ./__spark_libs__/mySparkApp.jar 读取了，而真正要用的外部集群配置，由于在 CLASSPATH 中位置较后，不会被加载到。\n解决方案 知道问题的原因后，根据 CLASSPATH 定义的顺序：\n$PWD 里面的文件无法控制，跳过 $PWD/__spark_conf__ 目录里面是Driver的SparkConf内容 __spark_conf__.properties ，及所有hadoop相关配置整合到一起的的 __spark_hadoop_conf__.xml ，也是无法控制的。注意这个 __spark_hadoop_conf__.xml 里面虽然已经由Driver打入了外部集群的配置，但由于文件名不是 hdfs-site.xml ，不会被 Configuration 加载的。 $PWD/__spark_libs__/* 这里面就是我们的jar包，目前里面有原集群的配置文件，这其实也违反了 12-Factor 的 Config 。 中间两个忽略 $PWD/__spark_conf__/__hadoop_conf__ 就是外部集群配置文件所在 那么解决方案也很简单了：\nspark应用jar包里不要放任何 `*-site.xml` 配置文件 考虑到我们的Spark应用是用maven的shade插件打包的，可以配置为跳过这些xml即可：\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;filters\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;yarn-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;hdfs-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;core-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;hbase-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;hive-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;kms-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;mapred-site.xml\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/filters\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 重新打包、运行任务，顺利执行。\nSpark2.2 与 Spark2.4 Yarn-Client 模式提交任务差异 AM的classpath、目录结构差异 问题解决了，那么为什么Spark2.2升级Spark2.4之后就有这样的问题呢？从上面的分析，不难猜测到是AM的 CLASSPATH 变了。随便找一个Spark2.2提交的任务也可以看到：\n#### launch_container.sh export CLASSPATH=\u0026#34;$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:$HADOOP_CONF_DIR\u0026#34; ## 对比 Spark2.4的： #export CLASSPATH=\u0026#34;$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:$HADOOP_CONF_DIR:$PWD/__spark_conf__/__hadoop_conf__\u0026#34; #### directory.info find -L . -maxdepth 5 -ls: 6554176 4 drwx--x--- 4 yarn hadoop 4096 May 13 11:31 . 6554177 4 -rw-r--r-- 1 yarn hadoop 69 May 13 11:31 ./container_tokens 6554182 4 -rw-r--r-- 1 yarn hadoop 16 May 13 11:31 ./.default_container_executor_session.sh.crc 6816116 4 drwxr-xr-x 2 yarn hadoop 4096 May 13 11:31 ./__spark_libs__ 32768042 559764 -r-xr-xr-x 1 yarn hadoop 573191196 May 13 10:44 ./__spark_libs__/titanServEtl.jar 6554180 4 -rw-r--r-- 1 yarn hadoop 52 May 13 11:31 ./.launch_container.sh.crc 31457924 4 drwx------ 2 yarn hadoop 4096 May 13 11:31 ./__spark_conf__ 31457928 20 -r-x------ 1 yarn hadoop 19371 May 13 11:31 ./__spark_conf__/hive-site.xml 31457926 4 -r-x------ 1 yarn hadoop 3064 May 13 11:31 ./__spark_conf__/core-site.xml 31457925 20 -r-x------ 1 yarn hadoop 17378 May 13 11:31 ./__spark_conf__/yarn-site.xml 31457929 4 -r-x------ 1 yarn hadoop 2473 May 13 11:31 ./__spark_conf__/__spark_conf__.properties 31457927 8 -r-x------ 1 yarn hadoop 8009 May 13 11:31 ./__spark_conf__/hdfs-site.xml 可以看到，Spark2.4对比Spark2.2:\nAM 执行任务的目录： 将 *-site.xml 配置文件独立放入了 ./__spark_conf__/__hadoop_conf__ 目录，而非原来的 ./__spark_conf__/ 目录 多了一个 ./__spark_conf__/__spark_hadoop_conf__.xml 文件，存放了所有hadoop相关配置 CLASSPATH 环境变量里将存放 *-site.xml 配置文件的 $PWD/__spark_conf__/__hadoop_conf__ 目录放到了最后面。 以上两个原因共同导致了本文的错误发生。\n附目录对比截图：\nSpark源码里的体现 上篇博客里提到Spark的yarn-client模式是通过 YarnClientSchedulerBackend 处理的。\n其 start() 方法会调用 org.apache.spark.deploy.yarn.Client 的 submitApplication() 方法提交Yarn AM。\nsubmitApplication() 调用 createContainerLaunchContext 构造ContainerLaunchContext对应的上下文，构建的启动Yarn AM的任务命令cmds，里面比较重要的有两步:\n调用 setupLaunchEnv() 构造环境变量，其中我们关心的 CLASSPATH 是在 populateClasspath() 方法里处理的； 调用 prepareLocalResources() 准备Yarn AM需要的一些资源，包括调用 createConfArchive() 创建 __spark_conf__.zip ，里面解压出来就是上面所讨论的AM 目录结构里面的 ./__spark_conf__/ 目录 populateClasspath() 对比两个版本的 populateClasspath() 方法，注意差异在最后：\n参考注释：\n// Add the localized Hadoop config at the end of the classpath, in case it contains other // files (such as configuration files for different services) that are not part of the // YARN cluster\u0026#39;s config. 是为了防止将其他非Yarn集群配置的文件也引入了。\ncreateConfArchive() 这个代码略多，挑一些重点的讲讲，以Spark2.4为基准。\nSPARK-23630 增加了一个用于测试的环境变量 SPARK_TEST_HADOOP_CONF_DIR ，该环境变量指定的目录里面的配置文件也会被打进去 __spark_conf__.zip 。\n// SPARK-23630: during testing, Spark scripts filter out hadoop conf dirs so that user\u0026#39;s // environments do not interfere with tests. This allows a special env variable during // tests so that custom conf dirs can be used by unit tests. val confDirs = Seq(\u0026#34;HADOOP_CONF_DIR\u0026#34;, \u0026#34;YARN_CONF_DIR\u0026#34;) ++ (if (Utils.isTesting) Seq(\u0026#34;SPARK_TEST_HADOOP_CONF_DIR\u0026#34;) else Nil) hadoop配置文件独立出来，放在 __hadoop_conf__ 目录。\n// Save the Hadoop config files under a separate directory in the archive. This directory // is appended to the classpath so that the cluster-provided configuration takes precedence. confStream.putNextEntry(new ZipEntry(s\u0026#34;$LOCALIZED_HADOOP_CONF_DIR/\u0026#34;)) confStream.closeEntry() hadoopConfFiles.foreach { case (name, file) =\u0026gt; if (file.canRead()) { confStream.putNextEntry(new ZipEntry(s\u0026#34;$LOCALIZED_HADOOP_CONF_DIR/$name\u0026#34;)) Files.copy(file, confStream) confStream.closeEntry() } } 增加了一个 __spark_hadoop_conf__.xml 存放所有hadoop配置。\n//Client 里面的代码 private val hadoopConf = new YarnConfiguration(SparkHadoopUtil.newConfiguration(sparkConf)) //createConfArchive() 里面的代码 // Save the YARN configuration into a separate file that will be overlayed on top of the // cluster\u0026#39;s Hadoop conf. confStream.putNextEntry(new ZipEntry(SparkHadoopUtil.SPARK_HADOOP_CONF_FILE)) hadoopConf.writeXml(confStream) confStream.closeEntry() ","date":"2022-05-14T20:33:58+08:00","image":"https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1%E4%B9%8BSpark2.4%E5%9D%91/blackcat_hu988cee6e86777a2a931b84d96a10c67f_110902_120x120_fill_q75_box_smart1.jpeg","permalink":"https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1%E4%B9%8BSpark2.4%E5%9D%91/","title":"跨Yarn集群提交spark任务——之Spark2.4坑"},{"content":"背景 之前写过一篇 Spark动态加载hive配置的方案 ，当时是为了spark应用的fat-jar里面已经有Hadoop相关xml配置文件的情况下，将数据输出到不是该配置的Hadoop集群的方案。\n现在这个需求有点类似，没有走spark-submit提交任务，而是在spark应用里面通过创建SparkContext的形式提交任务，而spark应用的fat-jar里面已经有Hadoop相关xml配置文件，在此情况下，想将Spakr任务提交到外部的Yarn集群（不是fat-jar里面配置文件对应的yarn集群）。\n思考一个问题 先思考一个问题，如果Spark应用的fat-jar里面有外部Yarn集群对应的配置文件(core-site.xml，hdfs-site.xml，yarn-site.xml等)，此时Spark应用代码里面创建SparkContext，是不是就一定能提交到那个集群里？\n可以做个实验，但实验不一定会cover到所有情况。\n直接给结论吧，不一定能提交过去，但自己做实验的话很可能还是能直接提交过去的，还是直接看代码吧（以yarn-client模式为例）。\nSpark Yarn-client 默认提交任务简析 通过代码创建SparkContext后，其动态代码块会根据启动模式创建SchedulerBackend和TaskScheduler并启动：\n// org.apache.spark.SparkContext #501 // Create and start the scheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode) _schedulerBackend = sched _taskScheduler = ts _dagScheduler = new DAGScheduler(this) // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler\u0026#39;s // constructor _taskScheduler.start() 其中 TaskScheduler 是通过 org.apache.spark.scheduler.cluster.YarnClusterManager#createTaskScheduler 创建的，对应 yarn-client 创建的是YarnScheduler（继承了TaskSchedulerImpl），start()方法调用到SchedulerBackend的start方法，后者就会创建yarn模式下的Client客户端（org.apache.spark.deploy.yarn.Client，不是yarn自己那个client），并调用其submitApplication方法提交任务到Yarn：\n//org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend#start override def start() { val driverHost = conf.get(\u0026#34;spark.driver.host\u0026#34;) val driverPort = conf.get(\u0026#34;spark.driver.port\u0026#34;) val hostport = driverHost + \u0026#34;:\u0026#34; + driverPort sc.ui.foreach { ui =\u0026gt; conf.set(\u0026#34;spark.driver.appUIAddress\u0026#34;, ui.webUrl) } val argsArrayBuf = new ArrayBuffer[String]() argsArrayBuf += (\u0026#34;--arg\u0026#34;, hostport) logDebug(\u0026#34;ClientArguments called with: \u0026#34; + argsArrayBuf.mkString(\u0026#34; \u0026#34;)) val args = new ClientArguments(argsArrayBuf.toArray) totalExpectedExecutors = YarnSparkHadoopUtil.getInitialTargetExecutorNumber(conf) client = new Client(args, conf) bindToYarn(client.submitApplication(), None) //……………… } 初始化Client的时候，会创建YarnConfiguration，此时就会读取到Configuration里面配置的默认资源，包括yarn-site.xml等；如果fatjar里面放的是外部集群的配置文件，那么对应的YarnClient就可以连接到外部Yarn集群。\n//org.apache.spark.deploy.yarn.Client private[spark] class Client( val args: ClientArguments, val hadoopConf: Configuration, val sparkConf: SparkConf) extends Logging { //……………… private val yarnClient = YarnClient.createYarnClient private val yarnConf = new YarnConfiguration(hadoopConf) 接着刚才说到SchedulerBackend调用Client的submitApplication方法:\n//org.apache.spark.deploy.yarn.Client#submitApplication def submitApplication(): ApplicationId = { var appId: ApplicationId = null try { launcherBackend.connect() // Setup the credentials before doing anything else, // so we have don\u0026#39;t have issues at any point. setupCredentials() yarnClient.init(yarnConf) yarnClient.start() logInfo(\u0026#34;Requesting a new application from cluster with %d NodeManagers\u0026#34; .format(yarnClient.getYarnClusterMetrics.getNumNodeManagers)) // Get a new application from our RM //新建一个Application val newApp = yarnClient.createApplication() val newAppResponse = newApp.getNewApplicationResponse() appId = newAppResponse.getApplicationId() new CallerContext(\u0026#34;CLIENT\u0026#34;, sparkConf.get(APP_CALLER_CONTEXT), Option(appId.toString)).setCurrentContext() // Verify whether the cluster has enough resources for our AM verifyClusterResources(newAppResponse) // Set up the appropriate contexts to launch our AM //创建environment, java options以及启动AM的命令 val containerContext = createContainerLaunchContext(newAppResponse) //创建提交AM的Context，包括名字、队列、类型、内存、CPU及参数 val appContext = createApplicationSubmissionContext(newApp, containerContext) // Finally, submit and monitor the application logInfo(s\u0026#34;Submitting application $appId to ResourceManager\u0026#34;) //向Yarn提交Application yarnClient.submitApplication(appContext) launcherBackend.setAppId(appId.toString) reportLauncherState(SparkAppHandle.State.SUBMITTED) appId } catch { case e: Throwable =\u0026gt; if (appId != null) { cleanupStagingDir(appId) } throw e } } 其中 createContainerLaunchContext 会创建environment, java options以及启动AM的命令等，也会收集本地资源（prepareLocalResources方法），其中包括__spark_conf__.zip，在createConfArchive方法中处理，压缩了本地的一些配置文件：\nprivate def createConfArchive(): File = { val hadoopConfFiles = new HashMap[String, File]() // Uploading $SPARK_CONF_DIR/log4j.properties file to the distributed cache to make sure that // the executors will use the latest configurations instead of the default values. This is // required when user changes log4j.properties directly to set the log configurations. If // configuration file is provided through --files then executors will be taking configurations // from --files instead of $SPARK_CONF_DIR/log4j.properties. // Also uploading metrics.properties to distributed cache if exists in classpath. // If user specify this file using --files then executors will use the one // from --files instead. for { prop \u0026lt;- Seq(\u0026#34;log4j.properties\u0026#34;, \u0026#34;metrics.properties\u0026#34;) url \u0026lt;- Option(Utils.getContextOrSparkClassLoader.getResource(prop)) if url.getProtocol == \u0026#34;file\u0026#34; } { hadoopConfFiles(prop) = new File(url.getPath) } Seq(\u0026#34;HADOOP_CONF_DIR\u0026#34;, \u0026#34;YARN_CONF_DIR\u0026#34;).foreach { envKey =\u0026gt; sys.env.get(envKey).foreach { path =\u0026gt; val dir = new File(path) if (dir.isDirectory()) { val files = dir.listFiles() if (files == null) { logWarning(\u0026#34;Failed to list files under directory \u0026#34; + dir) } else { files.foreach { file =\u0026gt; if (file.isFile \u0026amp;\u0026amp; !hadoopConfFiles.contains(file.getName())) { hadoopConfFiles(file.getName()) = file } } } } } } val confArchive = File.createTempFile(LOCALIZED_CONF_DIR, \u0026#34;.zip\u0026#34;, new File(Utils.getLocalDir(sparkConf))) val confStream = new ZipOutputStream(new FileOutputStream(confArchive)) //后面就是把这些文件写入到zip包的代码，略 } 可以看到，除了本地的log4j.properties和metrics.properties配置文件以外，还会读取HADOOP_CONF_DIR和YARN_CONF_DIR环境变量，读取对应目录下的文件放入hadoopConfFiles这个HashMap中，而这里面的文件都会压缩到__spark_conf__.zip中。\n再后续的代码就不分析了，可以参考网上其他文章。\n提交外部Yarn集群的障碍 所以，如果执行spark应用程序的机器中配置了 HADOOP_CONF_DIR 或 YARN_CONF_DIR 环境变量（如HDP的节点安装了对应客户端都会配置上），在Spark提交任务到外部yarn集群的时候，就会将里面的配置文件压缩传输到外部集群的Executor节点，这样Executor的各种操作都会使用原集群的配置，连接不到正确的Yarn服务，最后也就导致任务执行失败。\n解决方案 所以解决整个提交外部集群的问题，有两个问题要处理：\nSpark应用代码使用外部集群的配置文件进行任务提交 一种方案是启动Spark应用后，创建SparkContext之前，将外部集群的配置写入当前classpath的前面（如classpath是.:xxx.jar，那么放在当前目录就可以） 另一种方案是启动Spark应用前，将外部集群的配置写入当前目录，并通过jar uvf打入jar包中；当然只是针对当前问题的话，无需打入jar包 Spark准备executor的资源时，使用外部集群配置文件 一种方案是，创建SparkContext之前，将HADOOP_CONF_DIR和YARN_CONF_DIR环境变量删除，提交任务后再恢复环境变量；这样不会把集群配置传给Executor，Executor使用的是fatjar包里面的配置文件，需要提前替换。 另一种方案是，将外部集群的配置写入一个目录，并在创建SparkContext之前，将HADOOP_CONF_DIR和YARN_CONF_DIR环境变量改为那个目录；这样正确的配置会传给Executor使用。 结合起来最终的方案：\n外部集群的配置文件统一一个地方存储，可以直接存储在RDB。 启动Spark应用的时候，检查需要提交到的Yarn集群，如果是外部集群，那么： 下载外部集群的配置文件到当前目录，同时复制到一个子目录里面 将HADOOP_CONF_DIR和YARN_CONF_DIR环境变量改为那个子目录（不能用当前目录，因为当前目录包含fat-jar，根据代码jar包也会打包过去Executor） 正常创建SparkContext 恢复环境变量 具体实现不外乎一些黑魔法（环境变量在JVM里面修改不了，但可以修改JVM用到的那个环境变量Map），再考虑下要不要放上来吧，反正这个最主要是思路和里面的坑。\n","date":"2021-12-04T18:17:55+08:00","image":"https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/93990522_hu6aa0acda77cb3b4cc5d51f0a61452bcb_184592_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E8%B7%A8Yarn%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BA%A4spark%E4%BB%BB%E5%8A%A1/","title":"跨Yarn集群提交spark任务"},{"content":"故障背景 通过 ES-Hadoop （亦可参考 Github ） 查询ES时，若查询条件包含emoji，会在json序列化的时候抛出异常（在最新的 7.13.2 版本仍存在）：\nCaused by: org.codehaus.jackson.JsonGenerationException: Incomplete surrogate pair: first char 0xde97, second 0x22 at org.codehaus.jackson.impl.JsonGeneratorBase._reportError(JsonGeneratorBase.java:480) at org.codehaus.jackson.impl.Utf8Generator._decodeSurrogate(Utf8Generator.java:1708) at org.codehaus.jackson.impl.Utf8Generator._outputSurrogates(Utf8Generator.java:1663) at org.codehaus.jackson.impl.Utf8Generator._outputRawMultiByteChar(Utf8Generator.java:1649) at org.codehaus.jackson.impl.Utf8Generator.writeRaw(Utf8Generator.java:757) at org.codehaus.jackson.impl.Utf8Generator.writeRaw(Utf8Generator.java:697) at org.elasticsearch.hadoop.serialization.json.JacksonJsonGenerator.writeRaw(JacksonJsonGenerator.java:252) ... 21 more 故障根源分析 根据报错的调用栈，直接原因出在 org.codehaus.jackson.impl.Utf8Generator#_decodeSurrogate 方法中，其源码：\nprotected final int _decodeSurrogate(int surr1, int surr2) throws IOException { if (surr2 \u0026lt; 56320 || surr2 \u0026gt; 57343) { String msg = \u0026#34;Incomplete surrogate pair: first char 0x\u0026#34; + Integer.toHexString(surr1) + \u0026#34;, second 0x\u0026#34; + Integer.toHexString(surr2); this._reportError(msg); } int c = 65536 + (surr1 - \u0026#39;\\ud800\u0026#39; \u0026lt;\u0026lt; 10) + (surr2 - \u0026#39;\\udc00\u0026#39;); return c; } 是判断第二个字符在指定范围（[56320, 57343] 区间）外就报这个错误。\n同时注意，这里用的是 org.codehaus 的 jackson-core-asl 依赖包，众所周知 ，Jackson 自2.x版本开始就迁移到 com.fasterxml 下了，这个 org.codehaus 的 jackson-core-asl 是1.x版本的（Es-Spark通过打包时改第三方包名的方法将Jackson 打进其jar包中，具体参见 build.gradle文件的relocate操作 ，实际版本为 1.8.8 ）。\n针对这个报错，可以查到是一个已经 在2.3.0修复的bug ，是旧版本不完全支持UTF-8的 surrogate pair （这里又是一个兼容性的大坑，可以参见 维基百科 的介绍）导致的。\n综上所述，Es-Spark 在执行查询的时候，org.elasticsearch.hadoop.rest.RestClient#searchRequest 方法构建了 org.elasticsearch.hadoop.serialization.json.JacksonJsonGenerator 实例，并将 QueryBuilder 写入到 JacksonJsonGenerator 中序列化成查询json，在这一步中由于使用了 Jackson 1.x 版本对UTF8的emoji支持不好，导致抛出 JsonGenerationException 异常、中断查询。\n//RestClient 某查询方法 xxx queryXxx(......) { //...... Response response = execute(POST, uri.toString(), searchRequest(query)); //...... } static BytesArray searchRequest(QueryBuilder query) { FastByteArrayOutputStream out = new FastByteArrayOutputStream(256); JacksonJsonGenerator generator = new JacksonJsonGenerator(out); //注意此处 try { generator.writeBeginObject(); generator.writeFieldName(\u0026#34;query\u0026#34;); generator.writeBeginObject(); query.toJson(generator); generator.writeEndObject(); generator.writeEndObject(); } finally { generator.close(); } return out.bytes(); } 解决方案 故障分析到这里，似乎只要升级 jackson-core 版本就可以了……\n然而上面提到，在 jackson-core 2.3.0 才修复了这个问题，而 Es-Spark 使用的是内置的1.x 版本，前面也有提到 jackson-core 自2.x开始迁移到 com.fasterxml 。具体到代码，Es-spark 的 JacksonJsonGenerator 是这样使用 jackson 的：\npackage org.elasticsearch.hadoop.serialization.json; import org.codehaus.jackson.JsonEncoding; import org.codehaus.jackson.JsonFactory; import org.codehaus.jackson.JsonGenerator; import org.elasticsearch.hadoop.serialization.Generator; public class JacksonJsonGenerator implements Generator { //省略部分字段 private static final JsonFactory JSON_FACTORY; private final JsonGenerator generator; private final OutputStream out; static { //省略部分代码 JSON_FACTORY = new JsonFactory(); JSON_FACTORY.configure(JsonGenerator.Feature.QUOTE_FIELD_NAMES, true); } //RestClient 就是调用这个构造方法 public JacksonJsonGenerator(OutputStream out) { try { this.out = out; // use dedicated method to lower Jackson requirement this.generator = JSON_FACTORY.createJsonGenerator(out, JsonEncoding.UTF8); } catch (IOException ex) { throw new EsHadoopSerializationException(ex); } } } 也就是说，直接升级依赖版本是不行的，maven的GAV都变了，类名也变了，必须改代码。\n同名类的Patch 可以看到，虽说 Jackson 迁移了包名，但如果是通过创建同名类的方式Patch，其实也很简单，只要把 JSON_FACTORY 和 generator 这个两个属性替换为 Jackson 2.3+ 版本的类、并微调静态代码块和构造方法里面的代码即可：\npackage org.elasticsearch.hadoop.serialization.json; import com.fasterxml.jackson.core.JsonEncoding; import com.fasterxml.jackson.core.JsonGenerator; import com.fasterxml.jackson.core.JsonFactory; import org.elasticsearch.hadoop.serialization.Generator; public class JacksonJsonGenerator implements Generator { //省略部分字段 private static final JsonFactory JSON_FACTORY; private final JsonGenerator generator; private final OutputStream out; static { //省略部分代码 JSON_FACTORY = new JsonFactory(); JSON_FACTORY.configure(JsonGenerator.Feature.QUOTE_FIELD_NAMES, true); } public JacksonJsonGenerator(OutputStream out) { try { this.out = out; // use dedicated method to lower Jackson requirement this.generator = JSON_FACTORY.createJsonGenerator(out, JsonEncoding.UTF8); } catch (IOException ex) { throw new EsHadoopSerializationException(ex); } } } Javassist Patch 与上一篇博客一样，为了可维护性，最后还是选择使用 Javassist 进行Patch。但受限于 Javassist 的机制，这个Patch起来有点麻烦。\n首先，阅读 JacksonJsonGenerator 源码，它其实相当于是在 Es-spark 的 Generator 接口与 jackson 1.8.8 的 JsonGenerator 之间做了Adaptor；因此可以考虑写一个 Generator 接口与 jackson 2.3+ 之间的Adaptor给原调用者使用。\n但阅读 Es-spark 的其他代码可以发现，虽然它定义了 Generator 接口，但调用时都是直接面向 JacksonJsonGenerator 实现类，如上面给出过的 RestClient#searchRequest 的代码：\n//RestClient 某查询方法 static BytesArray searchRequest(QueryBuilder query) { //...... JacksonJsonGenerator generator = new JacksonJsonGenerator(out); //注意此处 //...... } 因此首先排除了通过修改 JacksonJsonGenerator 调用者来Patch的方向，还是需要从 JacksonJsonGenerator 内部入手。\n如果用javassist修改JacksonJsonGenerator，参考上一小节的内容，其实只要改俩成员变量的类型，再改改静态代码块即可。但真的如此吗？并不。写同名类能这样做到是因为会整个类重新编译，JacksonJsonGenerator中大量delegate的方法在编译时是用 Jackson 2.3+ 的类进行连接的；然而javassist修改成员变量的时候真的只是修改了成员变量本身，如果只改了这里，对应的delegate方法在运行时会找不到成员变量。\n如果是在静态代码块和构造方法新增 Jackson 2.3+ 对应的类，并给 writeRaw 方法增加try-catch，在catch中使用 Jackson 2.3+ 对应的类进行json序列化呢？也不行。因为序列化是输出到OutputStream（构造方法传入的那个），是有状态的，同时给jackson 1.8.8和jackson 2.3+持有并写入，恐怕会大乱（实测的确如此，不确定是不是没处理好flush，但至少这个方案太危险）。\n还有一个方案是替换 JacksonJsonGenerator 的 generator 成员变量，可以做一个 org.codehaus.jackson.JsonGenerator 与 Jackson 2.3+ 的 com.fasterxml.jackson.core.JsonGenerator 之间的Adaptor来替换之。\n首先是Adaptor：\npackage xxx.yyy.zzz; import com.fasterxml.jackson.core.JsonEncoding; import com.fasterxml.jackson.core.JsonFactory; import com.fasterxml.jackson.core.JsonGenerator; import org.apache.commons.logging.LogFactory; import org.codehaus.jackson.*; import org.elasticsearch.hadoop.serialization.EsHadoopSerializationException; import org.elasticsearch.hadoop.util.StringUtils; import java.io.IOException; import java.io.OutputStream; import java.math.BigDecimal; import java.math.BigInteger; import java.util.Deque; import java.util.LinkedList; public class JacksonJsonGeneratorAdaptor extends org.codehaus.jackson.JsonGenerator { private static final boolean HAS_UTF_8; private static final JsonFactory JSON_FACTORY; private final JsonGenerator generator; private final OutputStream out; private final Deque\u0026lt;String\u0026gt; currentPath = new LinkedList\u0026lt;\u0026gt;(); private String currentPathCached; private String currentName; protected ObjectCodec _objectCodec; static { boolean hasMethod = false; try { JsonGenerator.class.getMethod(\u0026#34;writeUTF8String\u0026#34;, byte[].class, int.class, int.class); hasMethod = true; } catch (NoSuchMethodException ignored) { } HAS_UTF_8 = hasMethod; if (!HAS_UTF_8) { LogFactory.getLog(JacksonJsonGeneratorAdaptor.class).warn(\u0026#34;Old Jackson version (pre-1.7) detected; consider upgrading to improve performance\u0026#34;); } JSON_FACTORY = new JsonFactory(); JSON_FACTORY.configure(JsonGenerator.Feature.QUOTE_FIELD_NAMES, true); } public JacksonJsonGeneratorAdaptor(OutputStream out) { try { this.out = out; // use dedicated method to lower Jackson requirement this.generator = JSON_FACTORY.createGenerator(out, JsonEncoding.UTF8); } catch (IOException ex) { throw new EsHadoopSerializationException(ex); } } //省略大量delegate方法，只列出不是简单delegate的 @Override public void writeStartObject() throws IOException { generator.writeStartObject(); if (currentName != null) { currentPath.addLast(currentName); currentName = null; currentPathCached = null; } } @Override public void writeEndObject() throws IOException { generator.writeEndObject(); currentName = currentPath.pollLast(); currentPathCached = null; } @Override public void writeFieldName(String name) throws IOException { generator.writeFieldName(name); currentName = name; } @Override public void writeUTF8String(byte[] text, int offset, int len) throws IOException { if (HAS_UTF_8) { generator.writeUTF8String(text, offset, len); } else { generator.writeString(new String(text, offset, len, StringUtils.UTF_8)); } } @Override public void writeBinary(Base64Variant var1, byte[] data, int offset, int len) throws IOException { generator.writeBinary(data, offset, len); } @Override public void writeBinary(byte[] data) throws IOException { writeBinary(Base64Variants.getDefaultVariant(), data, 0, data.length); } @Override public void copyCurrentEvent(JsonParser jp) throws IOException { JsonToken t = jp.getCurrentToken(); if (t == null) { throw new JsonGenerationException(\u0026#34;No current event to copy\u0026#34;); } switch(t) { case START_OBJECT: this.writeStartObject(); break; case END_OBJECT: this.writeEndObject(); break; case START_ARRAY: this.writeStartArray(); break; case END_ARRAY: this.writeEndArray(); break; case FIELD_NAME: this.writeFieldName(jp.getCurrentName()); break; case VALUE_STRING: if (jp.hasTextCharacters()) { this.writeString(jp.getTextCharacters(), jp.getTextOffset(), jp.getTextLength()); } else { this.writeString(jp.getText()); } break; case VALUE_NUMBER_INT: switch(jp.getNumberType()) { case INT: this.writeNumber(jp.getIntValue()); return; case BIG_INTEGER: this.writeNumber(jp.getBigIntegerValue()); return; default: this.writeNumber(jp.getLongValue()); return; } case VALUE_NUMBER_FLOAT: switch(jp.getNumberType()) { case BIG_DECIMAL: this.writeNumber(jp.getDecimalValue()); return; case FLOAT: this.writeNumber(jp.getFloatValue()); return; default: this.writeNumber(jp.getDoubleValue()); return; } case VALUE_TRUE: this.writeBoolean(true); break; case VALUE_FALSE: this.writeBoolean(false); break; case VALUE_NULL: this.writeNull(); break; case VALUE_EMBEDDED_OBJECT: this.writeObject(jp.getEmbeddedObject()); break; default: throw new RuntimeException(\u0026#34;Internal error: should never end up through this code path\u0026#34;); } } @Override public void copyCurrentStructure(JsonParser jp) throws IOException { JsonToken t = jp.getCurrentToken(); if (t == JsonToken.FIELD_NAME) { this.writeFieldName(jp.getCurrentName()); t = jp.nextToken(); } switch(t) { case START_OBJECT: this.writeStartObject(); while(jp.nextToken() != JsonToken.END_OBJECT) { this.copyCurrentStructure(jp); } this.writeEndObject(); break; case START_ARRAY: this.writeStartArray(); while(jp.nextToken() != JsonToken.END_ARRAY) { this.copyCurrentStructure(jp); } this.writeEndArray(); break; default: this.copyCurrentEvent(jp); } } @Override public void close() { try { generator.close(); } catch (IOException ex) { throw new EsHadoopSerializationException(ex); } } @Override public Object getOutputTarget() { //return generator.getOutputTarget(); return out; } @Override public org.codehaus.jackson.JsonGenerator enable(Feature feature) { switch (feature) { case AUTO_CLOSE_TARGET: generator.enable(JsonGenerator.Feature.AUTO_CLOSE_TARGET); break; case AUTO_CLOSE_JSON_CONTENT: generator.enable(JsonGenerator.Feature.AUTO_CLOSE_JSON_CONTENT); break; case QUOTE_FIELD_NAMES: generator.enable(JsonGenerator.Feature.QUOTE_FIELD_NAMES); break; case QUOTE_NON_NUMERIC_NUMBERS: generator.enable(JsonGenerator.Feature.QUOTE_NON_NUMERIC_NUMBERS); break; case WRITE_NUMBERS_AS_STRINGS: generator.enable(JsonGenerator.Feature.WRITE_NUMBERS_AS_STRINGS); break; case FLUSH_PASSED_TO_STREAM: generator.enable(JsonGenerator.Feature.FLUSH_PASSED_TO_STREAM); break; case ESCAPE_NON_ASCII: generator.enable(JsonGenerator.Feature.ESCAPE_NON_ASCII); break; } return this; } @Override public org.codehaus.jackson.JsonGenerator disable(Feature feature) { switch (feature) { case AUTO_CLOSE_TARGET: generator.disable(JsonGenerator.Feature.AUTO_CLOSE_TARGET); break; case AUTO_CLOSE_JSON_CONTENT: generator.disable(JsonGenerator.Feature.AUTO_CLOSE_JSON_CONTENT); break; case QUOTE_FIELD_NAMES: generator.disable(JsonGenerator.Feature.QUOTE_FIELD_NAMES); break; case QUOTE_NON_NUMERIC_NUMBERS: generator.disable(JsonGenerator.Feature.QUOTE_NON_NUMERIC_NUMBERS); break; case WRITE_NUMBERS_AS_STRINGS: generator.disable(JsonGenerator.Feature.WRITE_NUMBERS_AS_STRINGS); break; case FLUSH_PASSED_TO_STREAM: generator.disable(JsonGenerator.Feature.FLUSH_PASSED_TO_STREAM); break; case ESCAPE_NON_ASCII: generator.disable(JsonGenerator.Feature.ESCAPE_NON_ASCII); break; } return this; } @Override public boolean isEnabled(Feature feature) { switch (feature) { case AUTO_CLOSE_TARGET: return generator.isEnabled(JsonGenerator.Feature.AUTO_CLOSE_TARGET); case AUTO_CLOSE_JSON_CONTENT: return generator.isEnabled(JsonGenerator.Feature.AUTO_CLOSE_JSON_CONTENT); case QUOTE_FIELD_NAMES: return generator.isEnabled(JsonGenerator.Feature.QUOTE_FIELD_NAMES); case QUOTE_NON_NUMERIC_NUMBERS: return generator.isEnabled(JsonGenerator.Feature.QUOTE_NON_NUMERIC_NUMBERS); case WRITE_NUMBERS_AS_STRINGS: return generator.isEnabled(JsonGenerator.Feature.WRITE_NUMBERS_AS_STRINGS); case FLUSH_PASSED_TO_STREAM: return generator.isEnabled(JsonGenerator.Feature.FLUSH_PASSED_TO_STREAM); case ESCAPE_NON_ASCII: return generator.isEnabled(JsonGenerator.Feature.ESCAPE_NON_ASCII); } return false; } @Override public org.codehaus.jackson.JsonGenerator setCodec(ObjectCodec objectCodec) { this._objectCodec = objectCodec; return this; } @Override public ObjectCodec getCodec() { return this._objectCodec; } } 然后通过javassist 修改JacksonJsonGenerator 的 generator 成员变量实际取值：\nClassPool pool = ClassPool.getDefault(); try { CtClass cc = pool.get(\u0026#34;org.elasticsearch.hadoop.serialization.json.JacksonJsonGenerator\u0026#34;); //这里必须用类全限定名 //这里自己造了一个无参构造器给原构造器调用，否则 JacksonJsonGenerator 的 currentPath 一直是null（字段有初始化值但还是null），原因未知，可能是setBody影响了原构造器的行为 cc.addConstructor(CtNewConstructor.make(\u0026#34;JacksonJsonGenerator(){this.currentPath = new java.util.LinkedList();}\u0026#34;, cc)); //构造函数将 generator 替换成我们的 Adaptor CtConstructor jacksonJsonGeneratorConstructor = cc.getDeclaredConstructor(new CtClass[]{pool.get(OutputStream.class.getName())}); jacksonJsonGeneratorConstructor.setBody(\u0026#34;{\\n\u0026#34; + \u0026#34; this();\\n\u0026#34; + //调用无参构造器，这里用 $0() 是不行的，必须this(); \u0026#34; try {\\n\u0026#34; + \u0026#34; $0.out = $1;\\n\u0026#34; + \u0026#34; $0.generator = new xxx.yyy.zzz.JacksonJsonGeneratorAdaptor($1);\\n\u0026#34; + \u0026#34; } catch (java.io.IOException ex) {\\n\u0026#34; + \u0026#34; throw new org.elasticsearch.hadoop.serialization.EsHadoopSerializationException(ex);\\n\u0026#34; + \u0026#34; }\\n\u0026#34; + \u0026#34; }\u0026#34;); cc.toClass().getConstructor(OutputStream.class).newInstance(System.out); log.info(\u0026#34;完成对 JacksonJsonGenerator 进行静态代码块和构造方法的pack\u0026#34;); } catch (Exception e) { log.error(\u0026#34;给 JacksonJsonGenerator 进行静态代码块和构造方法的pack失败:\u0026#34; + e.getMessage(), e); } 完事儿。\n","date":"2021-06-20T13:17:54+08:00","image":"https://leibnizhu.github.io/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E6%9F%A5%E8%AF%A2%E6%9D%A1%E4%BB%B6%E5%B8%A6emoji%E6%97%B6%E7%9A%84JsonGenerationException/bg1_hu45c6181425d2bec34f45ee6e4ec4a6e2_647451_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E6%9F%A5%E8%AF%A2%E6%9D%A1%E4%BB%B6%E5%B8%A6emoji%E6%97%B6%E7%9A%84JsonGenerationException/","title":"修复Elasticsearch-hadoop查询条件带emoji时的JsonGenerationException"},{"content":"故障背景 众所周知，Elasticsearch（下文简称\u0026quot;ES\u0026quot;）的数值类型字段是支持一些特殊格式的。比如，integer 类型的字段取值可以是浮点数的字符串，如 \u0026quot;2.0\u0026quot;；long 类型的字段取值可以是科学计数法的字符串，如 \u0026quot;2E3\u0026quot;，诸如此类的一些。不同于时间类型的字段可以 通过 mapping 的 format 属性配置取值格式 ，数值字段的取值没有格式的配置、上面举例的取值都是可以直接索引文档的。\n然而，通过 ES-Hadoop （亦可参考 Github ） 查询ES时，这些特殊格式的取值往往会导致报错，如：\njava.lang.NumberFormatException: For input string: \u0026#39;2E3\u0026#39; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:441) at java.lang.Long.parseLong(Long.java:483) at org.elasticsearch.hadoop.serialization.builder.JdkValueReader.parseLong(JdkValueReader.java:296) at org.elasticsearch.hadoop.serialization.builder.JdkValueReader.longValue(JdkValueReader.java:288) ………… 故障根源分析 阅读 JdkValueReader 源码，以读取integer类型字段为例：\nprotected Object intValue(String value, Parser parser) { Integer val = null; if (value == null || isEmpty(value)) { return nullValue(); } else { Token tk = parser.currentToken(); if (tk == Token.VALUE_NUMBER) { val = parser.intValue(); } else { val = parseInteger(value); } } return processInteger(val); } protected Integer parseInteger(String value) { return Integer.parseInt(value); } 可以看到字段取值直接调用 Integer.parseInt 方法解析，且没捕获异常。\n不知道这样设计是处于什么考虑，但这个 NumberFormatException 异常会打断读取的流程：出现一条有问题的数据时，会影响整个查询任务的执行，在 Es-spark 使用于离线批处理的场景，是不恰当的，所以有必要进行调整。\n解决方案 自定义 ValueReader 进一步阅读 ES-spark 源码可以发现，用户可以自己实现 org.elasticsearch.hadoop.serialization.builder.ValueReader 接口，并通过 es.ser.reader.value.class 配置项（常量org.elasticsearch.hadoop.cfg.ConfigurationOptions.ES_SERIALIZATION_READER_VALUE_CLASS）配置自定义的 ValueReader 实现，从而可以实现对这些特殊取值的读取解析。 当然，后来在 官方文档 中也印证了这一点。\n这样实际处理下来，基本是要拷贝 JdkValueReader 源码进行修改作为自定义的 ValueReader 实现；显然，这样就不能随 ES-spark 升级而自动升级对应实现，同时，在代码中，自定义的修改也和原 JdkValueReader 的实现混杂在一起，给升级带来困难；因此考虑使用 Javassist 进行patch。\nJavassist Patch Javassist 入门和介绍的文章在网上已经很多了，在此不再赘述。\n列举一下patch过程中遇到的一些坑，或者说，Javassist 的一些使用注意事项：\n不支持泛型，请自行强转； 类要用全限定类名，没有import； $0=this, $1/$2/$3=方法的第1/2/3个参数； 代码块前后要用{}包裹； 不支持增强for、lambda等高级语法，需要手动转成基本语法。 最后给出针对Elasticsearch-hadoop读取特殊数字取值的 Javassist patch代码：\nClassPool pool = ClassPool.getDefault(); try { //这里必须用类全限定名，而不是JdkValueReader.class.getName(),否则会先加载类，后面的修改就没用了 CtClass cc = pool.get(\u0026#34;org.elasticsearch.hadoop.serialization.builder.JdkValueReader\u0026#34;); //修复 parseInteger 方法 CtMethod parseInteger = cc.getDeclaredMethod(\u0026#34;parseInteger\u0026#34;); CtClass exceptionClass = pool.get(Exception.class.getName()); String catchParseIntegerException = \u0026#34;try{return new java.lang.Integer(java.lang.Double.valueOf($1).intValue());}catch(java.lang.Exception e){e.printStackTrace();return null;}\u0026#34;; parseInteger.addCatch(\u0026#34;{\u0026#34; + catchParseIntegerException + \u0026#34;}\u0026#34;, exceptionClass); //修复 parseLong 方法 CtMethod parseLong = cc.getDeclaredMethod(\u0026#34;parseLong\u0026#34;); String catchParseLongExpSrc = \u0026#34;try{return new java.lang.Long(java.lang.Double.valueOf($1).longValue());}catch(java.lang.Exception e){e.printStackTrace();return null;}\u0026#34;; parseLong.addCatch(\u0026#34;{\u0026#34; + catchParseLongExpSrc + \u0026#34;}\u0026#34;, exceptionClass); cc.toClass().newInstance(); log.info(\u0026#34;完成对 JdkValueReader 进行 parseInteger() 和 parseLong() 的pack\u0026#34;); } catch (Exception e) { log.error(\u0026#34;给 JdkValueReader 进行 parseInteger() 和 parseLong() 的pack失败:\u0026#34; + e.getMessage(), e); } 在此基础上还可以做成按配置动态patch等骚操作。最后编译运行，Pass。\n","date":"2021-06-19T23:40:50+08:00","image":"https://leibnizhu.github.io/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E8%AF%BB%E5%8F%96%E7%89%B9%E6%AE%8A%E6%95%B0%E5%AD%97%E5%8F%96%E5%80%BC%E6%97%B6%E7%9A%84NumberFormatException/bg2_hu581b7b12baca6a4e8f95bd7dff860ba9_184982_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E4%BF%AE%E5%A4%8DElasticsearch-hadoop%E8%AF%BB%E5%8F%96%E7%89%B9%E6%AE%8A%E6%95%B0%E5%AD%97%E5%8F%96%E5%80%BC%E6%97%B6%E7%9A%84NumberFormatException/","title":"修复Elasticsearch-hadoop读取特殊数字取值时的NumberFormatException"},{"content":"一般来说，Spark写Hive，把xxx-site.xml系列配置文件打进jar包里，或spark-submit指定下file之类，new个HiveContext就完事了。\n要写外部集群，也不外乎是换对应的xxx-site.xml，改改thrift服务地址啥的，不费劲。\n好了，本文结束。\n不对，擅长断更的我不会为此特意写篇博客。\n现在的场景是，每次Spark任务启动的时候才能拿到外部Hive集群的配置信息（别问我为什么，问就是中台的需求，很多集群，java应用启动后才能去读到任务配置，反射组装RDD并执行，Hive配置？lazy的，到写入的时候才会去拿）。\n这个过程踩了不少坑，试了几种方案，直接说结论吧。\nSparkContext创建的时候会创建一个Configuration对象（注意 loadDefaults=true)，写入Hive会用到它；而这个Configuration对象里面已经放了常规的那些***-site.xml系列配置文件作为 defaultResources，这时写入Hive相当于按fat-jar里面的配置来了； 围观Configuration代码，reload配置之后会将defaultResources逐个读出，而defaultResources是个有序的List，那么显然可以用Configuration#addDefaultResource()把外部集群的相关配置xml设置为默认资源，这样拿配置的时候就会拿到外部集群的配置啦！！！ 为了方便配置的读取，直接放在hdfs吧，这样直接Configuration.addDefaultResource(\u0026quot;hdfs:///path/to/hive-site.xml\u0026quot;)不就可以了吗？诶怎么不行，再围观Configuration代码，可以看到加载默认资源最终用的是Configuration#getResource()方法，这个方法体就一句话：return classLoader.getResource(name);，也就是说，它不会去解析hdfs协议，而是直接从classpath里面去读取。所以不能直接从hdfs读取； 最后的方案是把配置文件放在hdfs，写入Hive前，把它下载到当前classpath的其中某个目录下（比如classpath包含. 则下载到System.getProperty(\u0026quot;user.dir\u0026quot;)下），然后Configuration.addDefaultResource(\u0026quot;hive-site.xml\u0026quot;)，因为Configuration是用ClassLoader进行加载的，所以注意路径没有/。 这就完事了？并不，跑起来会发现还是查询jar包里的hive metastore地址，所以还要解析hive-site.xml，读取出hive.metastore.uris值并放入环境变量中。 这就完事了？并不，考虑到后续还会有其他写入操作，以及SparkContext.stop()操作，这些操作都会用到Configuration读取配置，然而现在以及有了外部集群的默认资源了，需要删掉，然而Configuration并没有提供删除默认资源的方法，所以这里要手动反射删除之。 最终代码（简化版）：\n@Slf4j class WriteExtraHive{ public static final String HIVE_METASTORE_URIS_KEY = \u0026#34;hive.metastore.uris\u0026#34;; public static final String BASE_HDFS_PATH = \u0026#34;/path/to/\u0026#34;; private boolean useSparkSql; //实际的实现是支持走jdbc和走SparkSql，根据是否有hive的配置文件 private Set\u0026lt;String\u0026gt; extraDefaultResource = new HashSet\u0026lt;\u0026gt;(); private String hosts; //集群节点，这里只用于区分hdfs的配置路径 public void write(){ init(); //加载配置 write(); //真正写hive end(); //移除额外添加的默认资源 } public void init(){ URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); String hiveSiteXmlPath = calHadoopXmlPath(hosts, \u0026#34;hive-site\u0026#34;, false); useSparkSql = hiveSiteXmlPath != null; log.info(\u0026#34;hive-site.xml文件({})存在:{}\u0026#34;, hiveSiteXmlPath, useSparkSql); if (useSparkSql) { String hiveMetaStoreUris = parseMetaStoreUri(hiveSiteXmlPath); if (StringUtils.isNotEmpty(hiveMetaStoreUris)) { log.info(\u0026#34;从hive-site.xml文件读取到{}={},并设置到环境变量\u0026#34;, HIVE_METASTORE_URIS_KEY, hiveMetaStoreUris); System.setProperty(HIVE_METASTORE_URIS_KEY, hiveMetaStoreUris); calHadoopXmlPath(hosts, \u0026#34;hive-site\u0026#34;, true); calHadoopXmlPath(hosts, \u0026#34;hdfs-site\u0026#34;, true); } else { useSparkSql = false; } } } private void write(){ HiveContext hiveContext = new HiveContext(sc); //别问我从哪来的SparkContext,示例代码，随意看看 DataFrame docDataFrame = hiveContext.createDataFrame(rowRdd, sparkSchema); //rdd和Schema也是，别问 docDataFrame.write() .mode(SaveMode.Overwrite) .saveAsTable(\u0026#34;xxx.yyy\u0026#34;); } public void end(){ synchronized (Configuration.class) { Configuration tempalte = new Configuration(false); CopyOnWriteArrayList\u0026lt;String\u0026gt; defaultResources = TestUtil.getPrivateField(conf, \u0026#34;defaultResources\u0026#34;); //getPrivateField方法如其名，递归父类拿到字段并设可见再读 if (defaultResources == null) { return; } for (String resource : extraDefaultResource) { defaultResources.remove(resource); } WeakHashMap\u0026lt;Configuration, Object\u0026gt; REGISTRY = TestUtil.getPrivateField(conf, \u0026#34;REGISTRY\u0026#34;); if (REGISTRY == null) { return; } for (Configuration curConf : REGISTRY.keySet()) { Boolean loadDefaults = TestUtil.getPrivateField(curConf, \u0026#34;loadDefaults\u0026#34;); if (loadDefaults != null \u0026amp;\u0026amp; loadDefaults) { curConf.reloadConfiguration(); } } } } private String calHadoopXmlPath(String hosts, String fileName, boolean addToDefaultRs) { String hdfsPath = String.format(\u0026#34;hdfs://%shive/%s-%s.xml\u0026#34;, BASE_HDFS_PATH, hosts, fileName); try { FileSystem fs = FileSystem.get(new Configuration()); if (HdfsUtil.isFileExist(hdfsPath, fs)) { if (addToDefaultRs) { ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); URL cpResource = classLoader.getResource(\u0026#34;\u0026#34;); String cpDir = cpResource != null ? cpResource.getPath() : (System.getProperty(\u0026#34;user.dir\u0026#34;) + File.separator); String downloadFileName = String.format(\u0026#34;%s-%s_%s.xml\u0026#34;, hosts, fileName, System.currentTimeMillis()); //实际下载本地的名字 String fullDownloadFilePath = cpDir + downloadFileName; log.info(\u0026#34;增加Hadoop配置文件:{}到Configuration默认资源,下载到本地:{}\u0026#34;, hdfsPath, fullDownloadFilePath); try (OutputStream os = new BufferedOutputStream(new FileOutputStream(fullDownloadFilePath))) { HdfsUtil.copyFileAsStream(hdfsPath, os, fs); Configuration.addDefaultResource(downloadFileName); //加入默认资源 extraDefaultResource.add(downloadFileName); //记录加过哪些默认资源，后面要移除 } catch (Exception e) { log().error(e.getMessage(), e); } log.info(\u0026#34;增加Hadoop配置文件:{}后读取classLoader.getResource({})={}\u0026#34;, fileName, downloadFileName, classLoader.getResource(downloadFileName)); } return hdfsPath; } else { log.info(\u0026#34;不存在文件:{}\u0026#34;, fileName); } } catch (Exception e) { log.error(\u0026#34;get FileSystem fail!\u0026#34;, e); } return null; } private String parseMetaStoreUri(String hiveSiteXmlPath) { Configuration conf = new Configuration(false); try { conf.addResource(new URL(hiveSiteXmlPath)); } catch (IOException e) { e.printStackTrace(); } return conf.get(HIVE_METASTORE_URIS_KEY); } } ","date":"2020-05-06T13:10:28+08:00","image":"https://leibnizhu.github.io/p/Spark%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BDhive%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%A1%88/bg3_hu04adce3b52cc8e582e668512c3886fe4_174126_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Spark%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BDhive%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%A1%88/","title":"Spark动态加载hive配置的方案"},{"content":"DBeaver接入Kylin数据源现在网上好象没有靠谱的博客，写一篇吧。\n打开DBeaver： 新建： 属性 填的值 驱动名称 Apache Kylin 或随意 驱动类型 默认 类名 org.apache.kylin.jdbc.Driver URL模板 jdbc:kylin://{host}:{port}/{database} 默认端口 7070 目录 Hadoop 或随意 添加文件 可以在maven仓库找 ~/.m2/repository/org/apache/kylin/kylin-jdbc/\u0026lt;版本号\u0026gt;/kylin-jdbc-\u0026lt;版本号\u0026gt;.jar 驱动类 点击找到类，一般需要选择的就是列表中第一个，与类名一致 点高级参数（否则会使用catalog，然后导致查询失败）：\n然后新建连接，搜索kylin，填入连接信息即可：\n目前只能看到table，看不到model和cube。\n","date":"2020-04-27T14:05:57+08:00","image":"https://leibnizhu.github.io/p/DBeaver%E6%8E%A5%E5%85%A5Kylin%E6%95%B0%E6%8D%AE%E6%BA%90/bg4_hub7bb5059c14557e6b598b2fed9c03f8a_114183_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/DBeaver%E6%8E%A5%E5%85%A5Kylin%E6%95%B0%E6%8D%AE%E6%BA%90/","title":"DBeaver接入Kylin数据源"},{"content":"Spark写Mongodb的小坑 首先证明我还活着。\n因为Spark老集群版本限制(参见:https://docs.mongodb.com/spark-connector/master/)，mongodb-connector用的版本是1.1.0，以下坑基于该版本出现，新版本未验证。\n用MongoSpark.save写入RDD报错E11000 duplicate key error 观察源码\ndef save[D: ClassTag](rdd: RDD[D], writeConfig: WriteConfig): Unit = { val mongoConnector = MongoConnector(writeConfig.asOptions) rdd.foreachPartition(iter =\u0026gt; if (iter.nonEmpty) { mongoConnector.withCollectionDo(writeConfig, { collection: MongoCollection[D] =\u0026gt; iter.grouped(DefaultMaxBatchSize).foreach(batch =\u0026gt; collection.insertMany(batch.toList.asJava)) }) }) } Mongo的逻辑是，纯RDD，没有schema，那么无法得知_id类型信息，于是直接insertMany。当Document数据里有_id字段时，insertMany可能就会发生_id冲突，报E11000 duplicate key error异常错误。\n解决方案：\n改用 MongoSpark.save(dataFrame: DataFrame); 爆改MongoSpark, 将MongoSpark.save(rdd: RDD[D])改成根据Docuemnt是否有_id而分别生成ReplaceOneModel和InsertOneModel，再批量插入。 我一开始用了方案1，后来还是觉得限制太多，但也没有直接爆改MongoSpark，而是直接在代码里实现，不用MongoSpark了。\nList含null时写入失败 原因是com.mongodb.spark.sql.MapFunctions的arrayTypeToBsonValue方法没有处理List里面的空元素，而是直接每个元素调convertToBsonValue方法，后者再调用elementTypeToBsonValue的时候，在模式匹配里面就匹配不上，进入最后的默认分支，抛MongoTypeConversionException异常。\n解决方案很简单：\n对数组/List做预处理，过滤null元素，但可能这些null元素是有用的，此时此法无用； 爆改MapFunctions： 其实也不算爆改，小改几个地方即可：\nprivate def arrayTypeToBsonValue(elementType: DataType, containsNull: Boolean, data: Seq[Any]): BsonValue = { val internalData = elementType match { /*....省略无关代码....*/ case _ =\u0026gt; data.map(x =\u0026gt; if(x == null \u0026amp;\u0026amp; containsNull) new BsonNull() else convertToBsonValue(x, elementType)).asJava } new BsonArray(internalData) } private def elementTypeToBsonValue(element: Any, elementType: DataType): BsonValue = { elementType match { /*....省略无关代码....*/ case arrayType: ArrayType =\u0026gt; arrayTypeToBsonValue(arrayType.elementType, arrayType.containsNull, element.asInstanceOf[Seq[_]]) /*....省略无关代码....*/ } } private def mapTypeToBsonValue(valueType: DataType, data: Map[String, Any]): BsonValue = { val internalData = valueType match { /*....省略无关代码....*/ case subArray: ArrayType =\u0026gt; data.map(kv =\u0026gt; new BsonElement(kv._1, arrayTypeToBsonValue(subArray.elementType, subArray.containsNull, kv._2.asInstanceOf[Seq[Any]]))) /*....省略无关代码....*/ } new BsonDocument(internalData.toList.asJava) } ObjectId对象写入 Mongodb的_id默认是用ObjectId类型，如果在修改数据后重新写入Mongodb，也需要使用同样的ObjectId对象。\n在直接使用Mongodb的SDK的情况，这个很简单，直接new一个org.bson.types.ObjectId对象即可。\n如果是使用DataFrame，则structType和具体的Row数据构造都有一小点麻烦，直接上代码吧。\n构建Schema：\nvar structFieldList = List() /*....其他字段schema信息....*/ val idDataType = StructType(List(StructField(\u0026#34;oid\u0026#34;, StringType, true, Metadata.empty))) structFieldList += StructField(\u0026#34;_id\u0026#34;, idDataType, true, Metadata.empty)) StructType(structFieldList); 写入数据的处理：\nGenericRow(Array(\u0026#34;5e1db87e5d080f6e7eb7b067\u0026#34;)) //这是一个Row里面的一个字段值 ","date":"2020-01-15T15:47:27+08:00","image":"https://leibnizhu.github.io/p/Spark%E5%86%99Mongodb%E7%9A%84%E5%B0%8F%E5%9D%91/zelda_hu62caf65ed048351c8a298072e6e03294_171791_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Spark%E5%86%99Mongodb%E7%9A%84%E5%B0%8F%E5%9D%91/","title":"Spark写Mongodb的小坑"},{"content":"之前写了一篇文章 发布项目到Maven中央仓库, 最近发布一个scala项目的时候, 用原来的配置, deploy到仓库后,提示找不到javadoc.\n折腾了一番, 结论:\n***-javadoc.jar肯定是要有的, 否则Nexus校验不通过 用maven的javadoc插件默认查找src/main/java下面的源码进行文档构建, scala项目里这个目录当然是不存在的 可以通过\u0026lt;sourceDictionary\u0026gt;配置让javadoc插件去src/main/scala目录去找源码, 但是显然没有java文件给他找,也是构建不出文档的 所以要用maven的scala插件进行scaladoc构建文档并打包(默认jar包文件名跟javadoc兼容), 命令是mvn scala:doc-jar 为了和gpg签名,以及最后的deploy到远程maven仓库配合, 只要修改scala插件的配置即可 最后配置如下: 注: 主要增加了doc-jar的goal, 其实在profile或直接在build中配置均可\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!--scala编译,scala-doc等--\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.2\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;recompileMode\u0026gt;incremental\u0026lt;/recompileMode\u0026gt; \u0026lt;args\u0026gt; \u0026lt;arg\u0026gt;-target:jvm-1.8\u0026lt;/arg\u0026gt; \u0026lt;/args\u0026gt; \u0026lt;javacArgs\u0026gt; \u0026lt;javacArg\u0026gt;-source\u0026lt;/javacArg\u0026gt; \u0026lt;javacArg\u0026gt;1.8\u0026lt;/javacArg\u0026gt; \u0026lt;javacArg\u0026gt;-target\u0026lt;/javacArg\u0026gt; \u0026lt;javacArg\u0026gt;1.8\u0026lt;/javacArg\u0026gt; \u0026lt;/javacArgs\u0026gt; \u0026lt;jvmArgs\u0026gt; \u0026lt;jvmArg\u0026gt;-Xms1024m\u0026lt;/jvmArg\u0026gt; \u0026lt;jvmArg\u0026gt;-Xmx1024m\u0026lt;/jvmArg\u0026gt; \u0026lt;/jvmArgs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;scala-compile-first\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;process-resources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;add-source\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;doc-jar\u0026lt;/goal\u0026gt; \u0026lt;!--scaladoc打jar包--\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;scala-test-compile\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;process-test-resources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;add-source\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;testCompile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- 以前的配置 --\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; ","date":"2018-08-01T15:05:44+08:00","image":"https://leibnizhu.github.io/p/Scala%E9%A1%B9%E7%9B%AE%E5%8F%91%E5%B8%83%E7%9A%84%E5%B0%8F%E5%9D%91/raindrop_hue16895f2847785f1917ada9102cfd571_178220_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Scala%E9%A1%B9%E7%9B%AE%E5%8F%91%E5%B8%83%E7%9A%84%E5%B0%8F%E5%9D%91/","title":"Scala项目发布的小坑"},{"content":"第8章 集合 Scala对元素较少的Set进行了优化,4个元素以内的Set有专门的实现类(Set0,Set1,Set2,Set3,Set4),大于4个元素的使用HashSet实现 Set的方法(filter,map,foreach这些就不说了): mkString: 允许传入一个分隔符,相当于把集合所有元素用分隔符join起来返回字符串 ++: 合并两个Set,或者说,并集 \u0026amp;: 两个Set的交集 Map的方法: filterKeys: 根据key去过滤,而filter方法是根据(key,value)键值对去过滤 get: 根据key拿value,注意是返回Option[T] updated(K,V): 增加或更新键值对,返回新的Map,也可以用X() = b,等效于X.updated(b) List的a :: list读作将a前插到list,后面的list才是::方法的调用者; 而list1 ::: list2将list1前插到list2 List的forall()方法判断是否所有元素都满足条件, exists()方法判断是否有任意元素满足条件. 其实分别相当于: list.forall(f) 等效于 (true /: list) {_ \u0026amp;\u0026amp; f(_)} list.exists(f) 等效于 (false /: list) {_ || f(_)} 如果方法名以 : 结尾, 那么调用时的主语是操作符后面的实例, 即a (操作符): b等效于b.操作符:(a);同时scala不允许字母作为操作符的名称,除非用下划线对操作符增加前缀,如jump_:() + - ! ~作一元操作符时,也是调用的主语在操作符后面,分别映射到unary_+(),unary_-(),unary_!(),unary_~()等方法的调用,即-a调用a.unary_-()等 for表达式:for([parrten \u0026lt;- generator; definition*\u0026gt;]+;filter;) [yield] expression,可以加过滤条件,而yield也是可选的,有yield的时候返回一个值列表,没有yield的时候返回Unit 第9章 模式匹配和正则表达式 匹配List的时候,可以只获取感兴趣的元素,剩下的用_*省略, 如: list match { case List(head,\u0026#34;test\u0026#34;, _*) =\u0026gt; f(head) //后面的直接忽略 case List(\u0026#34;test\u0026#34;, others @ _*) =\u0026gt; f(others) //后面的tail要引用 case \u0026#34;haha\u0026#34;::\u0026#34;two\u0026#34;::tail =\u0026gt; f(tail) //这样其实也行 } scala的模式匹配case子句无需break scala约定模式变量名以小写字母开头(scala假设他是模式变量),常量为大写字母(会在作用域范围查找变量) 不遵守以上规则的时候, 比如需要用模式匹配以外的变量, 可以显式指定作用域(如this.max),或用反引号 ` 包住变量名(如`max`) case类用于创建轻量级值对象,经常用于模式匹配；如果主构造器无参数，调用时又没加括号，那么传递的是case类的伴生对象（混合了Function0特质，可以视为函数） 可使用自定义的提取器进行模式匹配，提取器在伴生对象中有unapply()方法,接受我们想要匹配的值,返回Boolean(传入的值是否可以匹配),例如: object Symbol { def unapply(s:String):Boolean = s == \u0026#34;TEST\u0026#34; } input match { case Symbol() =\u0026gt; println(s\u0026#34;matched, ${input}\u0026#34;) //即匹配 TEST case _ =\u0026gt; println(s\u0026#34;Invalid input: ${input}\u0026#34;) } 提取器还可以返回Boolean型以外的结果,即解析的结果,通过修改unapply()的返回类型实现(返回Option[T],T即解析成功的结果类型), 例如: object Splitor { def unapply(s:String):Option[(String,String)] = if(!s.contains(\u0026#34;:\u0026#34;)) None else { val splited = s.split(\u0026#34;:\u0026#34;) Some((splited(0), splited(1))) } } input match { case Splitor(a,b) =\u0026gt; println(s\u0026#34;matched, ${a} and ${b}\u0026#34;) //即匹配 ***:*** case _ =\u0026gt; println(s\u0026#34;Invalid input: ${input}\u0026#34;) } 使用提取器的时候还可以应用其他提取器进行模式匹配,如 input match { case Splitor(a @ Symbol(),b) =\u0026gt; println(s\u0026#34;matched, ${a} and ${b}\u0026#34;) //即匹配 TEST:*** case _ =\u0026gt; println(s\u0026#34;Invalid input: ${input}\u0026#34;) } 正则表达式: \u0026quot;regex\u0026quot;.r(可以用原始字符串\u0026quot;\u0026quot;\u0026quot;regex\u0026quot;\u0026quot;\u0026quot;.r), 实际上是String隐式转换成StringOps再调用其r方法获取Regex类实例 正则表达式的方法: findFirstIn(source): 获取正则表达式第一个匹配项 findAllIn(source): 获取正则表达式的所有匹配项 replaceFirstIn(source, replacement): 替换第一个匹配项 replaceAllIn(source, replacement): 替换所有匹配项 scala的正则表达式是提取器,返回值是匹配项(括号的分组)拼接成的元组 下划线的作用: 包引入的通配符 元组索引的前缀 函数值的隐式参数 用默认值初始化变量 在函数名中混合操作符和: 在模式匹配中作为通配符 处理异常时在catch代码块和case一起用(类似模式匹配了) 作为分解操作的一部分,如max(arg: _*)可以接受列表或数组参数,自动拆解成离散的值传递给可变长度参数 部分应用一个函数,如val square = Math.pow(_:Int, 2)部分应用了pow函数,返回一个新的单参数函数 第10章 处理异常 Java处理多个异常时,会检查多个异常的处理顺序,子类必须在前面,否则编译会出错(exception ***.*** has already been caught),但scala对此不会警告,要自己注意(在catch块中使用case匹配的顺序) 这本书竟然没有讲到Try类,吐槽一下 第11章 递归 尾递归优化(Tail Recursive Optimization): 不是尾递归的时候,递归调用在字节码对应invokespecial指令,表明是递归调用,会产生新一层栈;如果写成尾递归, 递归调用时在字节码对应goto指令,表明使用了迭代而非方法调用,放弃了当前的上下文 @scala.annotation.tailrec注解加载函数上,可以让scala检查是否使用了尾递归,如果非尾递归,会报错;该注解可选,主要是增加可读性,并在重构时保持尾递归性质 蹦床调用(trampoline call): 两个函数互相调用(f调用g,g调用f)构成递归, 对于蹦床调用即使是尾递归@tailrec注解也会报错(scala不能识别跨方法的递归);此时可以用TailRec类解决: 蹦床调用的函数返回TailRec[T],其中T是真正的返回值类型 蹦床调用的函数内部,需要结束递归时返回done(结果: T),需要递归调用其他函数时返回tailcall(其他函数());这两者都只是简单包装参数,以供后续调用或延迟执行 外部调用这些函数时, 对返回值TailRec[T]调用result方法可以获取最终递归结果;真正发生计算是调用result方法的时候 第12章 惰性求值和并行集合 使用关键字lazy修饰变量,scala会推迟绑定变量和他的值,直到该值第一次被使用(才会去绑定) 如果变量绑定的计算有副作用,那么多个变量的绑定顺序机会对绑定的值有影响,此时就不能随便用惰性求值(lazy)了,否则不可交换计算的结果将会得不可知 前面介绍的集合都是严格集合,所有计算都是严格(立刻)执行的; 通过集合的view()方法可以获得一个严格集合的惰性视图, 惰性集合会推迟计算,当且晋档请求了非惰性/非视图的结果时(比如head,last等等)前面的操作才会进行 但集合的惰性视图不一定比严格集合性能好,要看具体情况,比如一个集合进行一些操作之后,获取head,那么惰性视图要进行操作的次数就少一些;如果在多次filter后要拿last,那么惰性视图就要把每个元素执行filter计算一遍, 而严格集合每次filter后要计算的结果都小一些,这样计算量反而会少一点. Stream仅按需生成值,有天然的惰性.拥有#::方法连接(惰性,需要的时候才会连接)现有的Stream和新的值,通过递归定义可以得到一个Stream, 如: //第一个元素是start,下一个是前一个加1 def gen(start:Int): Stream[Int] = start #:: gen(start + 1) println(gen(10)) //Stream(10, ?) 调用Stream.force()方法可以强制求值, toList()方法也类似;如果在无限流上调用的话会抛OutOfMemoryError 对于无限流,可以使用take(Int)方法获取前N个值组成的Stream,也可以使用takeWhile()方法按条件生成值(参数的函数值返回false时终止生成新值) Stream会记住(memoize)已经生成的值,即按需生成新的值后,会先缓存再返回. 比如执行stream.take(3).force计算了3个值,再执行stream.take(4).force只计算第4个值,前3个值从缓存读取 对很多顺序集合,scala都有对应的并行版本, 如ParArray,ParHashMap,ParHashSet等等; 可以用par()和seq()方法在顺序集合及其并行版本之间转换 不适合使用并行集合的情况: 创建和调度线程的开销不应该大于执行这些任务所需要的时间, 对于慢型任务而言并行集合可能有所裨益,但对于小心集合的快速任务并不适合; 此外,在集合上的操作如果修改全局状态(线程不安全的修改),那么整体计算结果不可知; 如果操作不满足结合律也不要使用并行集合,因为并行集合的执行顺序是不确定的 第13章 使用Actor编程 Actor帮助我们将共享的可变性转换成隔离的可变性(isolated mutability),如果一个任务可以有意义地分成几个子任务,分而治之,那么可以使用Actor模型来解决这个任务 AtomicLog之类的类,虽然原子性保证了单个值的线程安全性,但并不能保证跨多个值的原子性,这些值可能同时发生变化 一个Actor是一个对象,由一个消息队列支撑,任意给定的时间,一个Actor只会处理一条消息; Akka提供Actor模型, 创建一个Actor只要继承Actor特质并实现receive()方法, receive()方法主题是模式匹配, 匹配发生在一个隐式消息对象上 Actor托管在ActorSystem中,管理了线程池(只要系统保持活跃,这个线程池就会一直保持活跃),消息队列,和Actor生命周期,使用actorOf()工厂方法创建Actor, 用!()方法(tell())发送消息: val system = ActorSystem(\u0026#34;sample\u0026#34;) //创建ActorSystem val depp = system.actorOf(Props[XxxActor]) //通过actorOd工厂方法创建Actor depp ! \u0026#34;Hello\u0026#34; //往Actor发送消息 val terminateFuture = system.terminate() //退出ActorSystem线程 Await.ready(terminateFuture, Duration.Inf) Actor一些细节: Actor在不同线程中进行,而不是调用主线程 每个Actor一次只处理一条消息,多个Actor并发运行处理多条消息 Actor是异步的,不会阻塞调用者(调用者不等待Actor回复) 线程和Actor不绑定(没什么线程亲和力),每次处理消息可能使用线程池中不同的线程 Actor中保存的任何字段都是自动线程安全的,可变但没有共享可变性;可以在Actor类中选择性地存储状态,比如用于存储状态的字段,等等 若希望从Actor得到响应,Akka提供了询问模式,但消息可能永远不会到达,因此强制使用超时时间;询问模式下, 使用?()方法(ask())发送消息, 返回一个Future,需要用这个Future实例等待响应(可以用Await.result()方法), 例如: //Actor类 class MyActor extends Actor { def recieve:Receive = { case msg =\u0026gt; sender ! s\u0026#34;Got message ${msg}\u0026#34; } } val system = ActorSystem(\u0026#34;sample\u0026#34;) //创建ActorSystem val depp = system.actorOf(Props[MyActor]) //通过actorOd工厂方法创建Actor implicit val timeout = Timeout(2.seconds) //通过隐式参数定义超时, ?方法要用到 val askFuture = depp ? \u0026#34;heihei\u0026#34; //询问模式,发出消息 val result = Await.result(askFuture, timeout.duration) //等待响应结果,这里也需要一个超时的参数 println(s\u0026#34;Response result : ${result}\u0026#34;) val terminateFuture = system.terminate() //退出ActorSystem线程 Await.ready(terminateFuture, Duration.Inf) Akka提供RoundRobinPool路由器,会将发送到这个路由器的所有消息均匀地路由到支撑他的多个Actor,使用方法: val system = ActorSystem(\u0026#34;sample\u0026#34;) //创建ActorSystem val router:ActorRef = system.actorOf(RoundRobinPool(100).props(Props[MyActor])) Actor中想访问ActorSystem可以使用context()方法 使用建议: 更多地依赖无状态的而非有状态的Actor 要保证recieve()方法中的处理速度非常狂,尤其是接收Actor具有状态时,改变状态的长时间运行任务将会降低并发性,要避免 确保在Actor之间传递的消息是不可变对象 避免使用ask() 第14章 和Java进行互操作 scala使用Java类,当Java变量/方法名等于scala关键字冲突的时候,可以将受影响的变量/方法用反引号`括起来 没有方法实现的trait在字节码层面是简单的接口;如果想在scala中创建接口,只能创建没有方法实现的trait 有方法实现的trait, 2.11及更早版本的scala会编译成一个接口(名为:trait名)和一个实现该接口的抽象类(名为:trait名$class), 2.12开始,只包含方法实现而不包含字段的trait会编译成带有默认方法的接口 scala将单例对象和伴生对象编译成一个单例类,在字节码层面只有static方法 单例对象(假设名为Single)编译后产生两个类, 一个Single$类存放具体的静态方法,一个Single类负责转发方法的调用,在Java中可以直接通过Single调用其方法 伴生对象(假设类名为Buddy)编译后产生两个类,一个Buddy$类存放伴生对象的静态方法,一个Buddy类存放对应类的方法等; 在Java中使用类本身的方法可以直接调用;而伴生对象,包含一个MODULE$的属性保存着该类的静态单例对象,可以通过它去访问其方法,如Buddy$.MODULE$.method() scala没有throws子句,抛出异常时无需在方法签名中显示声明,如果在Java中扩展类覆写方法时抛出异常就会报错; 可以在scala方法签名中增加@throw注解解决该问题,如: abstract class Bird { @throws(classOf[NoFlyException]) def fly(): Unit } (结束)\n","date":"2018-07-18T16:54:47+08:00","image":"https://leibnizhu.github.io/p/Scala%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%BA%8C/nightstreet_hu20b232da5d98b9e485e9d8181ea68fef_290037_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Scala%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"《Scala实用指南》读书笔记(二)"},{"content":"前言 好久没写博客了,在Kafka源码的海洋里挣扎ing,休息的时候还刷Leetcode玩儿,很多东西匆匆丢到OneNote里了.\n最近花了一周时间看了品神翻译的《Scala实用指南》, 这本书应该主要是面向刚入Scala大门的Javaer的,前面讲Scala基础,后期还讲了下Akka和Scala具体的应用.\n我虽然写scala也有一段时间了,再看这本书还是觉得受益匪浅,有很多地方以前没有注意到的.\n那么本文就把一些细节写写,仅仅是枚举一下记录一下,并不全面.\nP.S. 看书勘误找品神, 找到错误了发红包诶,我就领了一个冰可乐(等值)红包.\n第2章 体验Scala scala的REPL中, Ctrl+A调到行首,Ctrl+E调到行尾 scala \u0026lt;scala源文件\u0026gt;命令可以在单独的JVM中运行scala代码,而实际上是自动合成Main类调用main()方法 第3章 从Java到Scala 用val定义所有字段,并只提供读不提供更新状态的方法,可以使一个类的实例不可变 1 to 3,1 until 3这类表达式其实是隐式转换,1通过intWrapper()方法转换为成富封装器(rich wrapper,这里具体是RichInt类)再调用其to(),until()方法 Scala将Scala.Int视作Java基本类型int是纯粹的编译期转换 val (a,b)=(1,2)这种赋值方法叫多重赋值(multiple assignment) 元组的索引越界是在编译期报错的 一个函数如max(values:Int*)接受可变长度参数值,但不是字面上的数组类型,如果调用 max(Array(1,2)) 在编译期会报错,可以使用数组展开标记(Array explode notation)告诉编译期将数组展开成所需的形式:\nmax(Array(1,2):_*) 重载基类方法时,要小心参数默认值,如果派生类重载方法使用的参数默认值和基类对应的不一样,会让人困惑 重载基类方法时,应该保持参数名字一致性,否则使用参数命名调用(如f(a=3,b=4))时会优先使用基类的 隐式参数需要将参数标记为implicit且放在单独的参数列表中,如: f(a:int,b:Int)(implicit c:User) 隐式参数的值传递是可选的,没有传值时,scala会在调用的作用域中寻找一个隐式变量,因此每个作用域中每一种类型最多只能有一个隐式变量,如上面的例子中,定义一个隐式变量:\nimplicit def user:User = User(\u0026#34;Leibniz\u0026#34;) scala能自动将String转换成scala.runtime.RichString,因此可以使用capitalize(),lines(),reverse()等方法 跨行字符串:将多行的字符串放在\u0026quot;\u0026quot;\u0026quot;...\u0026quot;\u0026quot;\u0026quot;中,里面的引号和斜杠等等字符甚至不需要转译,又称为原始字符串,创建正则的时候就很方便了. 原始字符串中的缩进也会被带入结果字符串中,可以使用stripMargin()方法将管道符号|前面的空白或控制字符去掉, 如: val str = \u0026#34;\u0026#34;\u0026#34;Leibniz said |\u0026#34;Scala is exciting\u0026#34;.\u0026#34;\u0026#34;\u0026#34;.stripMargin 字符串拼接用字符串插值,如s\u0026quot;...${expresion}\u0026quot;,s代表s插值器,如果是简单的一个变量直接$val,复杂表达式才需要大括号括起来,而美元符号$此时需要转译:$$.表达式的值在插值时被捕获,变量后续的变更不会影响字符串. 字符串格式化可以用f插值器,如f\u0026quot;${a}%2.2f\u0026quot;将a变量显示小数点后2位,此外还有%s字符串和%d整数等格式 scala的类和方法默认公开 scala不强制要求捕获异常 scala默认导入的scala和scala.Predef包,包含了一些默认类型,隐式转换等等 scala没有操作符重载, 但是方法名可以是+_*/等操作符,同时方法调用的句号和括号可以省略,因此有操作符重载的假象;对这些方法,同级操作的左边优先级更高, scala使用方法的第一个字符决定其优先级,从低到高分别为: | ^ \u0026amp; \u0026lt; \u0026gt; = ! : - + / * % scala中赋值操作的返回值是Unit scala中==是基于值的对比(等效于equals()方法),需要比较引用的可以用eq()方法 scala的protected方法只有派生类可以访问,同包的非派生类不可访问,在派生类中也不可以通过基类实例来访问,只能是通过派生类的实例方法访问 scala中可以通过private[类名/包名/this]和protected[类名/包名/this]实现细粒度的权限控制,具体不表了\u0026hellip; 第4章 处理对象 定义类的时候,class A(var a:Int, val b:Int)被称作主构造器(primary constructor),其中可变的参数a自动生成getter和setter,不可变的参数b自动生成getter方法,但这些getter setter方法不符合JavaBean标准,没有get/set前缀,可以通过在期望产生符合JavaBean规范的字段加上@scala.reflect.BeanProperty注解来解决这个问题 scala中val修饰的属性编译后为private final 类中单独的代码会作为主构造器的一部分 还可以定义辅助构造器:def this(...),辅助构造器的第一行必须调用主构造器或者其他辅助构造器 重载方法时强制使用关键字override(不是注解),如override def f(...) = ... 派生类在主构造器(与基类同名的)参数前面加上要关键字override, 编译器将这些属性的getter方法路由到基类对应方法 只有object没有对应class叫单例对象(不能传递参数给其构造器),class对应object叫伴生对象 伴生对象可以访问private修饰的方法 字节码层面上,单例中的方法会被编译为static方法 scala创建枚举需要扩展Enumeration类,并用特殊的Value(表示枚举的类型)来赋值,如: object Currency extends Enumeration { type Currency = Value //高速编译器,将Currency视作一个类型而非实例 val CNY,USD,JPY = Value } scala支持包对象,为单例对象,与包同名,用package object关键字标记,当包中其他类import 包名._时,可以直接引用包对象里面的方法;包对象可以存储该包公用的一些方法如工具方法,举例, scala包也有包对象,包含了很多类型别名和隐式类型转换. 第5章 善用类型 scala偏向于使用类型推断,但以下情况必须显式指定类型:1.定义没有初始值的类字段;2.定义函数或方法的参数;3.使用return或递归时,定义函数或方法返回类型;4.将变量定义为与推断出来类型不一样 Nothing是所有类型的子类型, Any是所有类型的基础类型,包含AnyRef子类型(所有引用类型的基础类型,包含Java的Object类的方法)和AnyVal子类型(所有值类型Int,Double等的基础类型,映射到Java原始类型) 使用有类型参数的类但不指定泛型类型的时候,就会使用Nothing作为类型参数,如果没有定义协变,那么任何有意义的类型都不能使用 抛出异常的表达式的返回类型也是Nothing,可以替代任意类型,语义上是合法的,可以是类型推断进行下去 Option[T]有两个子类,Some[T](有值)和None(没有值),用于值可能存在或不存在的情况 Either有两个子类,Right[R](正确结果的值)和Left[L](错误的时候的值),用于结果可能存在两种不同类型的值的情况 定义函数的时候,用等号将方法声明和方法主题分开(如def f(a:Int) = {...})才能完成返回值类型推断,反之(如def f(a:Int) {...})不行 一个方法只是字段或属性的访问器,不要将()放在定义中,调用的时候也不用();但如果一个函数具有副作用,那么在声明和调用这个函数的时候都要使用() 任何返回Unit的函数必须产生副作用(否则,又不返回东西,又不产生副作用,那这个函数没什么用了) T \u0026lt;: P表示类型T派生自类型P, T \u0026gt;: S表示类型T是类型S的超类 [+T]表示支持协变(若接受基类实例集合,则也支持子类实例集合);[-T]表示支持逆变(若接受基类实例集合,则也支持超类实例集合) 使用隐式转换函数时,需要导入scala.language.implicitConversions(主要是提醒阅读代码的人,黑魔法出没) 与隐式参数类似,如果一个函数标记为implicit,且存在于当前作用域(import或在当前文件作用域),scala都会自动使用这个函数进行类型转换(如果可以) scala还支持隐式类(用implicit标记类),但不能是独立的类,必须在单例对象/类/特质中;而使用隐式类的时候不需要导入implicitConversions;例如: object MyUtil{ implicit class Wrapper(i:Int) { def conv(unit: String): String = i.toString + unit } } object MyUtilTest{ import MyUtil._ val s = 11 conv \u0026#34;cm\u0026#34; println(s) //调用这个对象的时候打印:11cm,仅作示例 } 隐式转换的时候会创建短生命周期的垃圾隐式对象,增加GC压力,而值对象可以解决这个问题(将隐式类示例上的调用自动改写成扩展方法),将隐式类继承AnyVal,如上面的例子改成: object MyUtil{ implicit class Wrapper(i:Int) extends AnyVal{ def conv(unit: String): String = i.toString + unit } } 值类型还可以用在其他简单值/原始值已经够用,但是希望使用类型进行更好抽象的地方:最终源码中显示是类/示例,字节码级别上是基础类型 自定义字符串插值器: 定义一个隐式类,主构造器接受一个StringContext类型的参数,定义方法name(args: Any*):StringBuilder,那么当程序作用域包含该隐式类的时候,对name\u0026quot;\u0026quot;\u0026quot;...\u0026quot;\u0026quot;\u0026quot;的字符串,会自动创建StringContext对象(其parts方法可以获取字符串中被表达式分割的各个子字符串),传入隐式转换的的name(Any*)方法,参数传入字符串中的各个${expression},处理完的StringBuilder对象返回,然后转换成字符串. 例如一个简单的例子,所有表达式前后加上井号: object MyInterpolator{ implicit class Interpolator(val context:StringContext) extends AnyVal { def my(args: Any*):StringBuilder = { new StringBuilder(context.parts.zip(args).map(item =\u0026gt; { val (text, expression) = item s\u0026#34;$text#${expression}#\u0026#34; }).mkString) } } } import MyInterpolator._ val name = \u0026#34;Leibniz\u0026#34; println(my\u0026#34;\u0026#34;\u0026#34;My name is ${name}\u0026#34;\u0026#34;\u0026#34;) //调用我们自定义的插值器,返回 My name is #Leibniz# 第6章 函数值与闭包 柯里化: 一个有分组参数的函数,如f(a:A)(b:B):C,使用f _创建一个部分应用函数(此处类型为A =\u0026gt; (B =\u0026gt; C)),可以用于创建可复用的临时便利函数; 多组参数的函数,如果有单独成组的函数参数,可以不使用小括号,直接用大括号,更直观,如f(a:Int)(g:A=\u0026gt;B)可以这样调用:f(1) {a =\u0026gt; xxx(a)} 用下划线代表函数值的参数时,如果无法判断类型,scala会报错,此时可以显式指定类型 scala自动将函数名视作函数值的引用 函数或代码块可能含有未绑定的变量,在调用前根据上下文绑定,形成闭包(closure);绑定的时候不会复制相应变量的值,实际上会绑定到变量本身,因此线程不安全 第7章 特质 在trait中定义并初始化的val/var变量，将会在混入了该trait的累的内部实现；定义并未初始化的val/var变量被认为是抽象的,混入该trait的类需要实现他们 类混入trait的时候,如果类没用用extends,则第一个trait用extends,后面的trait用with;如果类已经用extends了,那么所有trait都用with来混入 混入了trait的类可以调用trait的方法, 其实例引用也可以多态为trait实例 trait的构造器不接收任何参数 选择性混入: 可以对没有混入trait的类的实例混入trait,即只有该实例混入了trait,该类其他实例没有,也是用with进行混入,如: val angle = new Cat(\u0026#34;Angle\u0026#34;) with Friend` 在trait中,使用super调用的方法会触发延迟绑定(late binding),此时并非对基类方法的调用,而是转发到混入该trait的类中,如果混入了多个这样的trait(有同样父类方法,即extends自同一个trait或抽象类),那么从右向左,混入下一个trait直到混入到类, 例如: abstract class Check {def check:String = \u0026#34;Abstract check...\u0026#34;} trait CreditCheck extends Check {override def check:Stirng = s\u0026#34;Credit...${super.check}\u0026#34;} //并非调用父类方法,而是转发到下一个trait或类 trait MoneyCheck extends Check {override def check:Stirng = s\u0026#34;Money...${super.check}\u0026#34;} trait HouseCheck extends Check {override def check:Stirng = s\u0026#34;House...${super.check}\u0026#34;} val app1 = new Check with MoneyCheck with CreditCheck println(app1.check) //根据混入的顺序,打印 Credit...Money...Abstract check... val app2 = new Check with CreditCheck with HouseCheck println(app2.check) //根据混入的顺序,打印 House...Credit...Abstract check... 如果基类的方法是抽象的,那么方法绑定必须推迟到某个具体的方法已知为止;同样是从右到左的顺序连接混入trait,这样可以避免方法冲突的问题 ","date":"2018-07-17T15:14:37+08:00","image":"https://leibnizhu.github.io/p/Scala%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%80/summer2_hu36806978eb822407cd5170dd014e2121_201343_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Scala%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"《Scala实用指南》读书笔记(一)"},{"content":"前言 最近因为数据处理的需求, 用到Kettle进行MySQL到HDFS的数据导入,而Kettle的GUI界面导入比较繁琐,不易于复用,所以考虑其Java API.\n但是,网上的资料实在少得可怜, 而官网的文档也仅仅给出了一个例子,而且是版本很旧的. 于是只能根据这个很旧的版本, 加上Maven仓库摸索, 再加上官方最新版API文档,慢慢摸出来.\n代码清单 最后的结果就是这篇文章, 废话也不想多说了,也懒得打字,就是普通的Maven项目,主要三个文件:\npom.xml: Kettle依赖的版本比较麻烦,这个是个坑; 一个Java示例文件, 放了一个MySQL =\u0026gt; MySQL 和一个 MySQL =\u0026gt; HDFS 的例子,详见注释; Java里面写了一个自动读取数据源的方法, 把所有用到的数据源按下文给定的xml格式配置好,放到resources/db下面即可被程序读取. 目录结构 ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── turingdi │ │ └── kettle │ │ └── demo │ │ └── App.java │ └── resources │ ├── db │ │ └── 235test.xml │ └── log4j.properties └── test └── java └── com └── turingdi └── kettle └── demo └── AppTest.java 代码 pom.xml 给出核心的变量和依赖部分:\n\u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;pentaho.kettle.version\u0026gt;4.1.0-stable\u0026lt;/pentaho.kettle.version\u0026gt; \u0026lt;pentaho.kettle.plugin.version\u0026gt;8.0.0.4-247\u0026lt;/pentaho.kettle.plugin.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;updatePolicy\u0026gt;daily\u0026lt;/updatePolicy\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;id\u0026gt;pentaho\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://nexus.pentaho.org/content/groups/omni/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho-kettle\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kettle-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho-kettle\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kettle-db\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.4.0-stable\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho-kettle\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kettle-engine\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho-kettle\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kettle-ui-swt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pentaho-big-data-kettle-plugins-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pentaho-big-data-api-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pentaho-big-data-impl-cluster\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--插件所需依赖开始--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;pentaho\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;metastore\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pentaho.kettle.plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.pentaho\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pentaho-hadoop-shims-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-configuration\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-configuration\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.9\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--插件所需依赖结束--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.0-cdh5.9.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.0-cdh5.9.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.0-cdh5.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-logging\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-logging\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-vfs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-vfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.17\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.scannotation\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scannotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.3\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;javassist\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javassist\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.javassist\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javassist\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.22.0-GA\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.turingdi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commonutils\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.41\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.11\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Java的示例 包含MySQL =\u0026gt; MySQL和MySQL =\u0026gt; HDFS 的方法:\npackage com.turingdi.kettle.demo; import com.turingdi.commonutils.basic.FileUtils; import org.pentaho.big.data.api.cluster.NamedCluster; import org.pentaho.big.data.impl.cluster.NamedClusterImpl; import org.pentaho.big.data.impl.cluster.NamedClusterManager; import org.pentaho.big.data.impl.cluster.NamedClusterServiceOsgiImpl; import org.pentaho.big.data.kettle.plugins.hdfs.trans.HadoopFileOutputMeta; import org.pentaho.di.cluster.ClusterSchema; import org.pentaho.di.core.Const; import org.pentaho.di.core.KettleEnvironment; import org.pentaho.di.core.NotePadMeta; import org.pentaho.di.core.database.Database; import org.pentaho.di.core.database.DatabaseMeta; import org.pentaho.di.core.exception.KettleException; import org.pentaho.di.core.plugins.PluginFolder; import org.pentaho.di.core.plugins.StepPluginType; import org.pentaho.di.core.util.EnvUtil; import org.pentaho.di.trans.Trans; import org.pentaho.di.trans.TransHopMeta; import org.pentaho.di.trans.TransMeta; import org.pentaho.di.trans.step.StepMeta; import org.pentaho.di.trans.steps.selectvalues.SelectValuesMeta; import org.pentaho.di.trans.steps.tableinput.TableInputMeta; import org.pentaho.di.trans.steps.tableoutput.TableOutputMeta; import org.pentaho.di.trans.steps.textfileoutput.TextFileField; import org.pentaho.runtime.test.action.impl.RuntimeTestActionServiceImpl; import org.pentaho.runtime.test.impl.RuntimeTesterImpl; import java.io.DataOutputStream; import java.io.File; import java.io.FileOutputStream; import java.io.IOException; import java.util.ArrayList; import java.util.List; import java.util.Objects; public class App { //存放读取到的xml字符串 private static String[] databasesXML; //kettle插件的位置 private static final String KETTLE_PLUGIN_BASE_FOLDER = \u0026#34;/Users/leibnizhu/Downloads/kettle/plugins\u0026#34;; public static void main(String[] args) throws KettleException, IOException { // 这几句必须有, 官网例子是错的, 用来加载插件的 System.setProperty(\u0026#34;hadoop.home.dir\u0026#34;, \u0026#34;/\u0026#34;); StepPluginType.getInstance().getPluginFolders().add(new PluginFolder(KETTLE_PLUGIN_BASE_FOLDER,true, true)); EnvUtil.environmentInit(); KettleEnvironment.init(); // 加载db目录下的所有存放数据库配置的xml文件 // 这个在官方例子也是没有的, 自己写的, 而且没给出xml的例子, google到的一篇博客里面的, 坑死了 String rootPath = App.class.getResource(\u0026#34;/\u0026#34;).getPath(); File dbDir = new File(rootPath, \u0026#34;db\u0026#34;); List\u0026lt;String\u0026gt; xmlStrings = new ArrayList\u0026lt;\u0026gt;(); for (File xml : Objects.requireNonNull(dbDir.listFiles())) { if (xml.isFile() \u0026amp;\u0026amp; xml.getName().endsWith(\u0026#34;.xml\u0026#34;)) { xmlStrings.add(FileUtils.ReadFile(xml.getAbsolutePath())); } } databasesXML = xmlStrings.toArray(new String[0]); // 调用下面的方法, 创建一个复制数据库表的Transform任务 // TransMeta transMeta = buildCopyTable( // \u0026#34;trans\u0026#34;, // \u0026#34;235test\u0026#34;, // \u0026#34;user\u0026#34;, // new String[]{\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;}, // \u0026#34;235test\u0026#34;, // \u0026#34;user2\u0026#34;, // new String[]{\u0026#34;id2\u0026#34;, \u0026#34;name2\u0026#34;} // ); TransMeta transMeta = buildCopyTableToHDFS( \u0026#34;trans\u0026#34;, \u0026#34;235test\u0026#34;, \u0026#34;user\u0026#34;, new String[]{\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;} ); // 把以上transform保存到文件, 这样可以用Spoon打开,检查下有没有问题 String fileName = \u0026#34;/Users/leibnizhu/Desktop/test.ktr\u0026#34;; String xml = transMeta.getXML(); DataOutputStream dos = new DataOutputStream(new FileOutputStream(new File(fileName))); dos.write(xml.getBytes(\u0026#34;UTF-8\u0026#34;)); dos.close(); System.out.println(\u0026#34;Saved transformation to file: \u0026#34; + fileName); // 生成SQL,用于创建表(如果不存在的话) String sql = transMeta.getSQLStatementsString(); // 执行以上SQL语句创建表 Database targetDatabase = new Database(transMeta.findDatabase(\u0026#34;235test\u0026#34;)); targetDatabase.connect(); targetDatabase.execStatements(sql); // 执行transformation... Trans trans = new Trans(transMeta); trans.execute(null); trans.waitUntilFinished(); // 断开数据库连接 targetDatabase.disconnect(); } /** * Creates a new Transformation using input parameters such as the tablename to read from. * * @param transformationName The name of the transformation * @param sourceDatabaseName 数据源, 对应xml里面的name字段 * @param sourceTableName The name of the table to read from * @param sourceFields The field names we want to read from the source table * @param targetDatabaseName 复制的去向, 对应xml里面的name字段 * @param targetTableName The name of the target table we want to write to * @param targetFields The names of the fields in the target table (same number of fields as sourceFields) * @return A new transformation metadata object * @throws KettleException In the rare case something goes wrong */ private static TransMeta buildCopyTable(String transformationName, String sourceDatabaseName, String sourceTableName, String[] sourceFields, String targetDatabaseName, String targetTableName, String[] targetFields) throws KettleException { try { // 创建transformation... TransMeta transMeta = new TransMeta(); transMeta.setName(transformationName); // 增加数据库连接的元数据 for (String aDatabasesXML : databasesXML) { DatabaseMeta databaseMeta = new DatabaseMeta(aDatabasesXML); transMeta.addDatabase(databaseMeta); } DatabaseMeta sourceDBInfo = transMeta.findDatabase(sourceDatabaseName); DatabaseMeta targetDBInfo = transMeta.findDatabase(targetDatabaseName); // 增加备注 String note = \u0026#34;Reads information from table [\u0026#34; + sourceTableName + \u0026#34;] on database [\u0026#34; + sourceDBInfo + \u0026#34;]\u0026#34; + Const.CR + \u0026#34;After that, it writes the information to table [\u0026#34; + targetTableName + \u0026#34;] on database [\u0026#34; + targetDBInfo + \u0026#34;]\u0026#34;; NotePadMeta ni = new NotePadMeta(note, 150, 10, -1, -1); transMeta.addNote(ni); // 创建读数据库的step String fromStepName = \u0026#34;read from [\u0026#34; + sourceTableName + \u0026#34;]\u0026#34;; TableInputMeta tii = new TableInputMeta(); tii.setDatabaseMeta(sourceDBInfo); tii.setSQL(\u0026#34;SELECT \u0026#34; + Const.CR + String.join(\u0026#34;,\u0026#34;, sourceFields) + \u0026#34; \u0026#34; + \u0026#34;FROM \u0026#34; + sourceTableName); StepMeta fromStep = new StepMeta(fromStepName, tii); //以下几句是给Spoon看的, 用处不大 fromStep.setLocation(150, 100); fromStep.setDraw(true); fromStep.setDescription(\u0026#34;Reads information from table [\u0026#34; + sourceTableName + \u0026#34;] on database [\u0026#34; + sourceDBInfo + \u0026#34;]\u0026#34;); transMeta.addStep(fromStep); // 创建一个修改字段名的step SelectValuesMeta svi = new SelectValuesMeta(); //配置字段名修改的规则, 这里跟官方例子差别很大, 坑不少 svi.allocate(sourceFields.length, 0, 0); for (int i = 0; i \u0026lt; sourceFields.length; i++) { svi.getSelectName()[i] = sourceFields[i]; svi.getSelectRename()[i] = targetFields[i]; } String selStepName = \u0026#34;Rename field names\u0026#34;; StepMeta selStep = new StepMeta(selStepName, svi); //以下几句是给Spoon看的, 用处不大 selStep.setLocation(350, 100); selStep.setDraw(true); selStep.setDescription(\u0026#34;Rename field names\u0026#34;); transMeta.addStep(selStep); //建立读数据库step与修改字段名step的连接,增加到transformation中 TransHopMeta shi = new TransHopMeta(fromStep, selStep); transMeta.addTransHop(shi); // 创建一个输出到表的step String toStepName = \u0026#34;write to [\u0026#34; + targetTableName + \u0026#34;]\u0026#34;; TableOutputMeta toi = new TableOutputMeta(); toi.setDatabaseMeta(targetDBInfo); toi.setTablename(targetTableName); toi.setCommitSize(200); toi.setTruncateTable(true); toi.setSchemaName(\u0026#34;test\u0026#34;); toi.setTruncateTable(false); StepMeta toStep = new StepMeta(toStepName, toi); //以下几句是给Spoon看的, 用处不大 toStep.setLocation(550, 100); toStep.setDraw(true); toStep.setDescription(\u0026#34;Write information to table [\u0026#34; + targetTableName + \u0026#34;] on database [\u0026#34; + targetDBInfo + \u0026#34;]\u0026#34;); transMeta.addStep(toStep); // 建立修改字段名step到输出到数据库step的连接 TransHopMeta hi = new TransHopMeta(selStep, toStep); transMeta.addTransHop(hi); // 返回 return transMeta; } catch (Exception e) { throw new KettleException(\u0026#34;An unexpected error occurred creating the new transformation\u0026#34;, e); } } private static TransMeta buildCopyTableToHDFS(String transformationName, String sourceDatabaseName, String sourceTableName, String[] sourceFields) throws KettleException { try { // 创建transformation... TransMeta transMeta = new TransMeta(); transMeta.setName(transformationName); // 增加数据库连接的元数据 for (String aDatabasesXML : databasesXML) { DatabaseMeta databaseMeta = new DatabaseMeta(aDatabasesXML); transMeta.addDatabase(databaseMeta); } DatabaseMeta sourceDBInfo = transMeta.findDatabase(sourceDatabaseName); // 增加备注 String note = \u0026#34;Reads information from table [\u0026#34; + sourceTableName + \u0026#34;] on database [\u0026#34; + sourceDBInfo + \u0026#34;]\u0026#34; + Const.CR + \u0026#34;After that, it writes the information to HDFS ]\u0026#34;; NotePadMeta ni = new NotePadMeta(note, 150, 10, -1, -1); transMeta.addNote(ni); // 创建读数据库的step String fromStepName = \u0026#34;read from [\u0026#34; + sourceTableName + \u0026#34;]\u0026#34;; TableInputMeta tii = new TableInputMeta(); tii.setDatabaseMeta(sourceDBInfo); tii.setSQL(\u0026#34;SELECT \u0026#34; + Const.CR + String.join(\u0026#34;,\u0026#34;, sourceFields) + \u0026#34; \u0026#34; + \u0026#34;FROM \u0026#34; + sourceTableName); StepMeta fromStep = new StepMeta(fromStepName, tii); //以下几句是给Spoon看的, 用处不大 fromStep.setLocation(150, 100); fromStep.setDraw(true); fromStep.setDescription(\u0026#34;Reads information from table [\u0026#34; + sourceTableName + \u0026#34;] on database [\u0026#34; + sourceDBInfo + \u0026#34;]\u0026#34;); transMeta.addStep(fromStep); NamedClusterManager clusterManager = new NamedClusterManager(); NamedCluster cluster = new NamedClusterImpl(); cluster.setStorageScheme(\u0026#34;hdfs\u0026#34;); cluster.setHdfsHost(\u0026#34;bitest01\u0026#34;); cluster.setHdfsPort(\u0026#34;8020\u0026#34;); cluster.setName(\u0026#34;cloudera\u0026#34;); cluster.setHdfsUsername(\u0026#34;\u0026#34;); cluster.setHdfsPassword(\u0026#34;\u0026#34;); clusterManager.setClusterTemplate(cluster); // transMeta.setNamedClusterServiceOsgi(); // clusterManager.getClusterTemplate().setHdfsHost(\u0026#34;bitest01\u0026#34;); HadoopFileOutputMeta hadoopOut = new HadoopFileOutputMeta(clusterManager, null, null); // new RuntimeTestActionServiceImpl(null, null), // new RuntimeTesterImpl(null, null, \u0026#34;test\u0026#34;)); hadoopOut.setOutputFields(new TextFileField[]{}); hadoopOut.setFilename(\u0026#34;hdfs://bitest01:8020/tmp/aa\u0026#34;); hadoopOut.setExtension(\u0026#34;txt\u0026#34;); hadoopOut.setFileCompression(\u0026#34;None\u0026#34;); hadoopOut.setSourceConfigurationName(\u0026#34;Cloudera\u0026#34;); hadoopOut.setSeparator(\u0026#34;,\u0026#34;); hadoopOut.setFileFormat(\u0026#34;UNIX\u0026#34;); hadoopOut.setEncoding(\u0026#34;UTF-8\u0026#34;); StepMeta hadoopStep = new StepMeta(\u0026#34;HDFSOutput\u0026#34;, hadoopOut); hadoopStep.setLocation(550, 100); hadoopStep.setDraw(true); transMeta.addStep(hadoopStep); TransHopMeta hhm = new TransHopMeta(fromStep, hadoopStep); transMeta.addTransHop(hhm); // 返回 return transMeta; } catch (Exception e) { throw new KettleException(\u0026#34;An unexpected error occurred creating the new transformation\u0026#34;, e); } } } 存储数据库源配置的xml文件 放在resources/db下面:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;connection\u0026gt; \u0026lt;name\u0026gt;test\u0026lt;/name\u0026gt; \u0026lt;server\u0026gt;192.168.1.*\u0026lt;/server\u0026gt; \u0026lt;type\u0026gt;MySQL\u0026lt;/type\u0026gt; \u0026lt;access\u0026gt;Native\u0026lt;/access\u0026gt; \u0026lt;database\u0026gt;test\u0026lt;/database\u0026gt; \u0026lt;port\u0026gt;3306\u0026lt;/port\u0026gt; \u0026lt;username\u0026gt;root\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;root\u0026lt;/password\u0026gt; \u0026lt;servername/\u0026gt; \u0026lt;data_tablespace/\u0026gt; \u0026lt;index_tablespace/\u0026gt; \u0026lt;attributes\u0026gt; \u0026lt;attribute\u0026gt; \u0026lt;code\u0026gt;EXTRA_OPTION_MYSQL.defaultFetchSize\u0026lt;/code\u0026gt; \u0026lt;attribute\u0026gt;500\u0026lt;/attribute\u0026gt; \u0026lt;/attribute\u0026gt; \u0026lt;attribute\u0026gt; \u0026lt;code\u0026gt;EXTRA_OPTION_MYSQL.useCursorFetch\u0026lt;/code\u0026gt; \u0026lt;attribute\u0026gt;true\u0026lt;/attribute\u0026gt; \u0026lt;/attribute\u0026gt; \u0026lt;attribute\u0026gt; \u0026lt;code\u0026gt;EXTRA_OPTION_MYSQL.useSSL\u0026lt;/code\u0026gt; \u0026lt;attribute\u0026gt;false\u0026lt;/attribute\u0026gt; \u0026lt;/attribute\u0026gt; \u0026lt;attribute\u0026gt; \u0026lt;code\u0026gt;EXTRA_OPTION_MYSQL.useUnicode\u0026lt;/code\u0026gt; \u0026lt;attribute\u0026gt;true\u0026lt;/attribute\u0026gt; \u0026lt;/attribute\u0026gt; \u0026lt;attribute\u0026gt; \u0026lt;code\u0026gt;EXTRA_OPTION_MYSQL.characterEncoding\u0026lt;/code\u0026gt; \u0026lt;attribute\u0026gt;UTF-8\u0026lt;/attribute\u0026gt; \u0026lt;/attribute\u0026gt; \u0026lt;/attributes\u0026gt; \u0026lt;/connection\u0026gt; ","date":"2018-05-15T17:29:56+08:00","image":"https://leibnizhu.github.io/p/Kettle-Java-API%E5%A4%84%E7%90%86Hadoop%E6%95%B0%E6%8D%AE/yyz_hu9ab84cc96e37a17e62de568a816eda25_352465_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Kettle-Java-API%E5%A4%84%E7%90%86Hadoop%E6%95%B0%E6%8D%AE/","title":"Kettle Java API处理Hadoop数据"},{"content":"最近在写一个Vert.X+Lucene的搜索引擎,为Vert.X中国论坛提供的(详见Github),在尝试更新文档索引的时候,遇到如何获取所有有效文档的问题(未被删除索引的). 官方没有直接查询的API,在Google和Stack Overflow搜了之后没有满意的效果,网上的资源很多都是Lucene 6甚至更老的3,4版本的,有些API已经过时.\n经过深入查阅官方API文档之后,终于找到了解决方案.\nLucene删除索引之后,通过IndexReader.document()还是可以查到的,因此当有文档被删除后,不能直接用这个来查; 而MultiFields.getLiveDocs()在没有删除文档时返回null,有删除过文档时返回一个Bits对象,这里面可以通过get()方法,获取每个documentID是否有效(未被删除).\n因此可以这样写:\nprivate val indexDirectory = FSDirectory.open(Paths.get(indexDirectoryPath)) private var reader: DirectoryReader = DirectoryReader.open(indexDirectory) var indexSearcher = new IndexSearcher(reader) /** * 获取所有有效文档 * * @return */ def getAllDocuments: List[Document] = { //获取有哪些存活的文档 val liveDocs = MultiFields.getLiveDocs(reader) if (liveDocs != null) //liveDocs非null时有删除过文件,遍历所有文档ID,liveDocs.get为true的话就是存活的,要过滤存活的文档对象 (0 until reader.maxDoc()).filter(liveDocs.get).map(reader.document(_)).toList else //没有删除过文件的时候liveDocs为null,此时只能直接通过IndexSearcher去查询 topDocsToDocumentList(indexSearcher.search(new MatchAllDocsQuery, Integer.MAX_VALUE)) } def topDocsToDocumentList(topDocs: TopDocs): List[Document] = topDocs.scoreDocs.map(this.getDocument).toList ","date":"2018-05-15T16:38:41+08:00","image":"https://leibnizhu.github.io/p/Lucene7%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E6%9C%89%E6%95%88%E6%96%87%E6%A1%A3/mikuwing_hube5fd7f80f202177cfb1d1e7f3f6c08e_248194_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Lucene7%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E6%9C%89%E6%95%88%E6%96%87%E6%A1%A3/","title":"Lucene7获取所有有效文档"},{"content":"介绍在部署了Kerberos的安全Hadoop集群中, Sqoop,Hive,HBase,Kafka,Maxwell使用方法.\nSqoop使用 配置好Kerberos之后, sqoop不能直接使用, 需要进行一些配置:\n分配sqoop的组, 执行usermod -a -G hdfs sqoop加入到hdfs组, 使用groups sqoop确认执行成功; 进入Hue的用户管理界面, 新增sqoop用户, 在hdfs用户组中; 在Hue的HDFS文件管理页面中, 创建/user/sqoop\u0008目录, 从属于sqoop:hdfs用户/用户组; 进入cdh1, 创建Kerberos用户, 名为sqoop, 可以导出keytab; 使用kinit命令切换到sqoop用户(在脚本中可以使用keytab切换) 执行sqoop命令 Spark访问HBase 进入cdh1, 创建Kerberos用户, 名为hbase; 导出keytab, 名为hbase.keytab, 保存到本地; 下载krb5.conf到本地. 创建测试类, 并执行, 代码如下: /*HBase测试*/ object KerberosHBaseTest { def main(args: Array[String]) { val zkHosts = \u0026#34;cdh2:2181,cdh3:2181,cdh4:2181\u0026#34; System.setProperty(\u0026#34;java.security.krb5.conf\u0026#34;, \u0026#34;/path/to/krb5.conf\u0026#34;) //krb5.conf本地路径 val sparkConf = new SparkConf().setAppName(\u0026#34;KerberosHBaseTest\u0026#34;).setMaster(\u0026#34;local\u0026#34;) val sc = new SparkContext(sparkConf) //配置HBase连接 val hbaseConfig = HBaseConfiguration.create() hbaseConfig.set(\u0026#34;hbase.zookeeper.quorum\u0026#34;, zkHosts) hbaseConfig.set(\u0026#34;zookeeper.znode.parent\u0026#34;, \u0026#34;/hbase\u0026#34;) //设置集群和hbase的安全模式为kerberos hbaseConfig.set(\u0026#34;hadoop.security.authentication\u0026#34;, \u0026#34;kerberos\u0026#34;) hbaseConfig.set(\u0026#34;hbase.security.authentication\u0026#34;, \u0026#34;kerberos\u0026#34;) hbaseConfig.set(\u0026#34;hbase.master.kerberos.principal\u0026#34;, \u0026#34;hbase/_HOST@TURINGDI.COM\u0026#34;) //没有似乎也行 hbaseConfig.set(\u0026#34;hbase.regionserver.kerberos.principal\u0026#34;, \u0026#34;hbase/_HOST@TURINGDI.COM\u0026#34;) //必须有 UserGroupInformation.setConfiguration(hbaseConfig) UserGroupInformation.loginUserFromKeytab(\u0026#34;hbase\u0026#34;, \u0026#34;/path/to/hbase.keytab\u0026#34;) //\u0008Kerberos用户名, keytab本地路径 //设置广播变量，解决序列化问题 //HBase配置 val broadcastHBaseConf = sc.broadcast(new SerializableWritable(hbaseConfig)) //HBase连接工具类 val hbaseConnection = sc.broadcast(HBaseConnection(broadcastHBaseConf)) val result = scanByStartTimestamp(hbaseConnection, \u0026#34;t1\u0026#34;, 0L) result.foreach(r =\u0026gt; println(ConvertService.convertResultToHBaseRow(r))) sc.stop() } /** * 从HBase中scan指定表的所有列，从指定的时间戳开始 * * @param hBaseConnection HBase连接 * @param tableName 表名 * @param starTimestamp 开始scan的时间戳，从该时间戳scan到当前时间 * @return scan的结果，Result的List * @author Leibniz */ def scanByStartTimestamp(hBaseConnection: Broadcast[HBaseConnection], tableName: String, starTimestamp: Long): ArrayBuffer[Result] = { val resultList = new ArrayBuffer[Result]() Try({ val scan = new Scan() scan.setTimeRange(starTimestamp, System.currentTimeMillis) // 获取表 val table = hBaseConnection.value.connection.getTable(TableName.valueOf(tableName)) table.getScanner(scan).foreach(resultList.+=) }).recover({ case e: Throwable =\u0026gt; log.error(\u0026#34;从HBase表{}中按时间戳({}-\u0026gt;NOW)scan时抛出异常:{}\u0026#34;, Seq[AnyRef](tableName, starTimestamp.toString, e.getMessage): _*) }) resultList } } Spark访问Hive Hive可以沿用hbase的Kerberos\u0008用户, 也可以新建一个Hive用户及其对应\u0008keytab文件. 本地测试请将集群的hive-site.xml导出并保存在项目的src/main/resources/目录下; 编写Spark测试程序: /*Hive测试*/ object KerberosHiveTest { def main(args: Array[String]) { System.setProperty(\u0026#34;java.security.krb5.conf\u0026#34;, \u0026#34;/path/to/krb5.conf\u0026#34;) //krb5.conf本地路径 val sparkConf = new SparkConf().setAppName(\u0026#34;KerberosHiveTest\u0026#34;).setMaster(\u0026#34;local\u0026#34;) val sc = new SparkContext(sparkConf) val config = HBaseConfiguration.create() config.set(\u0026#34;hadoop.security.authentication\u0026#34;, \u0026#34;kerberos\u0026#34;) //必须有 UserGroupInformation.setConfiguration(config) UserGroupInformation.loginUserFromKeytab(\u0026#34;hbase\u0026#34;, \u0026#34;/path/to/hbase.keytab\u0026#34;) //\u0008Kerberos用户名, keytab本地路径 val sparkSession = SparkSession.builder.master(\u0026#34;local\u0026#34;).appName(\u0026#34;KerberosHiveTest\u0026#34;).enableHiveSupport() .config(\u0026#34;yarn.resourcemanager.principal\u0026#34;, \u0026#34;rm/_HOST@TURINGDI.COM\u0026#34;) //必须有 // .config(\u0026#34;spark.yarn.keytab\u0026#34;, \u0026#34;/path/to/hbase.keytab\u0026#34;) // .config(\u0026#34;spark.yarn.principal\u0026#34;, \u0026#34;hbase@TURINGDI.COM\u0026#34;) .getOrCreate() val dataFrame = sparkSession.sql(\u0026#34;select * from hivetest2\u0026#34;) dataFrame.rdd.foreach(row =\u0026gt; println(row.getInt(0) + \u0026#34; -\u0026gt; \u0026#34; + row.getString(1))) sc.stop() } } Spark访问Kafka 进入Cloudera Manager的Kafka配置页面, 搜索\u0026rsquo;Inter Broker Protocol\u0026rsquo;, 更改为\u0026rsquo;SASL_PLAINTEXT'; 重启Kafka配置; 进入cdh1, 创建Kerberos用户, 名为kafka; 导出keytab, 名为kafka.keytab, 并保存到本地(测试用); cdh1中新建一个jaas.conf配置文件, 并复制到本地(注意修改keyTab), 内容如下: KafkaClient { com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useTicketCache=true useKeyTab=true principal=\u0026#34;kafka@TURINGDI.COM\u0026#34; #根据实际修改 serviceName=\u0026#34;kafka\u0026#34; ## 固定 client=true keyTab=\u0026#34;/path/to/kafka.keytab\u0026#34;; ## keytab路径,节点和本地按实际路径填写 }; cdh1中新建一个kafka.properties文件, 内容如下: security.protocol=SASL_PLAINTEXT sasl.kerberos.service.name=kafka sasl.mechanism=GSSAPI security.inter.broker.protocol=SASL_PLAINTEXT 编写Spark程序进行测试: object KerberosKafkaTest { def main(args: Array[String]) { val zkHosts = \u0026#34;cdh2:2181,cdh3:2181,cdh4:2181\u0026#34; val kafkaBrokers = \u0026#34;cdh2:9092,cdh3:9092,cdh4:9092\u0026#34; val topics = List(\u0026#34;maxwell\u0026#34;) System.setProperty(\u0026#34;java.security.krb5.conf\u0026#34;, \u0026#34;/path/to/krb5.conf\u0026#34;) //本地krb5.conf路径 System.setProperty(\u0026#34;java.security.auth.login.config\u0026#34;, \u0026#34;/path/to/jaas.conf\u0026#34;)//本地jaas.conf路径 // 创建流处理上下文，并以启动参数指定的秒数为时间间隔做一次批处理。 val sparkConf = new SparkConf().setAppName(\u0026#34;KerberosKafkaTest\u0026#34;).set(\u0026#34;spark.streaming.kafka.consumer.poll.ms\u0026#34;, KAFKA_CONSUMER_POLL_MS).setMaster(\u0026#34;local\u0026#34;) val ssc = new StreamingContext(sparkConf, Seconds(10)) // 配置并创建Kafka输入流 // 如果zookeeper没有offset值或offset值超出范围，就给个初始的offset // 有earliest、largest可选，分别表示给当前最小的offset、当前最大的offset。默认largest val kafkaParams = Map[String, Object]( \u0026#34;auto.offset.reset\u0026#34; -\u0026gt; \u0026#34;earliest\u0026#34;, \u0026#34;bootstrap.servers\u0026#34; -\u0026gt; kafkaBrokers, \u0026#34;group.id\u0026#34; -\u0026gt; \u0026#34;testGroup\u0026#34;, \u0026#34;enable.auto.commit\u0026#34; -\u0026gt; (false: java.lang.Boolean), //禁用自动提交Offset，否则可能没正常消费完就提交了，造成数据错误 \u0026#34;key.deserializer\u0026#34; -\u0026gt; classOf[StringDeserializer], \u0026#34;value.deserializer\u0026#34; -\u0026gt; classOf[StringDeserializer], \u0026#34;sasl.kerberos.service.name\u0026#34; -\u0026gt; \u0026#34;kafka\u0026#34;, //必须有 \u0026#34;security.protocol\u0026#34; -\u0026gt; \u0026#34;SASL_PLAINTEXT\u0026#34;) //与Kafka配置一致 val kafkaStream = KafkaUtils.createDirectStream(ssc, PreferConsistent, ConsumerStrategies.Subscribe(topics, kafkaParams)) kafkaStream.foreachRDD(rdd =\u0026gt; { log.info(\u0026#34;接收到{}条Kafka消息\u0026#34;, rdd.count) rdd.foreach(message =\u0026gt; { println(\u0026#34;partition=\u0026#34; + message.partition + \u0026#34;, value=\u0026#34; + message.value + \u0026#34;, offset=\u0026#34; + message.offset.toString) }) }) ssc.start() ssc.awaitTermination() } } kafka自带的命令, 如kafka-console-consumer, kafka-topics还不能使用, 若要使用, 需要先执行: export KAFKA_OPTS=\u0026#34;-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/path/to/jaas.conf\u0026#34; 注意修改其中的jass.conf路径, 并确保其中配置的keytab存在; 再执行相应的kafka命令.\n如果觉得麻烦, 也可以编辑/opt/cloudera/parcels/KAFKA-3.0.0-1.3.0.0.p0.40/lib/kafka/bin/kafka-run-class.sh, 在exec $JAVA后面增加:\n-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/root/jaas.conf Maxwell配置 编辑${MAXWELL_HOME}/bin/maxwell, 在文件尾部附件的exec $JAVA $JAVA_OPTS后面增加: -Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/root/jaas.conf 编辑一个config.properties文件, 内容如下: kafka.security.protocol=SASL_PLAINTEXT kafka.sasl.kerberos.service.name=kafka kafka.sasl.mechanism=GSSAPI security.inter.broker.protocol=SASL_PLAINTEXT sasl.mechanism.inter.broker.protocol=PLAIN 在maxwell启动命令中增加参数: --config /path/to/config.properties ","date":"2018-03-07T16:23:56+08:00","image":"https://leibnizhu.github.io/p/Kerberos%E9%9B%86%E7%BE%A4%E7%9A%84SqoopHiveHBaseKafkaMaxwell%E4%BD%BF%E7%94%A8/flower_hu965ea70575a1884b164616c88e5f50b7_79930_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Kerberos%E9%9B%86%E7%BE%A4%E7%9A%84SqoopHiveHBaseKafkaMaxwell%E4%BD%BF%E7%94%A8/","title":"Kerberos集群的Sqoop,Hive,HBase,Kafka,Maxwell使用"},{"content":"下文以本地测试集群为例, 4节点(cdh1-4), cdh1为NameNode, cdh2-4为DataNode.\n基础概念 Kerberos principal用于在kerberos加密系统中标记一个唯一的身份。\nkerberos为kerberos principal分配tickets使其可以访问由kerberos加密的hadoop服务。\n对于hadoop，principals的格式为username/fully.qualified.domain.name@YOUR-REALM.COM.\nkeytab是包含principals和加密principal key的文件。\nkeytab文件对于每个host是唯一的，因为key中包含hostname。keytab文件用于不需要人工交互和保存纯文本密码，实现到kerberos上验证一个主机上的principal。\n因为服务器上可以访问keytab文件即可以以principal的身份通过kerberos的认证，所以，keytab文件应该被妥善保存，应该只有少数的用户可以访问。\nKDC服务安装及配置 安装KDC服务 选择NameNode节点(cdh1)安装KDC服务, 执行:\nyum -y install krb5-server krb5-libs krb5-auth-dialog krb5-workstation openldap-clients 其他节点(cdh2-4)只安装Kerberos客户端, 执行:\nyum -y install krb5-libs krb5-workstation 配置KDC服务 1.编辑/etc/krb5.conf:\n## Configuration snippets may be placed in this directory as well includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true #rdns = false default_realm = TURINGDI.COM #随意定义一个域 #default_ccache_name = KEYRING:persistent:%{uid} [realms] #与上面default_realm一致, 配置KDC服务所在的服务器 TURINGDI.COM = { kdc = cdh1 admin_server = cdh1 } [domain_realm] .turingdi.com = TURINGDI.COM turingdi.com = TURINGDI.COM 2.将/etc/krb5.conf复制到每个节点的/etc/目录下.\n3.修改/var/kerberos/krb5kdc/kadm5.acl, 配置用户名包含/admin的用户都是管理员用户:\n*/admin@TURINGDI.COM * 4.修改修改/var/kerberos/krb5kdc/kdc.conf, 配置令牌的生命周期, 并设置默认允许重新生成令牌:\n[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] TURINGDI.COM = { #master_key_type = aes256-cts max_renewable_life= 7d 0h 0m 0s default_principal_flags = +renewable acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } 创建Kerberos数据库 在cdh1执行以下命令, 注意域名:\nkdb5_util create –r TURINGDI.COM -s 按提示设置密码并重复密码.\n创建Kerberos的管理账号 在cdh1执行:\nkadmin.local 依次输入:\naddprinc admin/admin@TURINGDI.COM ## 按提示设置管理账号的密码并重复密码 exit 配置服务自启动 在cdh1执行:\nchkconfig krb5kdc on hkconfig kadmin on service krb5kdc start service kadmin start 然后尝试登陆Kerberos的管理员账号:\nkinit admin/admin@TURINGDI.COM ## 3. 输入\u0008刚才设定的管理账号密码 klist 应该会输出类似:\nTicket cache: FILE:/tmp/krb5cc_0 Default principal: admin/admin@TURINGDI.COM Valid starting Expires Service principal 2018-03-06T16:48:23 2018-03-07T16:48:23 krbtgt/TURINGDI.COM@TURINGDI.COM renew until 2018-03-13T16:48:23 \u0008即配置成功.\nCDH集群启用Kerberos 进入Cloudera Manager的“管理”-\u0026gt; “安全”界面:\n点击\u0026quot;启用Kerberos\u0026quot;按钮, 确保列出的所有检查项都已完成并勾选, 点击\u0026quot;继续\u0026quot;按钮:\n配置相关的KDC信息，包括类型、KDC服务器、KDC Realm、加密类型以及待创建的Service Principal（hdfs，yarn,，hbase，hive等）的更新生命期等, 与/etc/krb5.conf的配置一致, 点击\u0026quot;继续\u0026quot;按钮. 取消勾选\u0026quot;通过Cloudera Manager管理krb5.conf\u0026quot;, 点击\u0026quot;继续\u0026quot;按钮. 输入Cloudera Manager的Kerbers管理员账号，必须和之前创建的账号一致，点击\u0026quot;继续\u0026quot;. 最后点击\u0026quot;继续\u0026quot;, 勾选重启集群, 点击\u0026quot;继续\u0026quot;按钮, 等待配置重启集群. AES-256加密与JCE 对于使用centos5.6及以上的系统，默认使用AES-256来加密的。这就需要集群中的所有节点上安装JCE.\n打开http://www.oracle.com/technetwork/java/javase/downloads/index.html, 下载jdk对应的JCE文件.\n解压后的文件放入${JAVA_HOME}/jre/lib/security/中.\nKerberos的基础使用 Yarn配置 打开Cloudera Manager的Yarn配置页面, 搜索min.user, 修改为0, 然后按提示重启Yarn.\n使用Kerberos需要新建一些用户, 其id可能小于1000, 使用Yarn的默认配置可能会导致一些用户不能提交Yarn任务.\n导出keytab 进入cdh1, 输入kadmin.local, 输入以下命令:\nxst -k /path/to/*.keytab -norandkey \u0026lt;principal\u0026gt; 其中principal为需要导出keytab的用户名, 如hbase/cdh2, 注意-norandkey参数不可缺少, 否则可能会导致重新生成密码, 导致keytab失效.\n\u0008导出的keytab的效用等同账号密码, 请注意妥善保管.\n以某个Kerberos用户登录 两种方法:\nkinit 用户名@域名, 输入密码; kinit 用户名@域名 -k -t 对应keytab文件 后者无需输入密码, 适合在脚本中使用.\n一些组件对Kerberos的令牌有限制, 需要登录对应用户后才能使用, 包括HDFS的文件访问控制, 需要在Kerberos中建立对应的用户.\n创建Kerberos用户 前面已经使用过了, 进入cdh1, 输入kadmin.local, 输入以下命令:\naddprinc 用户名@域名 ## 按提示设置管理账号的密码并重复密码 ","date":"2018-03-07T16:23:33+08:00","image":"https://leibnizhu.github.io/p/Kerberos%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/room_huaaa06b625fbe727246165d1355526318_112362_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Kerberos%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/","title":"Kerberos部署,配置与基础使用"},{"content":"最近在CDH集群部署Sentry和Kerberos遇到了不少坑, 把过程总结一下, 都放上来吧.\nSentry组件安装 进入Cloudera Manager页面, 点击集群名右边的倒三角按钮, 选择\u0026quot;添加服务\u0026quot;:\n选择Sentry组件, 点击\u0026quot;继续\u0026quot;:\n选择集群主节点作为Sentry Server, 选择所有节点为Gateway, 然后点击\u0026quot;继续\u0026quot;:\n在集群元数据MySQL中执行:\ncreate database sentry default character set utf8 default collate utf8_general_ci; grant all on sentry.* to \u0026#39;sentry\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;sentrypassword\u0026#39;; 创建Sentry所需的数据库, 然后在Sentry安装页面中填上MySQL的地址账号密码, 点击继续, 等待安装和首次启动完毕.\nSentry及相关组件配置 Hue配置 进入Hue配置, 找到\u0026quot;Sentry 服务\u0026quot;, 选择Sentry:\nHive配置 进入Hive配置, 找到\u0026quot;Sentry 服务\u0026quot;, 选择Sentry:\n找到\u0026quot;HiveServer2 启用模拟\u0026quot;, 取消勾选:\n找到\u0026quot;sentry-site.xml 的 Hive 服务高级配置代码段（安全阀）\u0026quot;, 增加sentry.hive.testing.mode属性, 值为true:\nImpala配置 注: 需要确认集群每个节点都安装了Impala Daemon服务;如果Impala启动时提示cannot read or execute the parent directory of dfs.domain.socket.path, 则HDFS配置的dfs.client.read.shortcircuit勾选上, 并创建dfs.domain.socket.path的目录.\n进入Impala配置, 找到\u0026quot;Sentry 服务\u0026quot;, 选择Sentry:\nHDFS配置 进入HDFS配置, 找到\u0026quot;启用访问控制列表 dfs.namenode.acls.enabled\u0026quot;, 勾选:\n重启集群 Cloudera Manager会提示过期配置需要重启组件, 点击黄色\u0008圆形箭头, 点击\u0026quot;重启过时服务\u0026quot;, 并等待重启完成:\nHue用户权限配置 前提 Hue的用户权限体系是: 每个用户属于一个或多个组, 每个组可以配置其Hue页面访问权限及Hive/Solr/HDFS数据访问权限, 数据的访问权限由角色定义, 而用户组和角色之间是多对多关系.\nHue使用Sentry进行权限管理之后, 要求登录Hue的用户及其组需要在Sentry Server节点(以正式环境为例, 即gs01节点)Linux系统中存在对应的用户和组, 否则无法进行权限控制.\n目前已经在Hue创建了一个hdfs用户组(拥有最高权限), 包含用户admin及hdfs, 两者在gs01节点Linux系统中均存在. hdfs用户的存在主要考虑到Spark\u0008程序通过hue的oozie工作流提交时, 保证其执行权限.\n下文假定\u0008需要建立一个用户组hiveselect和用户hive1, 允许登录, 拥有部分hive表的select权限.\n创建Linux用户 ssh登录gs01节点, 执行:\ngroupadd hiveselect useradd -f hiveselect hive1 P.S. 上面命令建立的用户是没有密码的, 需要密码\u0008或其他选项的请自行添加参数.\n创建Hue用户 使用admin或hdfs用户登录Hue, 点击右上角用户名, 选择\u0026quot;Manage users\u0026quot;:\n点击\u0026quot;Group\u0026quot;选项卡, 点击\u0026quot;Add Group\u0026quot;按钮, \u0026ldquo;Name\u0026quot;填Linux的组名\u0026quot;hiveselect\u0026rdquo;, \u0026ldquo;members\u0026quot;为该组用户, 可以后期选择, \u0026ldquo;permissions\u0026quot;是该组用户的Hue页面访问权限, 其中\u0026quot;beeswax.access:Launch this application(2)\u0026ldquo;必选; 如果需要hive查询, 则需要选择\u0026quot;metadata.access:Launch this application(23)\u0026ldquo;和\u0026quot;metastore.access:Launch this application(12)\u0026rdquo;, 其他组件请根据具体需求而勾选:\n然后点击\u0026quot;Add group\u0026quot;按钮增加组.\n随后点击\u0026quot;Users\u0026quot;选项卡, 点击\u0026quot;Add user\u0026quot;按钮, Step 1填用户名(与Linux用户名一致)和Hue登录密码, Step 2选择所属用户组为\u0026quot;hiveselect\u0026rdquo;(与Linux用户组一致), 最后点击\u0026quot;Add user\u0026quot;按钮: 配置角色权限 点击左上角≡按钮, 再点击\u0026quot;Security\u0026rdquo;, 选择Hive Tables选项卡, 进行Hive访问权限\u0008配置.\n左侧选择Roles, 点击右边的\u0026quot;Add\u0026quot;按钮, \u0026ldquo;Name\u0026quot;中填写角色名, 可随意填写, \u0026ldquo;Groups\u0026quot;选择需要授予该角色的用户组, 此处选择了我们现在要处理的hiveselect用户组, \u0026ldquo;Privileges\u0026quot;中点击加号, 增加权限, 可以填写权限类型(select/insert/all), 及对应权限的库/表/列, 一个角色可以增加多条权限规则:\n填写完毕之后, 点击Save按钮, 稍等片刻即可生效(按目前经验来看, 新增的组可能需要3-5分钟才能生效, 已经配置过的组修改访问权限的话几乎是立刻生效).\nHive\u0008最高权限 Hue中配置的Hive权限是针对表的读写权限的, 并没有涉及到建库建表的权限, 使用管理员用户可以赋予该权限, 进入hue 的Hive编辑器, 执行:\ngrant all on server server1 to role hue角色名; ","date":"2018-03-07T16:23:22+08:00","image":"https://leibnizhu.github.io/p/Sentry%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/summer_hue6dbc15c317d55db6d7a714ffdf45d7a_70136_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Sentry%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8/","title":"Sentry部署,配置与使用"},{"content":"去年基于Vert.X写了一个微信支付宝公众号通用服务，放在Github上，也有一段时间没维护了，现在把文档发上来记录一下吧。\nGithub地址：https://github.com/Leibnizhu/AlipayWechatPlatform\nAlipay-Wechat-Platform 命名 原来命名是Wechat-Alipay-Platform，但这样缩写WAP、可能引起歧义，所以改成Alipay-Wechat-Platform，缩写AWP（还是有歧义，但至少不是一个领域的）。\n意义 本项目旨在减少Web项目关于微信、支付宝的重复代码，以及解决公众号的安全域名只能配置一个的问题。\n未来还可以托管Access Token。 本项目部署后，将公众号安全域名配置到本项目的域名，此后多个web项目（不同域名）可以使用同一个公众号 （理论上） 。\n技术栈 变迁 原来：Vue.js + Spring Boot + Druid + MySQL\n现在：Vue.js + Vert.X(Core+Web+JDBC) + HikariCP + MySQL(可能换成PostgreSQL)\nBenchmark 针对支付配置获取的API进行测试。\n测试工具：Jemeter3.2 r1790748 测试环境：Manjaro 17.0.5 x86_64 Linux 4.9.53-1-MANJARO,OpenJDK 1.8.0_144 硬件配置：Intel Core i7-6560U @ 4x 3.2GHz, 8G DDR3, 256G SSD Spring(无Shiro过滤)测试结果：\nVert.X(JWT授权+MariaDB)测试结果：\nVert.X(JWT授权+PostgreSQL)测试结果：\nMaven子模块 awp-base: PoJo类，工具类（通用工具、微信工具、支付宝工具） awp-final: 最终打成Vert.X整合包，包括后台管理和服务的入口 awp-verticle-bms: 后台管理页面，需要JWT授权登录 awp-verticle-base: Verticle基础通用类，包括一个基础接口机器抽象类，以及定义EventBus常量的命名空间 awp-verticle-db: 数据库相关类 awp-verticle-message: 微信、支付宝(模板/客服/图文)消息发送服务(TODO) awp-verticle-oauth: 微信、支付宝(TODO)授权服务 awp-verticle-pay: 微信、支付宝支付服务(TODO) 启动方式 增加支付宝Maven依赖 进入项目目录，执行以下命令：\nmvn install:install-file -Dfile=dependencies/com.alipay.api-1.0.jar -DgroupId=com.alipay -DartifactId=api -Dversion=1.0 -Dpackaging=jar mvn install:install-file -Dfile=dependencies/com.antgroup.zmxy.openplatform-1.0.jar -DgroupId=com.antgroup -DartifactId=zmxy.openplatform -Dversion=1.0 -Dpackaging=jar 配置文件 可以放在任何位置，任何文件名，内容参考awp-final/src/main/resources/config.json。\n命令行启动 mvn clean \u0026amp;\u0026amp; maven package java -jar awp-final/target/awp-0.0.1-SNAPSHOT-fat.jar run com.turingdi.awp.MainVerticle -conf [/path/to/配置文件] 调试 从awp-final子模块中的com.turingdi.awp.MainLauncher类启动项目即可，启动参数参考命令行启动的命令（从run开始）。\n后台管理页面 入口地址：http://localhost:8083/\nAPI 微信授权 申请微信授权。web服务需要授权时，向用户发送重定向到该接口。\n请求地址：http://localhost:8083/oauth/wx/apply/{body} 参数：body，格式为变种Base64编码的JSON，请用http://localhost:8083/static/page/sys/base64.html 进行编码 例如（请修改域名后，在微信打开，静默授权，授权后跳到百度首页(为了展示可以回调到任何地址)，观察地址，rs参数是图灵Base64加密后的结果）: http://localhost:8083/oauth/wx/apply/bgNVIODVIfwpZOI2dADsO3DVIOD3TmLgZSI2KOgxIODVIOkBHCjsHfqB1YI2IfhMTmD2oY60T0cuHfqpZm8uHt6nIVp6OV~~\n{ \u0026#34;eid\u0026#34;:1, /*web项目使用的公众号在本项目中的用户ID*/ \u0026#34;type\u0026#34;:0,/*0=静默授权，只能获取OpenID，1=正常授权，会弹出授权确认页面，可以获取到用户信息*/ \u0026#34;callback\u0026#34;:\u0026#34;http://dict.baidu.com\u0026#34;/*授权成功后调用的web项目回调接口地址,请使用完整地址,回调时会使用GET方法，加上rs参数，rs参数值是turingBase64加密的授权结果(JSON)*/ } 支付宝授权 申请支付宝授权。web服务需要授权时，向用户发送重定向到该接口。\n请求地址：http://localhost:8083/oauth/zfb/apply/{body} 参数：body，格式为变种Base64编码的JSON，请用http://localhost:8083/static/page/sys/base64.html 进行编码 例如（请修改域名后打开，静默授权，授权后跳到百度首页(为了展示可以回调到任何地址)，观察地址，rs参数是图灵Base64加密后的结果）: http://localhost:8083/oauth/wx/apply/bgNVIODVIfwpZOI2dADsO3DVIOD3TmLgZSI2KOgxIODVIOkBHCjsHfqB1YI2IfhMTmD2oY60T0cuHfqpZm8uHt6nIVp6OV~~\n{ \u0026#34;eid\u0026#34;:1, /*web项目使用的公众号在本项目中的用户ID*/ \u0026#34;type\u0026#34;:0,/*0=静默授权，只能获取OpenID，1=正常授权，会弹出授权确认页面，可以获取到用户信息*/ \u0026#34;callback\u0026#34;:\u0026#34;http://dict.baidu.com\u0026#34;/*授权成功后调用的web项目回调接口地址,请使用完整地址,回调时会使用GET方法，加上rs参数，rs参数值是turingBase64加密的授权结果(JSON)*/ } 微信公众号的AccessToken与JsTicket AccessToken 请求方法：POST 来源限制：与awp同网段的访问（通过请求头的X-Forwarded-For与X-Real-IP请求头判断，通过nginx反代访问的都会带上） 接口地址：http://localhost:8083/tk/wx/act/{eid} 请求参数：eid路径参数，用户ID JsTicket 请求方法：POST 来源限制：与awp同网段的访问（通过请求头的X-Forwarded-For与X-Real-IP请求头判断，通过nginx反代访问的都会带上） 接口地址：http://localhost:8083/tk/wx/jst/{eid} 请求参数：eid路径参数，用户ID 微信支付 微信支付相对比较麻烦，单次支付涉及到多个接口。\n具体的应用可以参考awp-verticle-admin/resources/static/page/paytest.html页面的例子。\nP.S. 支付的页面需要引入https://res.wx.qq.com/open/js/jweixin-1.2.0.js。\n预处理(wx.config用) 请求方法：GET，由页面AJAX调用 来源限制：暂无 接口地址：http://localhost:8083/pay/wx/pre/{eid}/{url} 请求参数：eid:路径参数，用户ID; url:当前页面URL，请将完整URL进行URL编码再发送 响应格式：JSON 响应内容：appId，timestamp（生成签名的时间戳），noncestr（生成签名的随机串），signture（签名），由wx.config()使用。 下单 请求方法：POST，由页面AJAX调用 来源限制：暂无 接口地址：http://localhost:8083/pay/wx/order 请求参数：JSON格式的字符串，包括以下参数：eid(企业用户ID),orderId(本地订单ID),openId(用户OpenID),price(价格，单位：分),name(产品名),callback(支付成功后回调接口，请填写完整URL，无需编码) 响应格式：JSON 响应内容：appId,timestamp（生成签名的时间戳），noncestr（生成签名的随机串）,packages(prepay_id),paysign(签名)。供WeixinJSBridge.invoke('getBrandWCPayRequest',{})使用 退款 (TODO)\n支付宝支付 下单 需要使用支付宝支付时，由由用户调用此接口（可以是web服务返回重定向到本接口，或后台计算出接口地址，让js跳转）。\n请求地址：http://localhost:8083/pay/zfb/order/{body} 参数：body，格式为变种Base64编码的JSON，请用http://localhost:8083/static/sys/page/base64.html 进行编码。 例如(如: http://localhost:8083/pay/zfb/order/bYkL1CX3PB7sIf6YZGwYSCX3P3IjKBKjKBKMdEH0POIsIWJY1CdLIBNjoOkuHCyLIBN32Iu55p2cI3g3HtqsvGkhHts3P3kNTmigP3Q-ZGLBTO53HCL9TS5BvtM3oOkzTCdBZedzIBN31miMcAN-otj-Htqs1G6zTAN4KAVzo0dMHeipHY6gHCTLo0d5cY63HedLdBXu1minvOk6\n{ \u0026#34;eid\u0026#34;:1,/*web项目使用的公众号在本项目中的用户ID*/ \u0026#34;orderId\u0026#34;:\u0026#34;1231234567\u0026#34;,/*本地订单ID，请保持唯一性*/ \u0026#34;price\u0026#34;:1,/*价格，单位：分*/ \u0026#34;name\u0026#34;:\u0026#34;苹果\u0026#34;,/*产品名称*/ \u0026#34;callback\u0026#34;:\u0026#34;http://dict.baidu.com\u0026#34;,/*支付成功后异步回调地址，将会带上支付宝回调的所有参数*/ \u0026#34;success\u0026#34;:\u0026#34;http://localhost:8083/static/page/sys/base64.html\u0026#34;/*支付后前段立即跳转的地址*/ } 退款 (TODO)\n(客服/模板)消息发送 微信客服消息 请求方法：PUT 来源限制：与awp同网段的访问（通过请求头的X-Forwarded-For与X-Real-IP请求头判断，通过nginx反代访问的都会带上） 接口地址：http://localhost:8083/msg/wx/kf 请求参数：JSON格式，无需编码，详见请求提示例 响应格式：JSON 响应内容：微信公众号模板消息接口返回的消息 请求体示例：\n{ \u0026#34;eid\u0026#34;: 2,/*web项目使用的公众号在本项目中的用户ID*/ \u0026#34;openId\u0026#34;: \u0026#34;of2333333333333333333333OBuk\u0026#34;,/*用户OpenID*/ \u0026#34;content\u0026#34;: \u0026#34;韩寒会画画后悔画韩红\u0026#34;/*客服消息内容*/ } 响应体示例：\n{ \u0026#34;errcode\u0026#34;: 0, \u0026#34;errmsg\u0026#34;: \u0026#34;ok\u0026#34; } 微信模板消息 请求方法：PUT 来源限制：与awp同网段的访问（通过请求头的X-Forwarded-For与X-Real-IP请求头判断，通过nginx反代访问的都会带上） 接口地址：http://localhost:8083/msg/wx/tp 请求参数：JSON格式，无需编码，详见请求体示例 响应格式：JSON 响应内容：微信公众号模板消息接口返回的消息 请求体示例：\n{ \u0026#34;eid\u0026#34;: 2,/*web项目使用的公众号在本项目中的用户ID*/ \u0026#34;openId\u0026#34;: \u0026#34;of2333333333333333333333OBuk\u0026#34;,/*用户OpenID*/ \u0026#34;tmpId\u0026#34;: \u0026#34;6p233333333333333333333333333333333333333oM\u0026#34;,/*模板ID*/ \u0026#34;url\u0026#34;: \u0026#34;https://www.baidu.com\u0026#34;,/*模板消息点击后跳转的地址*/ \u0026#34;data\u0026#34;: {/*按模板的字段填写具体的内容*/ \u0026#34;first\u0026#34;: \u0026#34;航班延误\u0026#34;, \u0026#34;keyword1\u0026#34;: \u0026#34;AA123\u0026#34;, \u0026#34;keyword2\u0026#34;: \u0026#34;延误\u0026#34;, \u0026#34;keyword3\u0026#34;: \u0026#34;北京-上海\u0026#34;, \u0026#34;keyword4\u0026#34;: \u0026#34;2017-9-21\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;韩寒会画画后悔画韩红\u0026#34; } } 响应体示例：\n{ \u0026#34;errcode\u0026#34;: 0, \u0026#34;errmsg\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;msgid\u0026#34;: 439631104 } 支付宝客服消息 (TODO)\n支付宝模板消息 请求方法：PUT 来源限制：与awp同网段的访问（通过请求头的X-Forwarded-For与X-Real-IP请求头判断，通过nginx反代访问的都会带上） 接口地址：http://localhost:8083/msg/zfb/tp 请求参数：JSON格式，无需编码，详见请求体示例 响应格式：JSON 响应内容：微信公众号模板消息接口返回的消息 与微信的类似，示例略。\n","date":"2018-01-30T14:02:10+08:00","image":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8EVert.X%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98%E5%AE%9D%E5%85%AC%E4%BC%97%E5%8F%B7%E9%80%9A%E7%94%A8%E6%9C%8D%E5%8A%A1/drink_hu0b3c81b279d1e938fecd45667abaa1fd_122178_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8EVert.X%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98%E5%AE%9D%E5%85%AC%E4%BC%97%E5%8F%B7%E9%80%9A%E7%94%A8%E6%9C%8D%E5%8A%A1/","title":"基于Vert.X的高性能微信支付宝公众号通用服务"},{"content":"问题 最近用Maxwell解析MySQL的Binlog，发送到Kafka进行处理，测试的时候发现一个问题，就是Kafka的Offset严重倾斜，三个partition，其中一个的offset已经快200万了，另外两个offset才不到两百。\nKafka数据倾斜的问题一般是由于生产者使用的Partition接口实现类对分区处理的问题，一般是对key做hash之后，对分区数取模。当出现数据倾斜时，小量任务耗时远高于其它任务，从而使得整体耗时过大，未能充分发挥分布式系统的并行计算优势（参考Apache Kafka 0.10 技术内幕：数据倾斜详解）。\n而使用Maxwell解析MySQL的Binlog发送到Kafka的时候，生产者是Maxwell，那么数据倾斜的问题明细就是Maxwell引起的了。\n原因 在Maxwell官网查文档（Producers:kafka-partitioning Maxwell\u0026rsquo;s Daemon）得知，在Maxwell没有配置的情况下，默认使用数据库名作为计算分区的key，并使用Java默认的hashcode算法进行计算：\nA binlog event\u0026#39;s partition is determined by the selected hash function and hash string as follows | HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS The HASH_FUNCTION is either java\u0026#39;s hashCode or murmurhash3. The default HASH_FUNCTION is hashCode. Murmurhash3 may be set with the kafka_partition_hash option. ………… The HASH_STRING may be (database, table, primary_key, column). The default HASH_STRING is the database. The partitioning field can be configured using the producer_partition_by option. 而在很多业务系统中，不同数据库的活跃度差异是很大的，主体业务的数据库操作频繁，产生的Binlog也就很多，而Maxwell默认使用数据库作为key进行hash，那么显而易见，Binglog的操作经常都被分到同一个分区里面了。\n解决 于是我们在Maxwell启动命令中加入对应参数即可，这里我选择了Rowkey作为分区key，同时选用murmurhash3 哈希算法，以获得更好的效率和分布：\nnohup /opt/maxwell-1.11.0/bin/maxwell --user=\u0026#39;maxwell\u0026#39; --password=\u0026#39;***\u0026#39; --host=\u0026#39;***\u0026#39; --exclude_dbs=\u0026#39;/^(mysql|maxwell|test)/\u0026#39; --producer=kafka --kafka.bootstrap.servers=*** --kafka_partition_hash=murmur3 --producer_partition_by=primary_key \u0026gt;\u0026gt; /root/maxwell.log \u0026amp; 用此命令重新启动Maxwell之后，观察Offset的变化，隔一段时间之后，各分区Offset的增量基本一致，问题解决！\n","date":"2018-01-03T17:34:30+08:00","image":"https://leibnizhu.github.io/p/%E8%A7%A3%E5%86%B3Maxwell%E5%8F%91%E9%80%81Kafka%E6%B6%88%E6%81%AF%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98/lotus_hu4d3bb5febb14b87d984ab69f868ff292_117866_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/%E8%A7%A3%E5%86%B3Maxwell%E5%8F%91%E9%80%81Kafka%E6%B6%88%E6%81%AF%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98/","title":"解决Maxwell发送Kafka消息数据倾斜问题"},{"content":"前言 网上流传一篇关于Spark Streaming消费Kafka时用Zookeeper保存Kafka队列offset的文章，如https://www.2cto.com/net/201710/692443.html，最初源头没找了，亲测在Spark1.6是可以用的。\n然而在Spark2中，这种方法的KafkaManager类中所依赖的KafkaCluster等等的类并不存在，因此此法无法直接套用到Spark中；此外，如果使用Cloudera的CDH集群的Spark2，其API更为缺少。因此，本文给出一种在CDH5.13的Spark2中通过Zookeeper管理Kafka消费Offset的方法。\n环境 集群：Cloudera CDH（Cloudera Manager 5.13.0） Spark：2.1.0 cloudera2 Scala：2.11.8 Java：1.8.0_u91 Maven依赖 \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;scala.version\u0026gt;2.11.8\u0026lt;/scala.version\u0026gt; \u0026lt;spark.version\u0026gt;2.1.0.cloudera2\u0026lt;/spark.version\u0026gt; \u0026lt;kafka.version\u0026gt;0.11.0-kafka-3.0.0\u0026lt;/kafka.version\u0026gt; \u0026lt;scala-test.version\u0026gt;3.0.0\u0026lt;/scala-test.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;cloudera\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://repository.cloudera.com/artifactory/cloudera-repos/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;aliyun\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.scala-lang\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-library\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${scala.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-core_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.spark\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spark-streaming-kafka-0-10_2.11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spark.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-clients\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${kafka.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-streams\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${kafka.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 管理Kafka消费Offset 使用方法 创建KafkaManager对象 使用类：\n/** * Kafka的连接和Offset管理工具类 * * @param zkHosts Zookeeper地址 * @param kafkaParams Kafka启动参数 * @author Leibniz */ class KafkaManager(zkHosts: String, kafkaParams: Map[String, Object]) extends Serializable 如：\nval zkHosts = \u0026#34;localhost:2181\u0026#34; val kafkaParams = Map[String, Object]( \u0026#34;auto.offset.reset\u0026#34; -\u0026gt; \u0026#34;latest\u0026#34;, \u0026#34;bootstrap.servers\u0026#34; -\u0026gt; kafkaBrokers, \u0026#34;group.id\u0026#34; -\u0026gt; MAXWELL_KAFKA_GROUP, \u0026#34;enable.auto.commit\u0026#34; -\u0026gt; (false: java.lang.Boolean), //禁用自动提交Offset，否则可能没正常消费完就提交了，造成数据错误 \u0026#34;key.deserializer\u0026#34; -\u0026gt; classOf[StringDeserializer], \u0026#34;value.deserializer\u0026#34; -\u0026gt; classOf[StringDeserializer]) val km = new KafkaManager(zkHosts, kafkaParams) 创建Kafka输入流 /** * 包装createDirectStream方法，支持Kafka Offset，用于创建Kafka Streaming流 * * @param ssc Spark Streaming Context * @param topics Kafka话题 * @tparam K Kafka消息Key类型 * @tparam V Kafka消息Value类型 * @return Kafka Streaming流 * @author Leibniz */ def createDirectStream[K: ClassTag, V: ClassTag](ssc: StreamingContext, topics: Seq[String]): InputDStream[ConsumerRecord[K, V] 如：\nval kafkaStream = km.createDirectStream[String, String](ssc, kafkaTopics.split(\u0026#34;,\u0026#34;).toSeq) 操作完毕后更新Offset /** * 保存Kafka消息队列消费的Offset * * @param rdd SparkStreaming的Kafka RDD，RDD[ConsumerRecord[K, V]] * @param storeEndOffset true=保存结束offset， false=保存起始offset * @author Leibniz */ def persistOffsets[K, V](rdd: RDD[ConsumerRecord[K, V]], storeEndOffset: Boolean = true): Unit 如：\nkm.persistOffsets[String, String](rdd) 详细代码 package com.turingdi.enmonster.nrt.common import java.lang.Object import com.turingdi.enmonster.nrt.common.Constants._ import kafka.utils.{ZKGroupTopicDirs, ZkUtils} import org.apache.kafka.clients.consumer.{ConsumerRecord, KafkaConsumer} import org.apache.kafka.common.TopicPartition import org.apache.spark.rdd.RDD import org.apache.spark.streaming.StreamingContext import org.apache.spark.streaming.dstream.InputDStream import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent import org.apache.spark.streaming.kafka010.{ConsumerStrategies, HasOffsetRanges, KafkaUtils} import org.slf4j.LoggerFactory import scala.collection.JavaConversions._ import scala.reflect.ClassTag import scala.util.Try /** * Kafka的连接和Offset管理工具类 * * @param zkHosts Zookeeper地址 * @param kafkaParams Kafka启动参数 * @author Leibniz */ class KafkaManager(zkHosts: String, kafkaParams: Map[String, Object]) extends Serializable { //Logback日志对象，使用slf4j框架 @transient private lazy val log = LoggerFactory.getLogger(getClass) //建立ZkUtils对象所需的参数 val (zkClient, zkConnection) = ZkUtils.createZkClientAndConnection(zkHosts, ZK_SESSION_TIMEOUT, ZK_CONNECTION_TIMEOUT) //ZkUtils对象，用于访问Zookeeper val zkUtils = new ZkUtils(zkClient, zkConnection, false) /** * 包装createDirectStream方法，支持Kafka Offset，用于创建Kafka Streaming流 * * @param ssc Spark Streaming Context * @param topics Kafka话题 * @tparam K Kafka消息Key类型 * @tparam V Kafka消息Value类型 * @return Kafka Streaming流 * @author Leibniz */ def createDirectStream[K: ClassTag, V: ClassTag](ssc: StreamingContext, topics: Seq[String]): InputDStream[ConsumerRecord[K, V]] = { val groupId = kafkaParams(\u0026#34;group.id\u0026#34;).toString val storedOffsets = readOffsets(topics, groupId) log.info(\u0026#34;Kafka消息偏移量汇总(格式:(话题,分区号,偏移量)):{}\u0026#34;, storedOffsets.map(off =\u0026gt; (off._1.topic, off._1.partition(), off._2))) val kafkaStream = KafkaUtils.createDirectStream[K, V](ssc, PreferConsistent, ConsumerStrategies.Subscribe[K, V](topics, kafkaParams, storedOffsets)) kafkaStream } /** * 从Zookeeper读取Kafka消息队列的Offset * * @param topics Kafka话题 * @param groupId Kafka Group ID * @return 返回一个Map[TopicPartition, Long]，记录每个话题每个Partition上的offset，如果还没消费，则offset为0 * @author Leibniz */ def readOffsets(topics: Seq[String], groupId: String): Map[TopicPartition, Long] = { val topicPartOffsetMap = collection.mutable.HashMap.empty[TopicPartition, Long] val partitionMap = zkUtils.getPartitionsForTopics(topics) // /consumers/\u0026lt;groupId\u0026gt;/offsets/\u0026lt;topic\u0026gt;/ partitionMap.foreach(topicPartitions =\u0026gt; { val zkGroupTopicDirs = new ZKGroupTopicDirs(groupId, topicPartitions._1) topicPartitions._2.foreach(partition =\u0026gt; { val offsetPath = zkGroupTopicDirs.consumerOffsetDir + \u0026#34;/\u0026#34; + partition val tryGetKafkaOffset = Try { val offsetStatTuple = zkUtils.readData(offsetPath) if (offsetStatTuple != null) { log.info(\u0026#34;查询Kafka消息偏移量详情: 话题:{}, 分区:{}, 偏移量:{}, ZK节点路径:{}\u0026#34;, Seq[AnyRef](topicPartitions._1, partition.toString, offsetStatTuple._1, offsetPath): _*) topicPartOffsetMap.put(new TopicPartition(topicPartitions._1, Integer.valueOf(partition)), offsetStatTuple._1.toLong) } } if(tryGetKafkaOffset.isFailure){ //http://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html val consumer = new KafkaConsumer[String, Object](kafkaParams) val partitionList = List(new TopicPartition(topicPartitions._1, partition)) consumer.assign(partitionList) val minAvailableOffset = consumer.beginningOffsets(partitionList).values.head consumer.close() log.warn(\u0026#34;查询Kafka消息偏移量详情: 没有上一次的ZK节点:{}, 话题:{}, 分区:{}, ZK节点路径:{}, 使用最小可用偏移量:{}\u0026#34;, Seq[AnyRef](tryGetKafkaOffset.failed.get.getMessage, topicPartitions._1, partition.toString, offsetPath, minAvailableOffset): _*) topicPartOffsetMap.put(new TopicPartition(topicPartitions._1, Integer.valueOf(partition)), minAvailableOffset) } }) }) topicPartOffsetMap.toMap } /** * 保存Kafka消息队列消费的Offset * * @param rdd SparkStreaming的Kafka RDD，RDD[ConsumerRecord[K, V]] * @param storeEndOffset true=保存结束offset， false=保存起始offset * @author Leibniz */ def persistOffsets[K, V](rdd: RDD[ConsumerRecord[K, V]], storeEndOffset: Boolean = true): Unit = { val groupId = kafkaParams(\u0026#34;group.id\u0026#34;).toString val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges offsetsList.foreach(or =\u0026gt; { val zkGroupTopicDirs = new ZKGroupTopicDirs(groupId, or.topic) val offsetPath = zkGroupTopicDirs.consumerOffsetDir + \u0026#34;/\u0026#34; + or.partition val offsetVal = if (storeEndOffset) or.untilOffset else or.fromOffset zkUtils.updatePersistentPath(zkGroupTopicDirs.consumerOffsetDir + \u0026#34;/\u0026#34; + or.partition, offsetVal + \u0026#34;\u0026#34; /*, JavaConversions.bufferAsJavaList(acls)*/) log.debug(\u0026#34;保存Kafka消息偏移量详情: 话题:{}, 分区:{}, 偏移量:{}, ZK节点路径:{}\u0026#34;, Seq[AnyRef](or.topic, or.partition.toString, offsetVal.toString, offsetPath): _*) }) } } ","date":"2017-12-22T08:46:44+08:00","image":"https://leibnizhu.github.io/p/Spark2%E7%9A%84Kafka%E6%B6%88%E8%B4%B9Offset%E7%AE%A1%E7%90%86/sunup_huf93ddb1d76af922954a0e2e222557da6_490394_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/Spark2%E7%9A%84Kafka%E6%B6%88%E8%B4%B9Offset%E7%AE%A1%E7%90%86/","title":"Spark2的Kafka消费Offset管理"},{"content":"源码版本：Tomcat 8.0.41\nRequest和Response的门面模式 从UML图可以看到，Tomcat中HttpServletRequest和HttpServletResponse的实现类是org.apache.catalina.connector.Request和org.apache.catalina.connector.Response，但实际提供给Servlet的时候用的是门面类RequestFacade和ResponseFacade。这是因为实现类里面的public方法比接口的多，而且可能涉及到安全问题，如果Servlet直接将其强转成实现类，是可以访问这些方法的，存在安全问题，因为使用了门面模式，将这些方法隐藏起来。\n统一日志消息处理 Tomcat8使用org.apache.catalina.tribes.util.StringManager对日志消息进行统一处理，每个包一般都有一个LocalStrings.properties文件，需要调用这些日志信息的类，会维护一个StringManager的实例，初始化时以当前包名为参数，以获取当前包对应的LocalStrings.properties文件：\nprivate StringManager(String packageName); private static final Hashtable\u0026lt;String, StringManager\u0026gt; managers = new Hashtable\u0026lt;\u0026gt;(); /** * Get the StringManager for a particular package. If a manager for * a package already exists, it will be reused, else a new * StringManager will be created and returned. * * @param packageName The package name */ public static final synchronized StringManager getManager(String packageName) { StringManager mgr = managers.get(packageName); if (mgr == null) { mgr = new StringManager(packageName); managers.put(packageName, mgr); } return mgr; } /* For Example */ protected static final StringManager sm = StringManager.getManager(Constants.Package); 可以看到它使用了Hashtable来维护每个包对应的StringManager单例。\n然后在需要读取消息的时候调用StringManager的getString(String key)方法，如：\nlog.info(sm.getString(\u0026#34;receiverBase.socket.bind\u0026#34;, addr)); 请求参数等的懒解析 为了提高效率，请求参数在第一次调用public String getParameter(String name)、public Enumeration\u0026lt;String\u0026gt; getParameterNames()等方法的时候才会解析，如果整个请求响应处理过程中都没有调用相关方法的话，请求参数将不会被解析，因为字符串处理的消耗不低。其他的一些属性也有类似的处理。主要的代码如下：\n/** * Request parameters parsed flag. */ protected boolean parametersParsed = false; /** * Return the value of the specified request parameter, if any; otherwise, * return \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt;. If there is more than one value defined, * return only the first one. * * @param name Name of the desired request parameter */ @Override public String getParameter(String name) { if (!parametersParsed) { parseParameters(); } return coyoteRequest.getParameters().getParameter(name); } /** * Returns a \u0026lt;code\u0026gt;Map\u0026lt;/code\u0026gt; of the parameters of this request. * Request parameters are extra information sent with the request. * For HTTP servlets, parameters are contained in the query string * or posted form data. * * @return A \u0026lt;code\u0026gt;Map\u0026lt;/code\u0026gt; containing parameter names as keys * and parameter values as map values. */ @Override public Map\u0026lt;String, String[]\u0026gt; getParameterMap() { if (parameterMap.isLocked()) { return parameterMap; } Enumeration\u0026lt;String\u0026gt; enumeration = getParameterNames(); while (enumeration.hasMoreElements()) { String name = enumeration.nextElement(); String[] values = getParameterValues(name); parameterMap.put(name, values); } parameterMap.setLocked(true); return parameterMap; } /** * Return the names of all defined request parameters for this request. */ @Override public Enumeration\u0026lt;String\u0026gt; getParameterNames() { if (!parametersParsed) { parseParameters(); } return coyoteRequest.getParameters().getParameterNames(); } /** * Parse request parameters. */ protected void parseParameters() { parametersParsed = true; /*………具体的解析处理，在此省略………*/ } 其中ParameterMap是一个继承了LinkedHashMap的类：\npublic final class ParameterMap\u0026lt;K,V\u0026gt; extends LinkedHashMap\u0026lt;K,V\u0026gt; Connector连接器 Tomcat8主要有四个Connector，分别为Http11Protocol、Http11NioProtocol、Http11Nio2Protocol、Http11AprProtocol，UML如上图所示，内容比较多，暂时不讨论了。\nTomcat容器层次 Tomcat中有四个层次的容器：\nEngine：整个Catalina Servlet引擎 Host：包含一个或多个Context容器的虚拟主机 Context：表示一个Web应用程序，包含一个或多个Wrapper Wrapper：表示一个独立的Servlet 以上四个类均实现了org.apache.catalina.Container接口，标准实现分别为org.apache.catalina.core包中的StandardEngine、StandardHost、StandardContext、StandardWrapper。 Pipeline管道 (尚未完工) org.apache.catalina.valves.AccessLogValve\n","date":"2017-11-30T11:53:14+08:00","image":"https://leibnizhu.github.io/p/Tomcat8%E6%BA%90%E7%A0%81%E8%AF%BB%E5%90%8E%E6%84%9F/sundown2_hue9945b5c7600d6848cb9cbbbf9a57528_100902_120x120_fill_q75_box_smart1.jpg","permalink":"https://leibnizhu.github.io/p/Tomcat8%E6%BA%90%E7%A0%81%E8%AF%BB%E5%90%8E%E6%84%9F/","title":"Tomcat8源码读后感"},{"content":"简单说几句，关于最近对Java服务框架的思考。\n最早我是用springMVC + Spring的，因为太臃肿，配置麻烦，很快切换到SpringBoot。\n用上SpringBoot后，觉得内置Tomcat/Jetty性能可能不够好，于是自己写了个基于Netty的内置Servlet容器，然而简单测试后发现性能与内置的Tomcat/Jetty相差不大（也有可能是因为测试用例太简单了，没有把Netty NIO在业务阻塞线程时的优势体现出来）。\n期间还考虑过直接用Netty原生API来写一个分发请求的简单框架，看了一些别人类似功能的项目，后来不了了之。\n结合这两点，盯上了Play Framework，这个在许久前就有关注过，但没深入了解，看了官方文档之后，发现这就是我想要的！开发起来很方便嘛，但是Session的实现有点………………建议用scala写，我个人是没问题，但不好带人一起写。\n后来在Telegram某群组里被安利了Vert.X，看了官方文档，还有详细的官方Demo，以及各种安利文章，发现这玩意真好用诶，跟Node.Js有点像诶，也不用跟用Netty原生API一样战战兢兢了，配套解决方案也不少，逼格也有，多语言支持（虽然对我而言用处不大），决定就是你了！\n所以最终结论就是：Vert.X大法好，退Spring保平安～\nP.S. 在Github写了一些简单的Vert.X学习例子，另外准备用Vert.X写一个微信/支付宝的微服务(2018-08-02更新:2017年年底已经写了,忘了更新这篇文章, 请参阅基于Vert.X的高性能微信支付宝公众号通用服务)。\n","date":"2017-10-11T14:19:12+08:00","image":"https://leibnizhu.github.io/p/%E6%9C%80%E8%BF%91%E5%AF%B9Java%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%9D%E8%80%83/mokou_kaguya_hu1b6d38c05bcdc128b7d4abee3ea0602c_80982_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E6%9C%80%E8%BF%91%E5%AF%B9Java%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%9D%E8%80%83/","title":"最近对Java服务框架的思考"},{"content":"基于Netty的Spring Boot内置Servlet容器的实现（五） BenchMark 程序编写 BenchMark可以用Jmeter进行，也可以直接编写java Test程序，通过@Befor进行时间计算。\n更方便的方法直使JM框架。\nJMH简介 JMH是新的microbenchmark（微基准测试）框架（2013年首次发布）。与其他众多框架相比它的特色优势在于，它是由Oracle实现JIT的相同人员开发的。特别是我想提一下Aleksey Shipilev和他优秀的博客文章。JMH可能与最新的Oracle JRE同步，其结果可信度很高。\nIDEA的JMH插件 直接使用JMH需要额外编写一些入口方法、增加依赖等，并不是特别方便，而IDEA有插件支持JMH。打开Files-Settings，找到Plugins选项卡，安装JMH Plugin插件，安装后重启如下：\n编写BenchMark方法 安装JMH Plugin插件之后无需编写入口方法、增加依赖，可以直接关注具体的测试。\n编写一个简单例子，测试我们容器的测试用例吞吐量：\n@BenchmarkMode(Mode.Throughput) public class NettyServletBenchmark { @Benchmark @Warmup(iterations = 10) @Measurement(iterations = 20) public void plaintext() { getUrl(\u0026#34;http://localhost:9999/netty/plaintext\u0026#34;, false); } @Benchmark @Warmup(iterations = 10) @Measurement(iterations = 20) public void json() { getUrl(\u0026#34;http://localhost:9999/netty/json?msg=1\u0026#34;, false); } private String getUrl(String url, boolean read) { BufferedReader br = null; InputStream is = null; StringBuilder sbuf = new StringBuilder(); try { URL reqURL = new URL(url); HttpURLConnection connection = (HttpURLConnection) reqURL.openConnection(); // 进行连接，但是实际上getrequest要在下一句的connection.getInputStream() 函数中才会真正发到服务器 connection.setDoOutput(false); connection.setUseCaches(false); connection.setRequestMethod(\u0026#34;GET\u0026#34;); connection.setConnectTimeout(200); connection.setDoInput(true); connection.connect(); if (read) { br = new BufferedReader(new InputStreamReader(connection.getInputStream())); String line; while ((line = br.readLine()) != null) { sbuf.append(line).append(\u0026#34;\\n\u0026#34;); } } else { is = connection.getInputStream(); } } catch (IOException e) { System.out.println(\u0026#34;连接服务器\u0026#39;\u0026#34; + url + \u0026#34;\u0026#39;时发生错误：\u0026#34; + e.getMessage()); } finally { try { if (null != br) { br.close(); } if (is != null) { is.close(); } } catch (IOException e) { e.printStackTrace(); } } return sbuf.toString(); } } 其中@BenchmarkMode(Mode.Throughput)表示测试吞吐量，即一秒内可以跑多少次测试方法。其他的测试模式如下：\n名称 描述 Mode.Throughput 计算一个时间单位内操作数量 Mode.AverageTime 计算平均运行时间 Mode.SampleTime 计算一个方法的运行时间(包括百分位) Mode.SingleShotTime 方法仅运行一次(用于冷测试模式)。或者特定批量大小的迭代多次运行；这种情况下JMH将计算批处理运行时间(一次批处理所有调用的总时间) 这些模式的任意组合 可以指定这些模式的任意组合——该测试运行多次(取决于请求模式的数量) Mode.All 所有模式依次运行 接下来是具体的测试方法：\n@Benchmark @Warmup(iterations = 10) @Measurement(iterations = 20) public void plaintext() { getUrl(\u0026#34;http://localhost:9999/netty/plaintext\u0026#34;, false); } 其中@Benchmark注解表示当前方法是需要JMH执行测试的方法，@Warmup(iterations = 10)表示每次正式测试前，先跑10次进行热身（不参与测试结果的计算），@Measurement(iterations = 20)表示每次正式测试执行20次方法。\n执行测试 执行方法很简单，点击菜单Run-Run...，弹出窗中选择当前类，即可：\n等测试完毕，就会打印出测试结果：\nResult \u0026#34;io.gitlab.leibnizhu.sbnetty.benchmark.NettyServletBenchmark.plaintext\u0026#34;: 6508.938 ±(99.9%) 189.498 ops/s [Average] (min, avg, max) = (157.637, 6508.938, 7098.929), stdev = 802.346 CI (99.9%): [6319.440, 6698.436] (assumes normal distribution) Benchmark Mode Cnt Score Error Units NettyServletBenchmark.json thrpt 200 6756.677 ± 182.976 ops/s NettyServletBenchmark.plaintext thrpt 200 6508.938 ± 189.498 ops/s 可以看到IDEA对该类两个带有@Benchmark注解的方法分别进行了测试，测试结果分别是6.76kQps和6.51kQps（平均值）。\n","date":"2017-09-13T21:51:44+08:00","image":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BA%94/lzwx_hu1afe6b3663fa6cb2c80c0a666eabfb57_157940_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BA%94/","title":"基于Netty的Spring Boot内置Servlet容器的实现（五）"},{"content":"发布项目到Maven中央仓库 前言 写完项目之后，为了方便别人调用，需要发布到网上，有以下方法：\n提交到Github/Gitlab上的公开项目（包含pom.xml），其他人clone后，通过mvn install安装到本地仓库，再通过maven依赖引入。缺点：需要使用者手动操作的比较多，体验不好。 提交到Maven中央仓库，使用者只要直接通过maven依赖引入即可。缺点：发布者操作较多麻烦。 本文讨论的是第二种方法，目前网上也有很多相关文章，经过实践发现一些具体的操作已经过时/不可用，本人也踩了不少坑。所以写下本文记录。\n注册Sonatype提交申请 Maven中央仓库由Sonatype公司在维护，在向Maven中央仓库提交项目前，需要先注册Sonatype账户，然后提交issue申请。\n注册Sonatype 到 Sonatype官网 注册帐号，随后登录。\n提交issue申请 登录点击上方导航栏的Create按钮： 弹出模态窗：\n其中Project选择Community Support - Open Source Project Repository Hosting，Already Synced to Central按默认选No其他信息按实际填写，页面上也有例子。值得注意的是Group Id要与项目pom.xml里面的以及实际包名一致，最好是你拥有的域名倒着写；Project URL和SCM url可以写Gitlab/Github地址。\n创建后可以通过导航栏的issue菜单找到我们提交的issue：\n等待工作人员确认 由于时差关系，Sonatype的工作人员大约会在北京时间22：00开始处理issue。\n分配工作人员后，在issue页面右边看到Assignee分配给谁了：\n如果这个groupId是第一次用来提交，会有工作人员留言，向你确认groupId：\n我们回复、工作人员确认没问题之后，会告诉你审批通过，准备好配置了：\n这时就可以准备上传了。\n上传前的准备 GPG签名 一般Linux发行版都会预装gpg，没有的自行安装即可（apt、pacman等命令，不再赘述）。输入命令：\ngpg --gen-key 需要输入姓名、邮箱等字段，其它字段可使用默认值，此外，还需要输入一个 Passphase，相当于一个密钥库的密码，要记好，deploy要用。\n再输入下面命令来获取公钥ID：\ngpg --list-signatures 上图是输出结果，其中红框里的就是公钥ID，记下来。\n最后输入以下命令将公钥发布到 PGP 密钥服务器（以下三句命令发布到三个不同的服务器，任意输入一句即可，Sonatype对这三个服务器均认可）：\ngpg --keyserver hkp://pool.sks-keyservers.net：11371 --send-keys 刚才的公钥ID gpg --keyserver hkp://pgp.mit.edu:11371 --send-keys 刚才的公钥ID gpg --keyserver hkp://keyserver.ubuntu.com:11371 --send-keys 刚才的公钥ID 然后验证是否发布成功（也是任选一句，对应上面输入的服务器）：\ngpg --keyserver hkp://pool.sks-keyservers.net：11371 --recv-keys 刚才的公钥ID gpg --keyserver hkp://pgp.mit.edu:11371 --recv-keys 刚才的公钥ID gpg --keyserver hkp://keyserver.ubuntu.com:11371 --recv-keys 刚才的公钥ID 正常结果应该类似：\ngpg: 密钥 F***9：“L***z \u0026lt;l***u@***.***\u0026gt;”未改变 gpg: 合计被处理的数量：1 gpg: 未改变：1 修改pom.xml pom.xml文件需要修改比较多，在这直接列出相关的：\n\u0026lt;licenses\u0026gt; \u0026lt;license\u0026gt; \u0026lt;name\u0026gt;The Apache Software License, Version 2.0\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://www.apache.org/licenses/LICENSE-2.0.txt\u0026lt;/url\u0026gt; \u0026lt;distribution\u0026gt;repo\u0026lt;/distribution\u0026gt; \u0026lt;/license\u0026gt; \u0026lt;/licenses\u0026gt; \u0026lt;scm\u0026gt; \u0026lt;!-- 修改自己的地址 --\u0026gt; \u0026lt;tag\u0026gt;master\u0026lt;/tag\u0026gt; \u0026lt;url\u0026gt;https://github.com/Leibnizhu/spring-boot-starter-netty\u0026lt;/url\u0026gt; \u0026lt;connection\u0026gt;scm:git:git@github.com:Leibnizhu/spring-boot-starter-netty.git\u0026lt;/connection\u0026gt; \u0026lt;developerConnection\u0026gt;scm:git:git@github.com:Leibnizhu/spring-boot-starter-netty.git\u0026lt;/developerConnection\u0026gt; \u0026lt;/scm\u0026gt; \u0026lt;developers\u0026gt; \u0026lt;developer\u0026gt; \u0026lt;id\u0026gt;***\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;***\u0026lt;/name\u0026gt; \u0026lt;email\u0026gt;L***@***.***\u0026lt;/email\u0026gt; \u0026lt;/developer\u0026gt; \u0026lt;/developers\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;javadoc.version\u0026gt;8\u0026lt;/javadoc.version\u0026gt; \u0026lt;compiler-plugin.version\u0026gt;3.6.2\u0026lt;/compiler-plugin.version\u0026gt; \u0026lt;war-plugin.version\u0026gt;3.1.0\u0026lt;/war-plugin.version\u0026gt; \u0026lt;clean-plugin.version\u0026gt;3.0.0\u0026lt;/clean-plugin.version\u0026gt; \u0026lt;resources-plugin.version\u0026gt;3.0.2\u0026lt;/resources-plugin.version\u0026gt; \u0026lt;surefire-plugin.version\u0026gt;2.20\u0026lt;/surefire-plugin.version\u0026gt; \u0026lt;jar-plugin.version\u0026gt;3.0.2\u0026lt;/jar-plugin.version\u0026gt; \u0026lt;source-plugin.version\u0026gt;3.0.1\u0026lt;/source-plugin.version\u0026gt; \u0026lt;javadoc-plugin.version\u0026gt;2.10.4\u0026lt;/javadoc-plugin.version\u0026gt; \u0026lt;gpg-plugin.version\u0026gt;1.6\u0026lt;/gpg-plugin.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;distributionManagement\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt; https://oss.sonatype.org/service/local/staging/deploy/maven2 \u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;snapshotRepository\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;uniqueVersion\u0026gt;false\u0026lt;/uniqueVersion\u0026gt; \u0026lt;url\u0026gt; https://oss.sonatype.org/content/repositories/snapshots \u0026lt;/url\u0026gt; \u0026lt;/snapshotRepository\u0026gt; \u0026lt;/distributionManagement\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;release\u0026lt;/id\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-source-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${source-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;attach-sources\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;jar-no-fork\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;attach\u0026gt;true\u0026lt;/attach\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-javadoc-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${javadoc-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;links\u0026gt; \u0026lt;link\u0026gt; http://docs.oracle.com/javase/${javadoc.version}/docs/api \u0026lt;/link\u0026gt; \u0026lt;/links\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-gpg-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${gpg-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;sign\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;distributionManagement\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt; https://oss.sonatype.org/service/local/staging/deploy/maven2/ \u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/distributionManagement\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;${java.version}\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;${java.version}\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${jar-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${surefire-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;skip\u0026gt;true\u0026lt;/skip\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 这里加了很多插件，因为Sonatype要求有javadoc、GPG签名等等。\n修改~/.m2/settings.xml \u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;oss\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;用户名\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;密码\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; ... \u0026lt;/settings\u0026gt; 上传\u0026amp;\u0026amp;发布 Maven上传 进入项目，输入以下命令：\nmvn deploy -P release -Dgpg.passphrase=[GPG密码] 稍等片刻，期间可能会弹窗要求输入GPG密码。最后看到BUILD SUCCESS成功。\n注: MacOS下可能会提示：\ngpg: signing failed: Inappropriate ioctl for device 请先输入：\nexport GPG_TTY=$(tty) 再执行mvn deploy\nOSS发布 打开 https://oss.sonatype.org/ 并登陆（帐号密码与前面注册一样）。\n点击左边的Staging Repositories，再在右上角搜索groupid：\n选中自己的项目，点击Close再在弹出模态窗点Confirm，描述可以不用写：\n等一两分钟再刷新上面的列表，可以看到状态已经变成Closed（如果没有的话，在下方的Acitivity可以看到close操作的情况，看具体是什么问题，我一开始就一直说找不到签名，将GPG公钥发布后又说缺少javadoc，于是再加Maven插件生成javadoc）。 这时候再选中自己的项目，然后点Release按钮，再点Confirm提交，即可。\n收尾工作 关闭OSS的issue 回到OSS https://issues.sonatype.org ，打开自己提交的issue，回复一下，告诉工作人员已经上传发布到OSS了，很快他们就会回复说10分钟内会同步到中央仓库，2小时内可以被搜索到：\n等待同步中央仓库 一般等最多两个小时，就可以在 https://search.maven.org 搜索到自己发布的项目啦。\n我的Spring-Boot-Starter-Netty项目地址： https://search.maven.org/#artifactdetails%7Cio.gitlab.leibnizhu%7Cspring-boot-starter-netty%7C1.0%7Cjar\n","date":"2017-09-05T13:36:04+08:00","image":"https://leibnizhu.github.io/p/%E5%8F%91%E5%B8%83%E9%A1%B9%E7%9B%AE%E5%88%B0Maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93/melen_hufbb6c8b3720cc5ca1a074a3085d8da3e_99570_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E5%8F%91%E5%B8%83%E9%A1%B9%E7%9B%AE%E5%88%B0Maven%E4%B8%AD%E5%A4%AE%E4%BB%93%E5%BA%93/","title":"发布项目到Maven中央仓库"},{"content":"基于Netty的Spring Boot内置Servlet容器的实现（四） Registration注册器的实现 设计与继承结构 在本系列第一篇提到了javax.servlet.Registration接口，用于实现Filter和Servlet的动态注册，这个接口相对比较简单；有两个子接口，详见下面的UML图：\n我们将要分别实现FilterRegistration和ServletRegistration接口，为了保持与Registration接口的继承关系相近，我们设计了三个类，分别是抽象类，Filter注册、Servlet注册：\nclass AbstractNettyRegistration implements Registration, Registration.Dynamic, ServletConfig, FilterConfig{} public class NettyFilterRegistration extends AbstractNettyRegistration implements FilterRegistration.Dynamic{} public class NettyServletRegistration extends AbstractNettyRegistration implements ServletRegistration.Dynamic{} 这些类构成的UML图如下：\n代码实现 在实际实现的过程中，忽略了很多用不到的方法，着重实现了加入Mapping以及获取Filter/Servlet实例的方法。\n加入Mapping的方法由ServletContext处理（Mapping本身也是由ServletContext维护），而获取Filter/Servlet的方法使用的是类似于懒加载单例的方法，每个Registration实例维护自己的一个Filter/Servlet实例，首次获取的时候通过反射获取到实例，并将反射获取到的实例由Registration实例持有。具体实现代码如下：\npublic class NettyServletRegistration extends AbstractNettyRegistration implements ServletRegistration.Dynamic { private final Logger log = LoggerFactory.getLogger(getClass()); private volatile boolean initialised; private Servlet servlet; private Collection\u0026lt;String\u0026gt; urlPatternMappings = new LinkedList\u0026lt;\u0026gt;(); public NettyServletRegistration(NettyContext context, String servletName, String className, Servlet servlet) { super(servletName, className, context); this.servlet = servlet; } public Servlet getServlet() throws ServletException { if (!initialised) { synchronized (this) { if (!initialised) { if (null == servlet) { try { servlet = (Servlet) Class.forName(getClassName()).newInstance(); //反射获取实例 } catch (Exception e) { throw new ServletException(e); } } servlet.init(this); //初始化Servlet initialised = true; } } } return servlet; } @Override public Set\u0026lt;String\u0026gt; addMapping(String... urlPatterns) { //在RequestUrlPatternMapper中会检查url Pattern是否冲突 NettyContext context = getNettyContext(); for (String urlPattern : urlPatterns) { try { context.addServletMapping(urlPattern, getName(), getServlet()); } catch (ServletException e) { log.error(\u0026#34;Throwing exception when getting Servlet in NettyServletRegistration.\u0026#34;, e); } } urlPatternMappings.addAll(Arrays.asList(urlPatterns)); return new HashSet\u0026lt;\u0026gt;(urlPatternMappings); } } public class NettyFilterRegistration extends AbstractNettyRegistration implements FilterRegistration.Dynamic { private volatile boolean initialised; private Filter filter; private Collection\u0026lt;String\u0026gt; urlPatternMappings = new LinkedList\u0026lt;\u0026gt;(); public NettyFilterRegistration(NettyContext context, String filterName, String className, Filter filter) { super(filterName, className, context); this.filter = filter; } public Filter getFilter() throws ServletException { if (!initialised) { synchronized (this) { if (!initialised) { if (null == filter) { try { filter = (Filter) Class.forName(getClassName()).newInstance(); //反射获取实例 } catch (Exception e) { throw new ServletException(e); } } filter.init(this); //初始化Filter initialised = true; } } } return filter; } @Override public void addMappingForUrlPatterns(EnumSet\u0026lt;DispatcherType\u0026gt; dispatcherTypes, boolean isMatchAfter, String... urlPatterns) { NettyContext context = getNettyContext(); for (String urlPattern : urlPatterns) { context.addFilterMapping(dispatcherTypes, isMatchAfter, urlPattern); } urlPatternMappings.addAll(Arrays.asList(urlPatterns)); } } FilterChain过滤器链的实现 大家应该都知道过滤器链的概念，所有过滤器都在过滤器链上，当有请求进入，将依次经过每个适用的过滤器（根据过滤器的Url Pattern与请求的路径而不同），过滤器里执行doFilter()方法让过滤器链执行下一个过滤器，直到最后一个，则执行Servlet的service()方法。而过滤器链对应的接口javax.servlet.FilterChain里面就一个方法：\npublic interface FilterChain { public void doFilter ( ServletRequest request, ServletResponse response ) throws IOException, ServletException; } 显然，实现FilterChain接口，应该要维护一个过滤器的数组或者List，而在doFilter()方法里面，应该判断有没有下一个过滤器，有则调用其doFilter()方法，无则调用当前请求对应Servlet实例的service()方法，可以用迭代器或者记录游标（数组或List的下标）来实现。具体代码：\npublic class NettyFilterChain implements FilterChain { /** * 考虑到每个请求只有一个线程处理，而且ServletContext在每次请求时都会new 一个SimpleFilterChain对象 * 所以这里把过滤器链的Iterator作为FilterChain的私有变量，没有线程安全问题 */ private final Iterator\u0026lt;Filter\u0026gt; filterIterator; private final Servlet servlet; public NettyFilterChain(Servlet servlet, Iterable\u0026lt;Filter\u0026gt; filters) throws ServletException { this.filterIterator = checkNotNull(filters).iterator(); this.servlet = checkNotNull(servlet); } /** * 每个Filter在处理完请求之后调用FilterChain的这个方法。 * 这时候应该找到下一个Filter，调用其doFilter()方法。 * 如果没有下一个了，应该调用servlet的service()方法了 */ @Override public void doFilter(ServletRequest request, ServletResponse response) throws IOException, ServletException { if (filterIterator.hasNext()) { Filter filter = filterIterator.next(); filter.doFilter(request, response, this); } else { servlet.service(request, response); } } } HttpServletRequest的实现 接口javax.servlet.http.HttpServletRequest的方法比较多，大概可以分为Cookie相关、Header相关、各种路径相关、Session相关、请求参数相关、请求协议/地址/端口相关、Attributes相关、异步相关、multipart/form-data相关（上传文件）等等方法，以上提到的方法本文基本实现了，还有一些没实现的是暂时用不到的。\nCookie相关方法 Cookie使用“懒解析”，就是用标识isCookieParsed记录Cookie是否被解析过，初始化Request对象的时候不解析，在获取Cookiea相关方法被调用的时候再判断是否未解析，若未解析则解析再返回，否则直接返回。NettyHttpServletRequest的构造方法传入了netty的HttpHeaders实例，可以从中获取Cookie请求头，再进行解析。具体代码如下：\n/*====== Cookie 相关方法 开始 ======*/ private Cookie[] cookies; private transient boolean isCookieParsed = false; @Override public Cookie[] getCookies() { if (!isCookieParsed) { parseCookie(); } return cookies; } /** * 解析request中的Cookie到本类的cookies数组中 * * @author Leibniz */ private void parseCookie() { if (isCookieParsed) { return; } String cookieOriginStr = this.headers.get(\u0026#34;Cookie\u0026#34;); if (cookieOriginStr == null) { return; } Set\u0026lt;io.netty.handler.codec.http.cookie.Cookie\u0026gt; nettyCookies = ServerCookieDecoder.LAX.decode(cookieOriginStr); if (nettyCookies.size() == 0) { return; } this.cookies = new Cookie[nettyCookies.size()]; Iterator\u0026lt;io.netty.handler.codec.http.cookie.Cookie\u0026gt; itr = nettyCookies.iterator(); int i = 0; while (itr.hasNext()) { io.netty.handler.codec.http.cookie.Cookie nettyCookie = itr.next(); Cookie servletCookie = new Cookie(nettyCookie.name(), nettyCookie.value()); // servletCookie.setMaxAge(Ints.checkedCast(nettyCookie.maxAge())); if(nettyCookie.domain() != null) servletCookie.setDomain(nettyCookie.domain()); if(nettyCookie.path() != null) servletCookie.setPath(nettyCookie.path()); servletCookie.setHttpOnly(nettyCookie.isHttpOnly()); this.cookies[i++] = servletCookie; } this.isCookieParsed = true; } /*====== Cookie 相关方法 结束 ======*/ Header相关方法 上面提到NettyHttpServletRequest的构造方法传入了netty的HttpHeaders实例，可以从中获取所有请求头，而Header相关方法的实现就靠他了：\n/*====== Header 相关方法 开始 ======*/ private HttpHeaders headers; @Override public long getDateHeader(String name) { return this.headers.getTimeMillis(name); } @Override public String getHeader(String name) { return this.headers.get(name); } @Override public Enumeration\u0026lt;String\u0026gt; getHeaders(String name) { return Collections.enumeration(this.headers.getAll(name)); } @Override public Enumeration\u0026lt;String\u0026gt; getHeaderNames() { return Collections.enumeration(this.headers.names()); } @Override public int getIntHeader(String name) { String headerStringValue = this.headers.get(name); if (headerStringValue == null) { return -1; } return Integer.parseInt(headerStringValue); } /*====== Header 相关方法 结束 ======*/ Session相关方法 Session相关方法相对多一些。Session的解析分两种，首先尝试从Cookie中获取Cookie（名为JSESSIONID），如果没有，则从请求路径中找类似\u0026quot;;jsessionid=*******\u0026ldquo;的参数作为SessionID。拿到SessionID后，再调用SessionManager的方法获取Session对象；而根据从哪里解析到的SessionID可以设置isCookieSession和isURLSession两个属性，用于isRequestedSessionIdFromCookie()和isRequestedSessionIdFromURL()方法。如果拿不到SessionID，则调用SessionManager的方法创建一个新Session。\n至于Session和SessionManager的实现我们在下一小节再讲，值得注意的是，getSession()方法返回的并不是我们定义的Session类实例，而是其门面类，是出于安全的考虑；这一点参考了Tomcat的做法（Tomcat的Request、Response、Session等对象都是用门面模式）。\nprivate NettyHttpSession session; private boolean isCookieSession; private boolean isURLSession; /** * 先后看请求路径和Cookie中是否有sessionid * 有，则从SessionManager获取session对象放入session属性 * 如果session对象过期，则创建一个新的并放入 * 无，则创建一个新Session并放入 */ private void parseSession() { String sessionId; NettyHttpSession curSession; //从Cookie解析SessionID sessionId = getSessionIdFromCookie(); if(sessionId != null){ curSession = servletContext.getSessionManager().getSession(sessionId); if (null != curSession) { this.isCookieSession = true; recoverySession(curSession); return; } } if (!this.isCookieSession) { // 从请求路径解析SessionID sessionId = getSessionIdFromUrl(); curSession = servletContext.getSessionManager().getSession(sessionId); if(null != curSession){ this.isURLSession = true; recoverySession(curSession); return; } } //Cookie和请求参数中都没拿到Session，则创建一个 if (this.session == null) { this.session = createtSession(); } } /** * @return 从URL解析到的SessionID */ private String getSessionIdFromUrl() { StringBuilder u = new StringBuilder(request.uri()); int sessionStart = u.toString().indexOf(\u0026#34;;\u0026#34; + NettyHttpSession.SESSION_REQUEST_PARAMETER_NAME + \u0026#34;=\u0026#34;); if(sessionStart == -1) { return null; } int sessionEnd = u.toString().indexOf(\u0026#39;;\u0026#39;, sessionStart + 1); if (sessionEnd == -1) sessionEnd = u.toString().indexOf(\u0026#39;?\u0026#39;, sessionStart + 1); if (sessionEnd == -1) // still sessionEnd = u.length(); return u.substring(sessionStart + NettyHttpSession.SESSION_REQUEST_PARAMETER_NAME.length() + 2, sessionEnd); } /** * @return 从Cookie解析到的SessionID */ private String getSessionIdFromCookie() { Cookie[] cookies = getCookies(); if(cookies == null){ return null; } for (Cookie cookie : cookies) { if (cookie.getName().equals(NettyHttpSession.SESSION_COOKIE_NAME)) { return cookie.getValue(); } } return null; } /** * 恢复旧Session * @param curSession 要恢复的Session对象 */ private void recoverySession(NettyHttpSession curSession) { this.session = curSession; this.session.setNew(false); this.servletContext.getSessionManager().updateAccessTime(this.session); } @Override public HttpSession getSession(boolean create) { boolean valid = isRequestedSessionIdValid(); //在管理器存在，且没到期 //可用则直接返回 if (valid) { return session.getSession(); } //不可用则判断是否新建 if (!create) { session = null; //如果过期了设为null return null; } //不可用且允许新建则新建之 this.session = createtSession(); return this.session.getSession(); } @Override public HttpSession getSession() { return getSession(true); } @Override public String changeSessionId() { this.session = createtSession(); return this.session.getId(); } private NettyHttpSession createtSession() { return servletContext.getSessionManager().createSession(); } @Override public boolean isRequestedSessionIdValid() { return servletContext.getSessionManager().checkValid(session); } @Override public boolean isRequestedSessionIdFromCookie() { return isCookieSession; } @Override public boolean isRequestedSessionIdFromURL() { return isURLSession; } @Override @Deprecated public boolean isRequestedSessionIdFromUrl() { return isRequestedSessionIdFromURL(); } @Override public String getRequestedSessionId() { return session.getId(); } /*====== Session 相关方法 结束 ======*/ HttpServletResponse实现 HttpServletResponse接口相对简单一点，方法少一点，下面列举出部分方法的实现。\nHeader相关方法 这里的Header指响应头。NettyHttpServletResponse的构造方法里传入了netty的HttpResponse对象，默认的调用是传入一个200的正常HTTP响应。我们可以通过这个HttpResponse对象的headers()方法对响应头进行操作。具体代码如下：\n@Override public void setDateHeader(String name, long date) { response.headers().set(name, date); } @Override public void addDateHeader(String name, long date) { response.headers().add(name, date); } @Override public void setHeader(String name, String value) { if (name == null || name.length() == 0 || value == null) { return; } if (isCommitted()) { return; } if (setHeaderField(name, value)) { return; } response.headers().set(name, value); } private boolean setHeaderField(String name, String value) { char c = name.charAt(0);//减少判断的时间，提高效率 if (\u0026#39;C\u0026#39; == c || \u0026#39;c\u0026#39; == c) { if (HttpHeaderNames.CONTENT_TYPE.contentEqualsIgnoreCase(name)) { setContentType(value); return true; } } return false; } @Override public void addHeader(String name, String value) { if (name == null || name.length() == 0 || value == null) { return; } if (isCommitted()) { return; } if (setHeaderField(name, value)) { return; } response.headers().add(name, value); } @Override public void setIntHeader(String name, int value) { if (name == null || name.length() == 0) { return; } if (isCommitted()) { return; } response.headers().set(name, value); } @Override public void addIntHeader(String name, int value) { if (name == null || name.length() == 0) { return; } if (isCommitted()) { return; } response.headers().add(name, value); } getNettyResponse()方法 方法public HttpResponse getNettyResponse()是我们自己定义的，用于响应输出流在写入时做的一些基本处理，主要是请求头的处理，具体代码如下：\n/** * 设置基本的请求头 */ public HttpResponse getNettyResponse() { if (committed) { return response; } committed = true; HttpHeaders headers = response.headers(); if (null != contentType) { String value = null == characterEncoding ? contentType : contentType + \u0026#34;; charset=\u0026#34; + characterEncoding; //Content Type 响应头的内容 headers.set(HttpHeaderNames.CONTENT_TYPE, value); } CharSequence date = getFormattedDate(); headers.set(HttpHeaderNames.DATE, date); // 时间日期响应头 headers.set(HttpHeaderNames.SERVER, servletContext.getServerInfo()); //服务器信息响应头 // cookies处理 // long curTime = System.currentTimeMillis(); //用于根据maxAge计算Cookie的Expires //先处理Session ，如果是新Session需要通过Cookie写入 if (request.getSession().isNew()) { String sessionCookieStr = NettyHttpSession.SESSION_COOKIE_NAME + \u0026#34;=\u0026#34; + request.getRequestedSessionId() + \u0026#34;; path=/; domain=\u0026#34; + request.getServerName(); headers.add(HttpHeaderNames.SET_COOKIE, sessionCookieStr); } //其他业务或框架设置的cookie，逐条写入到响应头去 for (Cookie cookie : cookies) { StringBuilder sb = new StringBuilder(); sb.append(cookie.getName()).append(\u0026#34;=\u0026#34;).append(cookie.getValue()) .append(\u0026#34;; max-Age=\u0026#34;).append(cookie.getMaxAge()); if (cookie.getPath() != null) sb.append(\u0026#34;; path=\u0026#34;).append(cookie.getPath()); if (cookie.getDomain() != null) sb.append(\u0026#34;; domain=\u0026#34;).append(cookie.getDomain()); headers.add(HttpHeaderNames.SET_COOKIE, sb.toString()); } return response; } Session实现 Session相关的包括Session实现类NettyHttpSession，Sessionn门面包装类NettyHttpSessionFacade，以及Session管理器NettySessionManager。\n门面类前面提及到了，构造的时候传入一个NettyHttpSession实例并持有，所有HttpSession接口的方法都调用NettyHttpSession实例的对应方法去处理。\nSession管理器NettySessionManager是单例，由NettyContext实例持有，负责存储所有Session的映射，方便其他类根据SessionID去获取Session对象，提供创建新Session的方法，允许更新Session访问时间，同时定时清理过期的Session。\n每个NettyHttpSession实例都持有NettySessionManager的引用，实现了HttpSession接口。\nNettyHttpSession 实现比较简单，Attribute由对象持有的HashMap进行存储，自身保存ID、创建时间、访问时间、生命周期等信息。 具体代码如下（部分过时的方法、简单的getter不列出）：\npublic class NettyHttpSession implements HttpSession, Serializable { public static final String SESSION_COOKIE_NAME = \u0026#34;JSESSIONID\u0026#34;; public static final String SESSION_REQUEST_PARAMETER_NAME = \u0026#34;jsessionid\u0026#34;; private NettySessionManager manager; private long creationTime; private long lastAccessedTime; private int interval = NettySessionManager.SESSION_LIFE_SECONDS; private String id; NettyHttpSession(String id, NettySessionManager manager){ long curTime = System.currentTimeMillis(); this.creationTime = curTime; this.lastAccessedTime = curTime; this.id = id; this.manager = manager; this.sessionFacade = new NettyHttpSessionFacade(this); } private HttpSession sessionFacade; public HttpSession getSession(){ return sessionFacade; } void updateAccessTime() { lastAccessedTime = System.currentTimeMillis(); } @Override public void setMaxInactiveInterval(int interval) { this.interval = interval; } private Map\u0026lt;String, Object\u0026gt; attributes = new ConcurrentHashMap\u0026lt;\u0026gt;(); @Override public Object getAttribute(String name) { return attributes.get(name); } @Override public void setAttribute(String name, Object value) { attributes.put(name, value); } @Override public void removeAttribute(String name) { attributes.remove(name); } @Override public void invalidate() { attributes.clear(); attributes = null; manager.invalidate(this); manager = null; } private boolean isNew = true; @Override public boolean isNew() { return isNew; } public void setNew(boolean isNew){ this.isNew = isNew; } /** * 是否过期 * @return */ public boolean expire(){ return System.currentTimeMillis() - creationTime \u0026gt;= interval * 1000; } } NettySessionManager Session管理器没有现成的接口，因为比较简单所以也没抽出接口，自己在实现的过程中根据需求写了一些public方法：\n使用ConcurrentHashMap存储所有Session。 在构造的同时开启一个线程，每隔SESSION_LIFE_CHECK_INTER毫秒扫描所有Session判断是否过期需要清除（有待优化，比如等待时间按最快过期的session的过期时间，或者记录预计下次需要处理的个数，减少遍历的数量）。 SessionID是6位随机数字+时间戳翻转。 public class NettySessionManager { private Logger log = LoggerFactory.getLogger(getClass()); private NettyContext servletContext; private Map\u0026lt;String, NettyHttpSession\u0026gt; sessions = new ConcurrentHashMap\u0026lt;\u0026gt;(); static final int SESSION_LIFE_SECONDS = 60 * 30; static final int SESSION_LIFE_MILLISECONDS = SESSION_LIFE_SECONDS * 1000; private static final int SESSION_LIFE_CHECK_INTER = 1000 * 60; public NettySessionManager(NettyContext servletContext){ this.servletContext = servletContext; new Thread(new checkInvalidSessions(), \u0026#34;Session-Check\u0026#34;).start(); } void invalidate(HttpSession session) { sessions.remove(session.getId()); } public void updateAccessTime(NettyHttpSession session){ if(session != null){ session.updateAccessTime(); } } public boolean checkValid(NettyHttpSession session) { return session != null \u0026amp;\u0026amp; sessions.get(session.getId()) != null \u0026amp;\u0026amp; !session.expire(); } public NettyHttpSession getSession(String id){ return id == null ? null : sessions.get(id); } public NettyHttpSession createSession(){ String id = createUniqueSessionId(); NettyHttpSession newSession = new NettyHttpSession(id, this); sessions.put(id ,newSession); return newSession; } private String createUniqueSessionId() { String prefix = String.valueOf(100000 + new Random().nextInt(899999)); return new StringBuilder().append(System.currentTimeMillis()).reverse().append(prefix).toString(); } public void setOldSession(NettyHttpSession session) { if(session != null){ session.setNew(false); } } /** * 超时的Session无效化，定期执行 */ private class checkInvalidSessions implements Runnable { @Override public void run() { log.info(\u0026#34;Session Manager expire-checking thread has been started...\u0026#34;); while(true){ try { Thread.sleep(SESSION_LIFE_CHECK_INTER); } catch (InterruptedException e) { e.printStackTrace(); } long curTime = System.currentTimeMillis(); for(NettyHttpSession session : sessions.values()){ if(session.expire()){ log.info(\u0026#34;Session(ID={}) is invalidated by Session Manager\u0026#34;, session.getId()); session.invalidate(); } } } } } } 可以改进的地方 Session持久化，包括可选的持久化时间间隔、shutdown自动持久化、startup自动读入。 Redis集中存储Session，便于服务集群使用 优化解析速度 ","date":"2017-09-02T15:11:03+08:00","image":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%9B%9B/jinjia_hu26351fb12c0954f3f1a41e5c79a350f9_35320_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%9B%9B/","title":"基于Netty的Spring Boot内置Servlet容器的实现（四）"},{"content":"基于Netty的Spring Boot内置Servlet容器的实现（三） EmbeddedServletContainer实现 Spring Boot启动过程与EmbeddedServletContainer 一般来说，Spring Boot的应用如果使用内置Servlet容器单独运行，我们都会在main()方法中调用\nSpringApplication.run(Object source, String... args); 方法。通过source参数构造一个SpringApplication对象再调用其\npublic ConfigurableApplicationContext run(String... args); 方法，这个方法先通过createApplicationContext()创建一个AnnotationConfigEmbeddedWebApplicationContext对象，随后会调用到\nprotected void refresh(ApplicationContext applicationContext); 方法，这个方法会调用到AbstractApplicationContext的refresh()方法。而通过下面的UML图可以看到，AnnotationConfigEmbeddedWebApplicationContext是AbstractApplicationContext的子类。 实际上，这里使用了模板设计模式，refresh()的具体流程由父类AbstractApplicationContext定义，具体的一些操作由子类去实现，在子类调用refresh()方法的时候，调用的是子类实现的操作方法，如：\nprotected void onRefresh() throws BeansException 方法。这个方法在AnnotationConfigEmbeddedWebApplicationContext的父类EmbeddedWebApplicationContext中有实现：\nprotected void onRefresh() { super.onRefresh(); try { this.createEmbeddedServletContainer(); } catch (Throwable var2) { throw new ApplicationContextException(\u0026#34;Unable to start embedded container\u0026#34;, var2); } } private void createEmbeddedServletContainer() { EmbeddedServletContainer localContainer = this.embeddedServletContainer; ServletContext localServletContext = this.getServletContext(); if (localContainer == null \u0026amp;\u0026amp; localServletContext == null) { EmbeddedServletContainerFactory containerFactory = this.getEmbeddedServletContainerFactory(); this.embeddedServletContainer = containerFactory.getEmbeddedServletContainer(new ServletContextInitializer[]{this.getSelfInitializer()}); } else if (localServletContext != null) { try { this.getSelfInitializer().onStartup(localServletContext); } catch (ServletException var4) { throw new ApplicationContextException(\u0026#34;Cannot initialize servlet context\u0026#34;, var4); } } this.initPropertySources(); } 检查embeddedServletContainer私有变量是否为空，为空的话获取EmbeddedServletContainerFactory工厂类（就是我们写的EmbeddedNettyFactory），获取到EmbeddedServletContainer实例并赋值给this.embeddedServletContainer。\n在模板方法AbstractApplicationContext.refresh()中，调用onRefresh()后，会继续调用finishRefresh()，通过上面的分析我们知道实际调用的是EmbeddedWebApplicationContext.finishRefresh()：\nprotected void finishRefresh() { super.finishRefresh(); EmbeddedServletContainer localContainer = this.startEmbeddedServletContainer(); if (localContainer != null) { this.publishEvent(new EmbeddedServletContainerInitializedEvent(this, localContainer)); } } private EmbeddedServletContainer startEmbeddedServletContainer() { EmbeddedServletContainer localContainer = this.embeddedServletContainer; if (localContainer != null) { localContainer.start(); } return localContainer; } 可以看到他调用了私有方法startEmbeddedServletContainer()启动容器，在这个方法里面，获取this.embeddedServletContainer（就是我们本文要实现的NettyContainer）然后执行其start()方法，以启动内置Servlet容器。\n因此我们应该在EmbeddedServletContainer实现类的start()对Netty服务器进行初始化。\nNettyContainer 自己编写的内置Servlet容器需要实现EmbeddedServletContainer接口，具体包括以下三个方法：\npublic interface EmbeddedServletContainer { void start() throws EmbeddedServletContainerException; //Spring Boot启动时调用 void stop() throws EmbeddedServletContainerException; //Spring Boot关闭时调用 int getPort(); //获取端口 } 这几个方法的用途比较清晰明确了，接下来就是实现。\n构造方法 首先在之前写的EmbeddedNettyFactory工厂类里面，需要调用将要写的EmbeddedNettyFactory的构造方法，并将必要的参数传入其构造方法，比如端口号、以及已经i初始化完毕的ServletContext实例。 构造方法：\nprivate final InetSocketAddress address; //监听端口地址 private final NettyContext context; //Context public NettyContainer(InetSocketAddress address, NettyContext context) { this.address = address; this.context = context; } 在EmbeddedNettyFactory中修改为：\npublic EmbeddedServletContainer getEmbeddedServletContainer(ServletContextInitializer... initializers) { /*…………………………*/ //return null; return new NettyContainer(address, context); //初始化容器并返回 } @Override public int getPort() { return address.getPort(); } start() 通过以上的分析，我们知道EmbeddedServletContainer的start()是由AbstractApplicationContext.refresh()模板方法负责调用启动的，我们应该在这个方法里面初始化Netty服务器。Netty的启动大家应该比较清楚了，无非就是设置两个EventLoopGroup用于处理请求的获取与读写，并设置Pipeline上的Handler，最后绑定端口，启动服务。以下是具体实现的代码：\n@Override public void start() throws EmbeddedServletContainerException { servletContext.setInitialised(false); ServerBootstrap sb = new ServerBootstrap(); //根据不同系统初始化对应的EventLoopGroup if (\u0026#34;Linux\u0026#34;.equals(StandardSystemProperty.OS_NAME.value())) { bossGroup = new EpollEventLoopGroup(1); workerGroup = new EpollEventLoopGroup();//不带参数，线程数传入0,实际解析为 Math.max(1, SystemPropertyUtil.getInt(\u0026#34;io.netty.eventLoopThreads\u0026#34;, Runtime.getRuntime().availableProcessors() * 2)); sb.channel(EpollServerSocketChannel.class) .group(bossGroup, workerGroup) .option(EpollChannelOption.TCP_CORK, true); } else { bossGroup = new NioEventLoopGroup(1); workerGroup = new NioEventLoopGroup(); sb.channel(NioServerSocketChannel.class) .group(bossGroup, workerGroup); } sb.option(ChannelOption.TCP_NODELAY, true) .option(ChannelOption.SO_REUSEADDR, true) .option(ChannelOption.SO_BACKLOG, 100); log.info(\u0026#34;Bootstrap configuration: \u0026#34; + sb.toString()); servletExecutor = new DefaultEventExecutorGroup(50); sb.childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline p = ch.pipeline(); p.addLast(\u0026#34;codec\u0026#34;, new HttpServerCodec(4096, 8192, 8192, false)); //HTTP编码解码Handler p.addLast(\u0026#34;servletInput\u0026#34;, new ServletContentHandler(servletContext)); //处理请求，读入数据，生成Request和Response对象 p.addLast(checkNotNull(servletExecutor), \u0026#34;filterChain\u0026#34;, new RequestDispatcherHandler(servletContext)); //获取请求分发器，让对应的Servlet处理请求，同时处理404情况 } }); servletContext.setInitialised(true); ChannelFuture future = sb.bind(address).awaitUninterruptibly(); Throwable cause = future.cause(); if (null != cause) { throw new EmbeddedServletContainerException(\u0026#34;Could not start Netty server\u0026#34;, cause); } log.info(servletContext.getServerInfo() + \u0026#34; started on port: \u0026#34; + getPort()); } 这里有两个Handler类是我们实现的——ServletContentHandler和RequestDispatcherHandler，我们将在后面讲解。\nstop() 在stop()方法里应该关闭在start()方法中开启的资源，以便Spring Boot关闭，防止资源/内存泄漏：\n@Override public void stop() throws EmbeddedServletContainerException { try { if (null != bossGroup) { bossGroup.shutdownGracefully().await(); } if (null != workerGroup) { workerGroup.shutdownGracefully().await(); } if (null != servletExecutor) { servletExecutor.shutdownGracefully().await(); } } catch (InterruptedException e) { throw new EmbeddedServletContainerException(\u0026#34;Container stop interrupted\u0026#34;, e); } } Netty服务设计 设计思路 看过Tomcat之类Servlet容器的源码的话，应该对Servleti容器设计有一点概念。\n首先我们需要通过Socket，处理HTTP连接，获取请求的数据，这一块可通过netty的API进行。 然后对接收到的数据进行解析封装成HttpServletRequest和HttpServletResponse对象，这一块需要netty自带的http解码编码器，并自定义Handler来i实现。 而HttpServletRequest本身也需要一些处理，比如Cookie、Session、Attributes（懒解析），需要自行实现。 接着需要对请求路径进行匹配，找到对应处理的Servlet， 这一部分前面已经实现了（2017-08-26似乎还有点Bug需要解决）。 接下来就是调用对应Servlet的service()方法，等待返回（在容器启动的时候需要对有on-startup的Servlet进行init()方法的调用）。 Servlet返回后，包装响应，处理异常和HTTP错误。 HTTP编码响应返回。 容器关闭的时候，调用所有已注册的Servlet的destroy()方法，并关闭打开的资源。 自定义Netty的Handler处理请求响应 根据前面的分析，我们的netty服务需要三个Handler，其中HTTP解码编码的有现成的HttpServerCodec，另外两个则需要我们自己实现。\n首先是对请求进行封装的Handler， 功能：\nchannel激活时， 开启一个新的输入流 有信息/请求进入时，封装请求和响应对象，执行读操作 channel恢复时，关闭输入流，等待下一次连接到来 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { inputStream = new HttpRequestInputStream(ctx.channel()); } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { if (msg instanceof HttpRequest) { HttpRequest request = (HttpRequest) msg; HttpResponse response = new DefaultHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, false); HttpUtil.setKeepAlive(response, HttpUtil.isKeepAlive(request)); NettyHttpServletResponse servletResponse = new NettyHttpServletResponse(ctx, servletContext, response); NettyHttpServletRequest servletRequest = new NettyHttpServletRequest(ctx, servletContext, request, servletResponse, inputStream); if (HttpUtil.is100ContinueExpected(request)) { //请求头包含Expect: 100-continue ctx.write(new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.CONTINUE), ctx.voidPromise()); } ctx.fireChannelRead(servletRequest); } if (msg instanceof HttpContent) { inputStream.addContent((HttpContent) msg); } } @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { inputStream.close(); } 然后是一个处理URL匹配分发请求的Handler，完成以下功能：\n读入请求数据时，对请求URI获取分发器 找不到返回404错误. 找到则调用FilterChain进行业务逻辑 最后关闭输出流 @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); } @Override protected void channelRead0(ChannelHandlerContext ctx, NettyHttpServletRequest request) throws Exception { HttpServletResponse servletResponse = (HttpServletResponse) request.getServletResponse(); try { NettyRequestDispatcher dispatcher = (NettyRequestDispatcher) context.getRequestDispatcher(request.getRequestURI()); if (dispatcher == null) { servletResponse.sendError(404); return; } dispatcher.dispatch(request, servletResponse); } finally { if (!request.isAsyncStarted()) { servletResponse.getOutputStream().close(); } } } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { logger.error(\u0026#34;Unexpected exception caught during request\u0026#34;, cause); ctx.close(); } ","date":"2017-08-27T08:52:03+08:00","image":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B8%89/yuyuko2_hueaf00a8425bfe94574c5d8750a924d5c_123372_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B8%89/","title":"基于Netty的Spring Boot内置Servlet容器的实现（三）"},{"content":"基于Netty的Spring Boot内置Servlet容器的实现（二） 实现Servlet Context接口 Servlet Context接口简介 接口ServletContext定义了一系列方法用于与相应的servlet容器通信，比如：获得文件的MIME类型，分派请求，或者是向日志文件写日志等。每一个web-app只能有一个ServletContext，webapp可以是一个放置有web application 文件的文件夹，也可以是一个.war的文件。ServletContext对象包含在ServletConfig对象之中，ServletConfig对象在servlet初始化时提供servlet对象。 接口ServletContext定义的方法比较多，大致可以分为：添加和配置Servlet、添加和配置Filter、添加和配置Listener、添加Servlet、Filter和Listener的注解处理需求、初始化参数、Context属性、资源获取等几大类方法。\n具体可以参考：\nTomcat的JavaDoc 中文翻译的文档 ServletContext 接口介绍 实现 实现的思想：\n不处理的：InitParameter相关的方法、Listener相关方法——目前用不到 以后处理的：Session Cookie相关的方法等待实现 Context的Attributes用Hashtable实现，主要是考虑到相关的方法需要返回Enumeration类型，用Hashtable有现成方法可以返回。 Filter的注册用HashMap存储FilterName及对应Registration的映射关系，暂时还没处理Filter的URL Pattern（所有注册的Filter对所有请求都会过滤，暂时可以满足需求） Servlet的注册也是用HashMap存储ServletName及对应Registration的映射关系，以及URL Pattern和ServletName的映射关系（相当与web.xml里的配置） 这里列出主要的方法，一些没具体实现，或者比较简单的方法就省略了（部分代码参考了Tomcat 8.0.45的源码）：\npackage io.gitlab.leibnizhu.sbnetty.core; /** * ServletContext实现 */ public class NettyContext implements ServletContext { private final Logger log = LoggerFactory.getLogger(getClass()); private final String contextPath; //保证不以“/”结尾 private final ClassLoader classLoader; private final String serverInfo; private volatile boolean initialized; //记录是否初始化完毕 private RequestUrlPatternMapper servletUrlPatternMapper; private final Map\u0026lt;String, NettyServletRegistration\u0026gt; servlets = new HashMap\u0026lt;\u0026gt;(); //getServletRegistration()等方法要用，key是ServletName private final Map\u0026lt;String, NettyFilterRegistration\u0026gt; filters = new HashMap\u0026lt;\u0026gt;(); //getFilterRegistration()等方法要用，Key是FilterName private final Map\u0026lt;String, String\u0026gt; servletMappings = new HashMap\u0026lt;\u0026gt;(); //保存请求路径urlPattern与Servlet名的映射,urlPattern是不带contextPath的 private final Hashtable\u0026lt;String, Object\u0026gt; attributes = new Hashtable\u0026lt;\u0026gt;(); /** * 默认构造方法 * * @param contextPath contextPath * @param classLoader classLoader * @param serverInfo 服务器信息，写在响应的server响应头字段 */ public NettyContext(String contextPath, ClassLoader classLoader, String serverInfo) { if(contextPath.endsWith(\u0026#34;/\u0026#34;)){ contextPath = contextPath.substring(0, contextPath.length() -1); } this.contextPath = contextPath; this.classLoader = classLoader; this.serverInfo = serverInfo; servletUrlPatternMapper = new RequestUrlPatternMapper(servletMappings, contextPath); } private void checkNotInitialised() { checkState(!isInitialised(), \u0026#34;This method can not be called before the context has been initialised\u0026#34;); } public void addServletMapping(String urlPattern, String name, Servlet servlet) { checkNotInitialised(); servletMappings.put(urlPattern, checkNotNull(name)); servletUrlPatternMapper.addWrapper(urlPattern, servlet, name); } public void addFilterMapping(EnumSet\u0026lt;DispatcherType\u0026gt; dispatcherTypes, boolean isMatchAfter, String urlPattern) { checkNotInitialised(); //TODO 过滤器的urlPatter解析 } @Override public String getMimeType(String file) { return MimeTypeUtil.getMimeTypeByFileName(file); } @Override public Set\u0026lt;String\u0026gt; getResourcePaths(String path) { Set\u0026lt;String\u0026gt; thePaths = new HashSet\u0026lt;\u0026gt;(); if (!path.endsWith(\u0026#34;/\u0026#34;)) { path += \u0026#34;/\u0026#34;; } String basePath = getRealPath(path); if (basePath == null) { return thePaths; } File theBaseDir = new File(basePath); if (!theBaseDir.exists() || !theBaseDir.isDirectory()) { return thePaths; } String theFiles[] = theBaseDir.list(); if (theFiles == null) { return thePaths; } for (String filename : theFiles) { File testFile = new File(basePath + File.separator + filename); if (testFile.isFile()) thePaths.add(path + filename); else if (testFile.isDirectory()) thePaths.add(path + filename + \u0026#34;/\u0026#34;); } return thePaths; } @Override public URL getResource(String path) throws MalformedURLException { if (!path.startsWith(\u0026#34;/\u0026#34;)) throw new MalformedURLException(\u0026#34;Path \u0026#39;\u0026#34; + path + \u0026#34;\u0026#39; does not start with \u0026#39;/\u0026#39;\u0026#34;); URL url = new URL(getClassLoader().getResource(\u0026#34;\u0026#34;), path.substring(1)); try { url.openStream(); } catch (Throwable t) { log.error(\u0026#34;Throwing exception when getting InputStream of \u0026#34; + path, t); url = null; } return url; } @Override public InputStream getResourceAsStream(String path) { try { return getResource(path).openStream(); } catch (IOException e) { log.error(e.getMessage(), e); return null; } } @Override public RequestDispatcher getRequestDispatcher(String path) { String servletName = servletUrlPatternMapper.getServletNameByRequestURI(path); Servlet servlet; try { servlet = null == servletName ? null : servlets.get(servletName).getServlet(); if (servlet == null) { return null; } //TODO 过滤器的urlPatter解析 List\u0026lt;Filter\u0026gt; allNeedFilters = new ArrayList\u0026lt;\u0026gt;(); for (NettyFilterRegistration registration : this.filters.values()) { allNeedFilters.add(registration.getFilter()); } FilterChain filterChain = new SimpleFilterChain(servlet, allNeedFilters); return new NettyRequestDispatcher(this, filterChain); } catch (ServletException e) { log.error(\u0026#34;Throwing exception when getting Filter from NettyFilterRegistration of path \u0026#34; + path, e); return null; } } @Override public String getRealPath(String path) { if (!path.startsWith(\u0026#34;/\u0026#34;)) return null; try { File f = new File(getResource(path).toURI()); return f.getAbsolutePath(); } catch (Throwable t) { log.error(\u0026#34;Throwing exception when getting real path of \u0026#34; + path, t); return null; } } @Override public String getServerInfo() { return serverInfo; } // InitParameter相关的方法不实现（返回空/空集合）基本用不到 @Override public Object getAttribute(String name) { return attributes.get(name); } @Override public Enumeration\u0026lt;String\u0026gt; getAttributeNames() { return attributes.keys(); } @Override public void setAttribute(String name, Object object) { attributes.put(name, object); } @Override public void removeAttribute(String name) { attributes.remove(name); } @Override public ServletRegistration.Dynamic addServlet(String servletName, String className) { return addServlet(servletName, className, null); } @Override public ServletRegistration.Dynamic addServlet(String servletName, Servlet servlet) { return addServlet(servletName, servlet.getClass().getName(), servlet); } @Override public ServletRegistration.Dynamic addServlet(String servletName, Class\u0026lt;? extends Servlet\u0026gt; servletClass) { return addServlet(servletName, servletClass.getName()); } private ServletRegistration.Dynamic addServlet(String servletName, String className, Servlet servlet) { NettyServletRegistration servletRegistration = new NettyServletRegistration(this, servletName, className, servlet); servlets.put(servletName, servletRegistration); return servletRegistration; } @Override public javax.servlet.FilterRegistration.Dynamic addFilter(String filterName, String className) { return addFilter(filterName, className, null); } @Override public javax.servlet.FilterRegistration.Dynamic addFilter(String filterName, Filter filter) { return addFilter(filterName, filter.getClass().getName(), filter); } private javax.servlet.FilterRegistration.Dynamic addFilter(String filterName, String className, Filter filter) { NettyFilterRegistration filterRegistration = new NettyFilterRegistration(this, filterName, className, filter); filters.put(filterName, filterRegistration); return filterRegistration; } @Override public javax.servlet.FilterRegistration.Dynamic addFilter(String filterName, Class\u0026lt;? extends Filter\u0026gt; filterClass) { return addFilter(filterName, filterClass.getName()); } @Override public \u0026lt;T extends Filter\u0026gt; T createFilter(Class\u0026lt;T\u0026gt; c) throws ServletException { try { return c.newInstance(); } catch (InstantiationException | IllegalAccessException e) { e.printStackTrace(); return null; } } @Override public javax.servlet.FilterRegistration getFilterRegistration(String filterName) { return filters.get(filterName); } @Override public Map\u0026lt;String, ? extends FilterRegistration\u0026gt; getFilterRegistrations() { return ImmutableMap.copyOf(filters); } //TODO Session Cookie相关的方法等待实现 //TODO 暂不支持Listener，现在很少用了吧 } URL Pattrn匹配查找 参考Tomcat源码，设计了一个RequestUrlPatternMapper类用于保存，计算URL-pattern与请求路径的匹配关系。在NettyContext的public RequestDispatcher getRequestDispatcher(String path)方法中可以看到对其的调用，传入请求的路径，返回对应处理的Servlet名称。此外在NettyContext的 public void addServletMapping(String urlPattern, String name, Servlet servlet)方法中也调用该类，增加新的Servlet映射。\n增加映射的时候，先后判断：\n路径匹配 扩展名匹配 默认匹配 精确匹配 用MappedWrapper类包装起新的Servlet，根据对应的匹配策略，放加入到ContextVersion实例的wildcardWrappers、extensionWrappers、defaultWrapper、exactWrappers中进行保存。\n在查询匹配的时候，处理完请求路径后，根据URL Pattern的定义，先后根据以下匹配方法进行匹配：\n精确匹配 路径匹配 后缀名匹配 Welcome资源匹配 默认Servlet匹配 使用MappingData类实例对查询结果进行保存，每一级匹配如果已经找到对应的Servlet，那么下一级的匹配将不会进行，直接返回，此时MappingData对象里保存的就是最终匹配到的结果。\n具体的匹配中，精确匹配直接对Map进行查找即可，后缀名匹配类似，根据当前请求的后缀名进行精确匹配；而路径匹配，则是将路径进行降序排序，匹配的时候依次匹配，就能匹配到最长的那一个。\n下面贴上主要的实现代码：\npackage io.gitlab.leibnizhu.sbnetty.utils; /** * 保存，计算URL-pattern与请求路径的匹配关系 * * @author Leibniz.Hu * Created on 2017-08-25 11:32. */ public class RequestUrlPatternMapper { private final Logger log = LoggerFactory.getLogger(getClass()); private UrlPatternContext urlPatternContext; private String contextPath; public RequestUrlPatternMapper(String contextPath) { this.urlPatternContext = new UrlPatternContext(); this.contextPath = contextPath; } /** * 增加映射关系 * * @param urlPattern urlPattern * @param servlet servlet对象 * @param servletName servletName * @author Leibniz */ public void addServlet(String urlPattern, Servlet servlet, String servletName) throws ServletException { if (urlPattern.endsWith(\u0026#34;/*\u0026#34;)) { // 路径匹配 String pattern = urlPattern.substring(0, urlPattern.length() - 1); for (MappedServlet ms : urlPatternContext.wildcardServlets) { if (ms.pattern.equals(pattern)) { throw new ServletException(\u0026#34;URL Pattern(\u0026#39;\u0026#34; + urlPattern + \u0026#34;\u0026#39;) already exists!\u0026#34;); } } MappedServlet newServlet = new MappedServlet(pattern, servlet, servletName); urlPatternContext.wildcardServlets.add(newServlet); urlPatternContext.wildcardServlets.sort((o1, o2) -\u0026gt; o2.pattern.compareTo(o1.pattern)); log.debug(\u0026#34;Curretn Wildcard URL Pattern List = \u0026#34; + Arrays.toString(urlPatternContext.wildcardServlets.toArray())); } else if (urlPattern.startsWith(\u0026#34;*.\u0026#34;)) { // 扩展名匹配 String pattern = urlPattern.substring(2); if (urlPatternContext.extensionServlets.get(pattern) != null) { throw new ServletException(\u0026#34;URL Pattern(\u0026#39;\u0026#34; + urlPattern + \u0026#34;\u0026#39;) already exists!\u0026#34;); } MappedServlet newServlet = new MappedServlet(pattern, servlet, servletName); urlPatternContext.extensionServlets.put(pattern, newServlet); log.debug(\u0026#34;Curretn Extension URL Pattern List = \u0026#34; + Arrays.toString(urlPatternContext.extensionServlets.keySet().toArray())); } else if (urlPattern.equals(\u0026#34;/\u0026#34;)) { // Default资源匹配 if (urlPatternContext.defaultServlet != null) { throw new ServletException(\u0026#34;URL Pattern(\u0026#39;\u0026#34; + urlPattern + \u0026#34;\u0026#39;) already exists!\u0026#34;); } urlPatternContext.defaultServlet = new MappedServlet(\u0026#34;\u0026#34;, servlet, servletName); } else { // 精确匹配 String pattern; if (urlPattern.length() == 0) { pattern = \u0026#34;/\u0026#34;; } else { pattern = urlPattern; } if (urlPatternContext.exactServlets.get(pattern) != null) { throw new ServletException(\u0026#34;URL Pattern(\u0026#39;\u0026#34; + urlPattern + \u0026#34;\u0026#39;) already exists!\u0026#34;); } MappedServlet newServlet = new MappedServlet(pattern, servlet, servletName); urlPatternContext.exactServlets.put(pattern, newServlet); log.debug(\u0026#34;Curretn Exact URL Pattern List = \u0026#34; + Arrays.toString(urlPatternContext.exactServlets.keySet().toArray())); } } /** * 删除映射关系 * * @param urlPattern */ public void removeServlet(String urlPattern) { if (urlPattern.endsWith(\u0026#34;/*\u0026#34;)) { //路径匹配 String pattern = urlPattern.substring(0, urlPattern.length() - 2); urlPatternContext.wildcardServlets.removeIf(mappedServlet -\u0026gt; mappedServlet.pattern.equals(pattern)); } else if (urlPattern.startsWith(\u0026#34;*.\u0026#34;)) { // 扩展名匹配 String pattern = urlPattern.substring(2); urlPatternContext.extensionServlets.remove(pattern); } else if (urlPattern.equals(\u0026#34;/\u0026#34;)) { // Default资源匹配 urlPatternContext.defaultServlet = null; } else { // 精确匹配 String pattern; if (urlPattern.length() == 0) { pattern = \u0026#34;/\u0026#34;; } else { pattern = urlPattern; } urlPatternContext.exactServlets.remove(pattern); } } public String getServletNameByRequestURI(String absoluteUri) { MappingData mappingData = new MappingData(); try { matchRequestPath(absoluteUri, mappingData); } catch (IOException e) { log.error(\u0026#34;Throwing exception when getting Servlet Name by request URI, maybe cause by lacking of buffer size.\u0026#34;, e); } return mappingData.servletName; } /** * Wrapper mapping. * * @throws IOException buffer大小不足 */ private void matchRequestPath(String absolutePath, MappingData mappingData) throws IOException { // 处理ContextPath，获取访问的相对URI boolean noServletPath = absolutePath.equals(contextPath) || absolutePath.equals(contextPath + \u0026#34;/\u0026#34;); if (!absolutePath.startsWith(contextPath)) { return; } String path = noServletPath ? \u0026#34;/\u0026#34; : absolutePath.substring(contextPath.length()); //去掉查询字符串 int queryInx = path.indexOf(\u0026#39;?\u0026#39;); if(queryInx \u0026gt; -1){ path = path.substring(0, queryInx); } // 优先进行精确匹配 internalMapExactWrapper(urlPatternContext.exactServlets, path, mappingData); // 然后进行路径匹配 if (mappingData.servlet == null) { internalMapWildcardWrapper(urlPatternContext.wildcardServlets, path, mappingData); //TODO 暂不考虑JSP的处理 } if (mappingData.servlet == null \u0026amp;\u0026amp; noServletPath) { // 路径为空时，重定向到“/” mappingData.servlet = urlPatternContext.defaultServlet.object; mappingData.servletName = urlPatternContext.defaultServlet.servletName; return; } // 后缀名匹配 if (mappingData.servlet == null) { internalMapExtensionWrapper(urlPatternContext.extensionServlets, path, mappingData); } //TODO 暂不考虑Welcome资源 // Default Servlet if (mappingData.servlet == null) { if (urlPatternContext.defaultServlet != null) { mappingData.servlet = urlPatternContext.defaultServlet.object; mappingData.servletName = urlPatternContext.defaultServlet.servletName; } //TODO 暂不考虑请求静态目录资源 if (path.charAt(path.length() - 1) != \u0026#39;/\u0026#39;) { } } } /** * 精确匹配 */ private void internalMapExactWrapper(Map\u0026lt;String, MappedServlet\u0026gt; servlets, String path, MappingData mappingData) { MappedServlet servlet = servlets.get(path); if (servlet != null) { mappingData.servlet = servlet.object; mappingData.servletName = servlet.servletName; } } /** * 路径匹配 */ private void internalMapWildcardWrapper(List\u0026lt;MappedServlet\u0026gt; servlets, String path, MappingData mappingData) { if (!path.endsWith(\u0026#34;/\u0026#34;)) { path = path + \u0026#34;/\u0026#34;; } MappedServlet result = null; for (MappedServlet ms : servlets) { if (path.startsWith(ms.pattern)) { result = ms; break; } } if (result != null) { mappingData.servlet = result.object; mappingData.servletName = result.servletName; } } /** * 后缀名匹配 */ private void internalMapExtensionWrapper(Map\u0026lt;String, MappedServlet\u0026gt; servlets, String path, MappingData mappingData) { int dotInx = path.lastIndexOf(\u0026#39;.\u0026#39;); path = path.substring(dotInx + 1); MappedServlet servlet = servlets.get(path); if (servlet != null) { mappingData.servlet = servlet.object; mappingData.servletName = servlet.servletName; } } /* * 以下是用到的内部类 */ private class UrlPatternContext { MappedServlet defaultServlet = null; //默认Servlet Map\u0026lt;String, MappedServlet\u0026gt; exactServlets = new HashMap\u0026lt;\u0026gt;(); //精确匹配 List\u0026lt;MappedServlet\u0026gt; wildcardServlets = new LinkedList\u0026lt;\u0026gt;(); //路径匹配 Map\u0026lt;String, MappedServlet\u0026gt; extensionServlets = new HashMap\u0026lt;\u0026gt;(); //扩展名匹配 } private class MappedServlet extends MapElement\u0026lt;Servlet\u0026gt; { @Override public String toString() { return pattern; } String servletName; MappedServlet(String name, Servlet servlet, String servletName) { super(name, servlet); this.servletName = servletName; } } private class MapElement\u0026lt;T\u0026gt; { final String pattern; final T object; MapElement(String pattern, T object) { this.pattern = pattern; this.object = object; } } } public class MappingData { Servlet servlet = null; String servletName; String redirectPath ; public void recycle() { servlet = null; servletName = null; redirectPath = null; } } 再次启动 现在ServletContext有了，再次启动，不再报错了。\n::: Using Embedded Netty Servlet Container (version:) ::: ＼(^O^)／ Spring-Boot 1.5.2.RELEASE 2017-08-25 22:08:33.019 INFO 17565 --- [ main] io.gitlab.leibnizhu.sbnetty.TestWebApp : Starting TestWebApp on XPS13 with PID 17565 ……………… ……………… ……………… 2017-08-25 22:08:35.760 INFO 17565 --- [ main] io.gitlab.leibnizhu.sbnetty.TestWebApp : Started TestWebApp in 3.383 seconds (JVM running for 4.012) 2017-08-25 22:08:35.761 INFO 17565 --- [ Thread-2] ationConfigEmbeddedWebApplicationContext : Closing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@4a07d605: startup date [Fri Aug 25 22:08:33 CST 2017]; root of context hierarchy 2017-08-25 22:08:35.763 INFO 17565 --- [ Thread-2] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown Disconnected from the target VM, address: \u0026#39;127.0.0.1:46101\u0026#39;, transport: \u0026#39;socket\u0026#39; 可是…………好像有点不对劲……\n启动之后过一会儿就自动关了。\n原因很简单，在EmbeddedNettyFactory类里面，我们还没返回真正的EmbeddedServletContainer实现类，而只是返回null，所以Spring没有Servlet容器可用，也就只能关闭啦。\n我们将在下一篇文章里讨论如何实现EmbeddedServletContainer——与netty结合最紧密的地方。\n","date":"2017-08-24T17:43:37+08:00","image":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BA%8C/sanae_hu112f514aec6702f2f444da4fa7f9ccac_72624_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BA%8C/","title":"基于Netty的Spring Boot内置Servlet容器的实现（二）"},{"content":"基于Netty的Spring Boot内置Servlet容器的实现（一） 前言 Spring Boot有Tomcat、Jetty和undertow三种内置Servlet容器，默认使用Tomcat。\n一般来说已经够用了，但当Spring Boot用于高并发微服务的时候，可能并不够用，而且tomcat的资源占用在这种情况下说不上轻量化了。于是萌生了自己实现一个Spring Boot的Netty Servlet容器的想法。\n接下来可能会有几篇文章关于这个的，相应的代码也在开发之中，放在Gitlab 和 GitHub里。\n需要完成的任务 实现Servlet容器 Servlet规范有以下几个核心类(接口)：\nServletContext：定义了一些可以和Servlet Container交互的方法。 Registration：实现Filter和Servlet的动态注册。 ServletRequest(HttpServletRequest)：对HTTP请求消息的封装。 ServletResponse(HttpServletResponse)：对HTTP响应消息的封装。 RequestDispatcher：将当前请求分发给另一个URL，甚至ServletContext以实现进一步的处理。 Servlet(HttpServlet)：所有“服务器小程序”要实现了接口，这些“服务器小程序”重写doGet、doPost、doPut、doHead、doDelete、doOption、doTrace等方法(HttpServlet)以实现响应请求的相关逻辑。 Filter(FilterChain)：在进入Servlet前以及出Servlet以后添加一些用户自定义的逻辑，以实现一些横切面相关的功能，如用户验证、日志打印等功能。 AsyncContext：实现异步请求处理。 我们想要实现一个Servlet容器，不管是要重头实现一个类似tomcat的容器，还是要实现一个Spring Boot内置Servlet容器，都需要实现以上接口。\n我们的任务就是利用Netty的API实现以上接口。\n实现Spring Boot内置Servlet容器接口 具体来说，就是要实现EmbeddedServletContainer接口，同时实现一个配置类，配置Spring Boot在哪些情况下启动我们的Netty Servlet容器。\n编写测试类/方法 需要测试以下内容:\n基本的SpringMVC功能，如请求分发、响应是否正常 异步请求 热交换 缓存 Session 在一个现有Spring Boot项目中测试使用 与内置Tomcat、Jetty的性能对比 ………… 参考 感谢以下项目/博文的作者：\nSpringBoot源码分析之内置Servlet容器 Github DanielThomas/spring-boot-starter-netty 现在开始 首先创建一个Maven项目。\nMaven依赖 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;io.gitlab.leibnizhu\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-netty\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;spring-boot-starter-netty\u0026lt;/name\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.2.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- Netty及其建议的反射依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.netty\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;netty-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.2.Final\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.javassist\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javassist\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.20.0-GA\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Spring Boot基本依赖及测试，排除内置tomcat，我们自己来实现 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Servleten基本API --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javax.servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;18.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- 省略 --\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; Web应用测试类 我们直接在test包里创建一个SpringBoot应用，暂时先覆盖最基本的SpringMVC使用。\npackage io.gitlab.leibnizhu.sbnetty; @Controller @EnableAutoConfiguration(exclude = WebMvcAutoConfiguration.class) @ComponentScan @EnableWebMvc public class TestWebApp { private static final String MESSAGE = \u0026#34;Hello, World!这是一条测试语句\u0026#34;; public static void main(String[] args) { SpringApplication.run(TestWebApp.class, args); } @RequestMapping(value = \u0026#34;/plaintext\u0026#34;, produces = \u0026#34;text/plain\u0026#34;) @ResponseBody public String plaintext() { return MESSAGE; } @RequestMapping(value = \u0026#34;/async\u0026#34;, produces = \u0026#34;text/plain\u0026#34;) @ResponseBody public Callable\u0026lt;String\u0026gt; async() { return () -\u0026gt; MESSAGE; } @RequestMapping(value = \u0026#34;/json\u0026#34;, produces = \u0026#34;application/json\u0026#34;) @ResponseBody public Message json() { return new Message(MESSAGE); } @Bean public ServletRegistrationBean nullServletRegistration() { return new ServletRegistrationBean(new HttpServlet(){ @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getOutputStream().print(\u0026#34;Null Servlet Test\u0026#34;); } }, \u0026#34;/null\u0026#34;); } private static class Message { private final String message; public Message(String message) { this.message = message; } public String getMessage() { return message; } } } 实现EmbeddedServletContainerFactory接口 直接启动，提示缺少EmbeddedServletContainerFactoryBean：\norg.springframework.context.ApplicationContextException: Unable to start embedded container; nested exception is org.springframework.context.ApplicationContextException: Unable to start EmbeddedWebApplicationContext due to missing EmbeddedServletContainerFactory bean. Spring Boot会查找EmbeddedServletContainerFactory接口的实现类(工厂类)，调用其getEmbeddedServletContainer()方法，来获取web应用的容器。 所以我们要实现这个接口，这里不直接实现，而是通过继承AbstractEmbeddedServletContainerFactory类来实现。 其中最重要的就是\npublic EmbeddedServletContainer getEmbeddedServletContainer(ServletContextInitializer... initializers); 方法，用于生成EmbeddedServletContainer容器实例，顺便可以做一些初始化动作，比如定义监听的端口号，初始化Context，同时调用传入参数的ServletContextInitializer（Servlet初始化器）们的onStartup()方法以设置ServletContext中的一些配置。\n目前的实现是这样的：\npackage io.gitlab.leibnizhu.sbnetty.bootstrap; /** * Spring Boot会查找EmbeddedServletContainerFactory接口的实现类(工厂类)，调用其getEmbeddedServletContainer()方法，来获取web应用的容器 * 所以我们要实现这个接口，这里不直接实现，而是通过继承AbstractEmbeddedServletContainerFactory类来实现 * * @author Leibniz on 2017-08-24. */ public class EmbeddedNettyFactory extends AbstractEmbeddedServletContainerFactory implements ResourceLoaderAware { private final static Logger LOG = LoggerFactory.getLogger(EmbeddedNettyFactory.class); private static final String SERVER_INFO = \u0026#34;Netty@SpringBoot\u0026#34;; private ResourceLoader resourceLoader; @Override public EmbeddedServletContainer getEmbeddedServletContainer(ServletContextInitializer... initializers) { //Netty启动环境相关信息 Package nettyPackage = Bootstrap.class.getPackage(); String title = nettyPackage.getImplementationTitle(); String version = nettyPackage.getImplementationVersion(); LOG.info(\u0026#34;Running with \u0026#34; + title + \u0026#34; \u0026#34; + version); //上下文，暂时为空 ServletContext context = null; if (isRegisterDefaultServlet()) { LOG.warn(\u0026#34;This container does not support a default servlet\u0026#34;); } for (ServletContextInitializer initializer : initializers) { try { initializer.onStartup(context); } catch (ServletException e) { throw new RuntimeException(e); } } //从SpringBoot配置中获取端口，如果没有则随机生成 int port = getPort() \u0026gt; 0 ? getPort() : new Random().nextInt(65535 - 1024) + 1024; InetSocketAddress address = new InetSocketAddress(port); LOG.info(\u0026#34;Server initialized with port: \u0026#34; + port); return null; //初始化容器并返回 } @Override public void setResourceLoader(ResourceLoader resourceLoader) { this.resourceLoader = resourceLoader; } } 现在ServletContext和EmbeddedServletContainer接口还没实现，先用null代替。\n配置Spring Boot启动自定义Servlet容器 就这样直接启动测试Web应用是不行的，因为这个EmbeddedNettyFactory并没有被Spring加载。\n想被Spring加载很简单，类加@Component之类的注解就行，但这样集成在任何环境中都会加载，可能引起端口冲突。\n所以我们还要写一个配置类，配置Spring什么时候去加载EmbeddedNettyFactory，具体如下，注释里写得比较清楚了：\npackage io.gitlab.leibnizhu.sbnetty.bootstrap; /** * 配置加载内置Netty容器的工厂类Bean。 * 最早是直接将EmbeddedNettyFactory加@Component注解，这样集成在任何环境中都会加载，可能引起端口冲突。 * 所以通过这个配置类，配置在当前上下文缺少EmbeddedServletContainerFactory接口实现类时（即缺少内置Servlet容器），加载EmbeddedNettyFactory * 这样SpringBoot项目在引入这个maven依赖，并且排除了内置tomcat依赖、且没引入其他servlet容器（如jetty）时，就可以通过工厂类加载并启动netty容器了。 * * @author Leibniz 2017-08-24 */ @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @Configuration @ConditionalOnWebApplication // 在Web环境下才会起作用 public class EmbeddedNettyAutoConfiguration { @Configuration @ConditionalOnClass({Bootstrap.class}) // Netty的Bootstrap类必须在classloader中存在，才能启动Netty容器 @ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT) //当前Spring容器中不存在EmbeddedServletContainerFactory接口的实例 public static class EmbeddedNetty { //上述条件注解成立的话就会构造EmbeddedNettyFactory这个EmbeddedServletContainerFactory @Bean public EmbeddedNettyFactory embeddedNettyFactory() { return new EmbeddedNettyFactory(); } } } 再次启动 这样子是启动不了的，但启动报错信息已经改了，变成：\n2017-08-24 14:20:25.660 ERROR 16708 --- [ main] o.s.boot.SpringApplication : Application startup failed org.springframework.context.ApplicationContextException: Unable to start embedded container; nested exception is java.lang.NullPointerException at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.onRefresh(EmbeddedWebApplicationContext.java:137) ~[spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:536) ~[spring-context-4.3.7.RELEASE.jar:4.3.7.RELEASE] at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122) ~[spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:737) [spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:370) [spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:314) [spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1162) [spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1151) [spring-boot-1.5.2.RELEASE.jar:1.5.2.RELEASE] at io.gitlab.leibnizhu.sbnetty.TestWebApp.main(TestWebApp.java:102) [test-classes/:na] Caused by: java.lang.NullPointerException: null 因为SpringBoot在启动的时候，SpringApplication会调用refresh(context)方法进行初始化动作，而我们的context传入了null，当然报空指针异常了。\n我们将在下一篇文章再讨论怎么实现这个。\n","date":"2017-08-24T14:30:11+08:00","image":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B8%80/yuyuko_hua56c2355a3520ae6430109398d99a70a_98080_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E5%9F%BA%E4%BA%8ENetty%E7%9A%84Spring-Boot%E5%86%85%E7%BD%AEServlet%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B8%80/","title":"基于Netty的Spring Boot内置Servlet容器的实现（一）"},{"content":"HashMap线程安全性讨论 去年写的 HashMap源码阅读笔记 分析了JDK8 中的HashMap源码的get()和put()两大方法，当时并没有考虑到HashMap的线程安全性。\n众所周知，HashMap并非线程安全的，但在 JDK8 之前，HashMap的线程安全不但体现在多线程读写可能出现数据错误，还存在一个多线程扩容导致的死循环Bug。本文将讨论这一Bug，及在 JDK8 中的修复，以及相关的ConcurrentHashMap。\nJDK8 之前的多线程扩容Bug 扩容代码解析 我们直接看JDK7 中HashMap的扩容方法：\nvoid resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; //最大长度限制在Integer.MAX_VALUE if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity];//按新的长度分配哈希表数组 transfer(newTable);//将旧的哈希表重新分配到新哈希表里，可能导致环链 table = newTable;//将新的哈希表复制到HashMap的table变量 threshold = (int)(newCapacity * loadFactor);//按新容量更新扩容阈值 } void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j \u0026lt; src.length; j++) { Entry\u0026lt;K,V\u0026gt; e = src[j]; if (e != null) { src[j] = null; //关键部分 do { Entry\u0026lt;K,V\u0026gt; next = e.next;//获得原来链表中的下一个元素 int i = indexFor(e.hash, newCapacity);//获取扩容后新的下标，可能跟原来一样，或者比原来的大newCapacity/2 e.next = newTable[i];//设置当前元素在链表的下一个元素为新哈希表对应位置原来的元素，也就是说旧元素会被往后推，该链表上最早读取的元素会成为尾部 newTable[i] = e;//哈希表的入口改为当前元素 e = next;//遍历原来链表的下一个元素 } while (e != null); } } } 工作原理在上面代码的注释里说得比较清楚了，值得注意的是，transfer(newTable)之后的新哈希表里，每个链表的顺序都与扩容之前的刚好相反，这一点直接注定了会出现多线程扩容Bug。\n扩容时的多线程死循环Bug 如上面的分析，由于扩容后每个链表的顺序都调转了，因此定性地分析，多线程同时触发扩容的时候，有可能其中一个线程已经把链表调转了，而另一个线程获取了链表调转前的状态，重新获取到时间片的时候，再次翻转，导致next引用的赋值出错，链表中构成环；这样在调用get()方法的时候，进入到这个链表就会陷入死循环。 具体的举例分析，可以参考美团点评技术团队的文章 Java 8系列之重新认识HashMap ，在此贴出关键的部分：\n代码例子如下(便于理解，仍然使用JDK1.7的环境)：\npublic class HashMapInfiniteLoop { private static HashMap\u0026lt;Integer,String\u0026gt; map = new HashMap\u0026lt;Integer,String\u0026gt;(2，0.75f); public static void main(String[] args) { map.put(5， \u0026#34;C\u0026#34;); new Thread(\u0026#34;Thread1\u0026#34;) { public void run() { map.put(7, \u0026#34;B\u0026#34;); System.out.println(map); }; }.start(); new Thread(\u0026#34;Thread2\u0026#34;) { public void run() { map.put(3, \u0026#34;A\u0026#34;); System.out.println(map); }; }.start(); } } 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。\n通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。\n注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。\n线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。\ne.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。\n于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。\nJDK8 如何修复多线程扩容Bug 正如我在 HashMap源码阅读笔记 中分析到的，JDK8 中Node\u0026lt;K,V\u0026gt;[] resize()扩容方法利用了哈希表长度为2的幂，以及get()方法对哈希值取低位的特性，结合每次扩容哈希表大小都增倍等特性，每次扩容，一个哈希桶里的元素在扩容后的位置，只会是原位置，或者原位置+原哈希表大小。\n因此扩容的时候可以将一个哈希桶的扩容结果分为两个链表，还在原来位置的记为 低位链表(我自己起的名字)，用loHead loTail标记其头尾；扩容后分配到原位置+原哈希表大小的构成 高位链表(同样是我自己起的名字)，用hiHead hiTail标记其头尾。\n具体实现的核心部分代码如下（省略了特殊容量处理、红黑树处理等等代码）：\nfinal Node\u0026lt;K,V\u0026gt;[] resize() { /*…………省略前面代码…………*/ Node\u0026lt;K,V\u0026gt;loHead=null,loTail=null;//记录低位链表头尾位置 Node\u0026lt;K,V\u0026gt;hiHead=null,hiTail=null;//记录高位链表头尾位置 Node\u0026lt;K,V\u0026gt;next;//记录当前链表元素在原来链表中的下一个元素，便于下次循环使用 //遍历哈希桶的链表，拆分成高位和低位链表 do{ next=e.next; if((e.hash\u0026amp;oldCap)==0){ //新增的有效哈希位为0，即当前元素扩容后分配到 低位链表 if(loTail==null) //低位链表尚未初始化 loHead=e; //设置低位链表头部 else loTail.next=e; //低位链表尾部增加当前元素，以保持原链表顺序 loTail=e; //更新低位链表的尾部 }else{ //新增的有效哈希位为1，即当前元素扩容后分配到 高位链表 if(hiTail==null) //高低位链表尚未初始化 hiHead=e; //设置高位链表头部 else hiTail.next=e; //高位链表尾部增加当前元素，以保持原链表顺序 hiTail=e; //更新高位链表的尾部 } }while((e=next)!=null); //更新两个链表到哈希表中 if(loTail!=null){ //扩容后低位链表不为空，需要处理 loTail.next=null; //低位链表设置尾部结束 newTab[j]=loHead; //哈希桶设置链表入口 } if(hiTail!=null){ //扩容后高位链表不为空，需要处理 hiTail.next=null; //高位链表设置尾部结束 newTab[j+oldCap]=hiHead; //哈希桶设置链表入口 } /*…………省略后面代码…………*/ } 具体处理的过程我在上面的注释里说得比较清楚了，包括代码之前的那段文字。\n可以看到，扩容后，原来哈希桶的链表被拆分为两个，两个链表中的元素都能继续维持原有的顺序。这样就算在多线程环境下同时扩容，一个线程A读取链表状态后停止工作，另一个线程B对同一链表的前几个元素进行扩容分成两个链表，此时线程A恢复工作，由于线程B对链表元素的顺序没有发生变化，所以线程A恢复工作后只是重复了拆分链表的工作，而不会因为链表已被改变顺序而导致环的生成，因此不会发生死循环的问题。\n也就是说 JDK8 的HashMap扩容方法不但效率提升了（根据哈希值特点拆分链表，红黑树），而且还维持了扩容前后的链表顺序，从而解决了多线程扩容使链表产生环，导致死循环的问题。\n其他线程安全Map Hashtable类 类名叫Hashtable不叫HashTable真是逼死强迫症。这个类现在很少用了，从源码可以看到，它是在get() put()等方法的声明里加了synchronized关键字来实现多线程安全的，因此显然效率比较低。\nCollections.synchronizedMap()方法 在Collections工具类里有\npublic static \u0026lt;K,V\u0026gt; Map\u0026lt;K,V\u0026gt; synchronizedMap(Map\u0026lt;K,V\u0026gt; m); 方法，传入将普通的Map实例，返回一个线程安全的Map实例。实现方法也比较简单，返回的是Collections的内部类：\nprivate static class SynchronizedMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt;, Serializable { private final Map\u0026lt;K,V\u0026gt; m; // Backing Map final Object mutex; // Object on which to synchronize SynchronizedMap(Map\u0026lt;K,V\u0026gt; m) { this.m = Objects.requireNonNull(m); mutex = this; } SynchronizedMap(Map\u0026lt;K,V\u0026gt; m, Object mutex) { this.m = m; this.mutex = mutex; } public V get(Object key) { synchronized (mutex) {return m.get(key);} } public V put(K key, V value) { synchronized (mutex) {return m.put(key, value);} } /*………省略其他方法，大同小异………*/ } 的实例。上面也给出了简要的类代码，实现的方法相当简单粗暴，由构造方法传入线程不安全的类实例，用一个锁（默认是自身，也可以在构造方法里传入），在各种Map接口的方法里面使用这个锁对线程不安全的类实例方法进行同步，也就是一个包装的设计模式，用同步代码块包装原有方法。\n显然这样得到的线程安全类的效率也不高。\nConcurrentHashMap类 JDK8 之前的ConcurrentHashMap使用Segment（锁段）提高同步的效率，而 JDK8 开始利用CAS算法大大提高了实现线程安全的效率。有空要再写一篇博客分析一下ConcurrentHashMap的源码。\n","date":"2017-08-01T16:36:39+08:00","image":"https://leibnizhu.github.io/p/HashMap%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E8%AE%A8%E8%AE%BA/sundown_hu870a7df00d029233d76d0371e0b6a9e2_43340_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/HashMap%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E8%AE%A8%E8%AE%BA/","title":"HashMap线程安全性讨论"},{"content":"LinkedList源码阅读笔记 继承结构 成员变量 //记录长度 transient int size = 0; //第一个元素的指针 transient Node\u0026lt;E\u0026gt; first; //最后一个元素的指针（为了增加从尾部开始的速度，及支持一些增删改查尾部元素的方法） transient Node\u0026lt;E\u0026gt; last; 以上成员变量都用了transient关键字避免被持久化。\n从继承结构来看，LinkedList继承了AbstractSequentialList，而AbstractSequentialList继承了AbstractList，因此LinkedList也继承了protected transient int modCount变量，具体作用与ArayList中一样，不再赘述。\n构造方法 public LinkedList() 默认构造方法，实现为空方法。\npublic LinkedList(Collection\u0026lt;? extends E\u0026gt; c) 调用addAll()方法将参数c中的所有元素增加到当前实例中，完成构造。\n内部类Node 定义 在LinkedList中，使用Node内部类存储List的每个元素，其结构比较简单:\nitem存储具体的List元素引用 next和prev分别存储后一个和上一个元素的引用，由此构成链表。 private static class Node\u0026lt;E\u0026gt; { E item; Node\u0026lt;E\u0026gt; next; Node\u0026lt;E\u0026gt; prev; Node(Node\u0026lt;E\u0026gt; prev, E element, Node\u0026lt;E\u0026gt; next) { this.item = element; this.next = next; this.prev = prev; } } 维护链表的基础方法 /** * 将元素插入到链表头部 * prev=LinkedList.next, next=null * 然后更新first，原来first的prev引用更新为新元素 */ private void linkFirst(E e); /** * 将元素插入到链表尾部 * prev=null, next=LinkedList.first * 然后更新last，原来last的next引用更新为新元素 */ void linkLast(E e); /** * 插入到指定元素前面 * 设置新元素的前后引用 * 然后对应修改succ的前一项为新元素，以及原来succ的前一项的后者为新元素 * 如果succ原来的前一项为null，那么新的LinkedList.first就是新元素 * 我觉得可以直接进来就判断nullsucc原来的前一项时调用linkFirst()进行处理 */ void linkBefore(E e, Node\u0026lt;E\u0026gt; succ); /** * 将链表头部元素从链表中移除并返回 * 主要任务是将LinkedList.first指向原来first的next所引用的元素 * 同时将原来first的item以及next设为null * item设为null是为了原来first元素的GC回收，而next设为null是方便新的first在未来被回收 * 最后size--，modCount++，维护边界条件并返回 */ private E unlinkFirst(Node\u0026lt;E\u0026gt; f); /** * 类似unlinkLast()，将链表尾部元素从链表中移除并返回，不再赘述 */ private E unlinkLast(Node\u0026lt;E\u0026gt; l)； /** * 将任一位置的元素从链表中移除并返回 * 代码实现上类似于unlinkFirst()与unlinkLast()的结合 */ E unlink(Node\u0026lt;E\u0026gt; x) 常规方法 维护头部尾部的方法 //first为空则抛NoSuchElementException异常，否则返回first.item public E getFirst(); //last为空则抛NoSuchElementException异常，否则返回last.item public E getLast(); //first为空则抛异常，否则调用unlinkFirst()删除 public E removeFirst(); //last为空则抛异常，否则调用unlinkLast()删除 public E removeLast(); //调用linkFirst()进行处理，允许e为null public void addFirst(E e); //调用linkLast()进行处理，允许e为null public void addLast(E e); 接口常规增删改查方法 增 public boolean add(E e); //调用linkLast()进行处理，并返回true public void add(int index, E element); 指定下标插入元素的方法add()先对index进行下标越界检查，然后如果index==size，即插入到List最后面，那么直接调用linkLast()执行插入就可以；否则调用linkBefore()插入到指定下标的元素前面，即新的元素拥有index的下标（符合add()方法对index参数的定义）。\npublic boolean addAll(Collection\u0026lt;? extends E\u0026gt; c); //调用addAll(size, c) public boolean addAll(int index, Collection\u0026lt;? extends E\u0026gt; c); 批量增加方法addAll()将参数c里面的元素全部逐个加到当前List中。先检查下标是否越界，然后将Collection转为Object数组，若数组长度为0则返回false。index为要插入的第一个元素所在的下标，如果不是size（最后一个元素之后），则需要读取对应插入处原来的元素及其上一个元素。\nNode\u0026lt;E\u0026gt; pred, succ; //插入新元素的前后元素引用 if (index == size) { succ = null; pred = last; } else { succ = node(index); pred = succ.prev; } 随后遍历Object数组，逐个生成Node对象，并将上一个元素的next指向当前元素。遍历完后，维护插入新元素的前/后元素的后/前引用，如果succ为null则LinkedList.last赋值为pred，同时维护size，更新modCount。\n删 public boolean remove(Object o); 删除元素的方法remove()类似indexOf()，从first开始遍历List，逐个元素判断是否与参数o相同（判断方法与indexOf()一样），找到后调用unlink()进行删除并返回true。如果没找到相同的元素则返回false。\npublic E remove(int index); 该方法删除指定下标的元素。检查下标没有越界后，调用unlink(node(index))删除指定元素（其中node()是查找指定下标的元素，详见 查 部分）。\npublic void clear(); 清空整个List，从first开始，遍历整个List，把每个Node的item next prev都置为null（方便GC），以及LinkedList的first last也置为null，size归零。\n改 public E set(int index, E element); 将下标为index的元素设置为element，并将旧值返回。该方法在检查index是否越界后，先调用node()方法查找到指定下标的Node元素，直接修改Node实例的item属性为新值，并返回旧值（而不是新建一个Node，这样既减少操作，也减少多余的对象，从而减轻GC负担）。\n旧的查找方法 public boolean contains(Object o); public int indexOf(Object o); public int lastIndexOf(Object o); 方法contains()判断当前List是否包含指定元素，返回indexOf(o) != -1。\n方法indexOf()从first开始通过next引用遍历List，逐个元素判断是否与参数o相同（o为null则用item==null进行判断，o非null则调用o.equals(item)进行判断），与ArrayList的实现类似。\n方法lastIndexOf()的实现与indexOf()类似，只是查找的起点改成last，且遍历的方向相反。\npublic int size(); //返回size属性 Node\u0026lt;E\u0026gt; node(int index); public E get(int index); 前者node()方法是通过遍历从链表中查找到指定下标的元素，其中有个优化，如果查找的下标大于size的一半则从last引用开始找，否则从first引用开始找。\n后者get()方法在检查index参数没有越界后，调用前者进行查找。\nJDK5 开始引入的方法 //获取但不移除该List的头部，first为null时返回null public E peek(); //获取但不移除该List的头部，调用getFirst()，first为null时抛NoSuchElementException异常 public E element(); //获取并移除该List的头部，first为null时返回null，否则调用unlinkFirst()处理 public E poll(); //获取并移除该List的头部，调用removeFirst()，first为null时抛NoSuchElementException异常 public E remove(); //将指定元素添加到该List的尾部，调用add()方法处理 public boolean offer(E e); JDK6 开始引入的方法 //在该List的开头插入指定的元素，调用addFirst()方法，后者又调用linkFirst()处理，返回true public boolean offerFirst(E e); // 在该List末尾插入指定的元素，调用addLast()方法，后者又调用linkLast()处理，返回true。实际实现与offer()一致 public boolean offerLast(E e); // 获取但不移除该List的第一个元素；如果该List为空，则返回 null。实现与peek()一样 public E peekFirst(); //获取但不移除该List的最后一个元素；如果该List为空，则返回 null。实现与peek()类似 public E peekLast(); //获取并移除该List的第一个元素；如果该List为空，则返回 null；实现与poll()一样 public E pollFirst(); //获取并移除该List的最后一个元素；如果该List为空，则返回 null；否则调用unlinkLast()进行处理 public E pollLast(); //将元素推入该List所表示的堆栈。实际调用addFirst()进行处理 public void push(E e); //从该List所表示的堆栈处弹出一个元素。实际调用removeFirst()进行处理 public E pop(); //从该List中移除第一次出现的指定元素（从头部到尾部遍历列表时）。调用remove(o)进行处理 public boolean removeFirstOccurrence(Object o); //从该List中移除最后一次出现的指定元素。从last开始向前遍历，找到与o相同的元素; //再调用unlink()进行删除处理，返回true；遍历完后还找不到对应元素则返回false。 public boolean removeLastOccurrence(Object o); 可以看到 JDK6 新增的这些方法实际上是对 JDK5 新增的方法的扩展，从默认的操作（可能是针对头部或尾部）扩展到对头部和尾部的操作。\n边界检查方法 //是否合法的元素下标 private boolean isElementIndex(int index); //对迭代器和add()方法而言是否合法下标（允许等于size） private boolean isPositionIndex(int index); //生成报错信息字符串 private String outOfBoundsMsg(int index); //检查是否合法的元素下标，不合法则抛IndexOutOfBoundsException异常 private void checkElementIndex(int index); //检查对迭代器和add()方法而言是否合法下标，不合法则抛出IndexOutOfBoundsException异常 private void checkPositionIndex(int index); 其他方法 //先调用父类的clone方法复制（其实是Object的本地方法）， //然后遍历原List，逐个add到克隆后的新List中并返回 public Object clone(); //List转数组 //根据size新建一个Object数组，遍历List，逐个Node的item属性加到Object数组中 public Object[] toArray(); //如果a的长度小于size，需要调用java.lang.reflect.Array.newInstance(方法新建一个size长度的T[]数组 //然后遍历List，将每个Node元素的item属性复制到数组中 //最后将下标为size的元素置为null（标识数组结束， 数组长度大于size才有这个操作）并返回 public \u0026lt;T\u0026gt; T[] toArray(T[] a); //序列化方法 //写入到输出流，先写入size，然后遍历List逐个Node元素的item属性写入输出流 private void writeObject(java.io.ObjectOutputStream s); //读取输入流构造List，先读入一个int作为size， //然后从输入流读取size次，每次读取的对象强转为E类型，并调用linkLast()加入到L链表尾部 private void readObject(java.io.ObjectInputStream s); JDK8 开始引入的方法 //spliterator：返回一个内部类static final class LLSpliterator\u0026lt;E\u0026gt; implements Spliterator\u0026lt;E\u0026gt; 的实例。 //为了并行遍历数据源中的元素，Stream.isParallel()实现调用。 //关于Spliterator接口在ArrayList源码的笔记里有介绍，在此不再赘述 //这个实现类在final int getEst()est = LinkedList的size，expectedModCount = LinkedList的modCount public Spliterator\u0026lt;E\u0026gt; spliterator(); 迭代器 获取迭代器的方法 //返回此列表中的元素的列表迭代器（按适当顺序），从列表中指定位置开始。 //检查下标越界后，返回LinkedList的内部类ListItr的实例 public ListIterator\u0026lt;E\u0026gt; listIterator(int index); //返回以逆向顺序在此双端队列的元素上进行迭代的迭代器。 //返回的是LinkedList的内部类DescendingIterator的实例 public Iterator\u0026lt;E\u0026gt; descendingIterator(); //返回父类AbstractList的内部类ListItr实例 public Iterator\u0026lt;E\u0026gt; AbstractSequentialList.iterator(); 迭代器ListItr 包含以下成员变量：\n//next()，previous()，remove()，set()，add()等方法返回的Node元素记录 private Node\u0026lt;E\u0026gt; lastReturned; //当前游标指向的Node元素 private Node\u0026lt;E\u0026gt; next; //当前游标的下标值 private int nextIndex; //记录应有的modCount，迭代器对链表进行修改后会更新modCount， //而迭代器以外对链表的操作不会更新这个属性， //因此外部操作后再进行迭代器的操作会抛出ConcurrentModificationException异常 private int expectedModCount = modCount; 其余ListIterator接口方法的具体实现在此不表，与ArrayList的ListItr的实现类似，只是在一些修改链表的方法里需要对应修改前后元素的引用而已，此外上一个下一个的迭代效率也较高（直接读取Node元素的prev/next引用）。\n迭代器DescendingIterator 该迭代器是为了实现java.util.Deque接口的Iterator\u0026lt;E\u0026gt; descendingIterator()方法。\n具体实现还是用前面介绍的LinkedList的内部类ListItr。由于是反向的迭代器，因此用私有变量\nprivate final ListItr itr = new ListItr(size()); 实现从链表尾部开始迭代。\n从尾部迭代到头部的迭代方式（反向迭代）从其他几个方法的具体实现也可以看出来，比较简单，只贴出代码就算了：\npublic boolean hasNext() { return itr.hasPrevious(); } public E next() { return itr.previous(); } public void remove() { itr.remove(); } 可以看出并没有去重写 JDK8 的\ndefault void forEachRemaining(Consumer\u0026lt;? super E\u0026gt; action) 方法。\n","date":"2017-07-31T14:35:06+08:00","image":"https://leibnizhu.github.io/p/LinkedList%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0JDK8/night_hu8536bef82175abe0e2e5afa324bef947_75256_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/LinkedList%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0JDK8/","title":"LinkedList源码阅读笔记（JDK8）"},{"content":"SpringMVC中的WebSocket开发 WebSocket简介 WebSocket背景 在WebSocket出现之前，服务器的状态更新想要通知客户端，只能由客户端发起轮询（如Ajax）， 即在特定的的时间间隔（如每1秒），由浏览器对服务器发出HTTP request，然后由服务器返回最新的数据给客服端的浏览器。这种传统的HTTP request 的模式带来很明显的缺点 – 浏览器需要不断的向服务器发出请求，然而HTTP request 的header是非常长的，里面包含的有用数据可能只是一个很小的值，这样会占用很多的带宽。 WebSocket是HTML5开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。WebSocket通讯协议于2011年被IETF定为标准RFC 6455，WebSocketAPI被W3C定为标准。 在WebSocket API中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。WebSocket不仅允许服务器和客户端双向通信，而且互相沟通的Header是很小的-大概只有 2 Bytes。\n支持情况 Spring： Spring从4.0开始加入了spring-websocket这个模块，并能够全面支持WebSocket，它与Java WebSocket API标准（JSR-356）保持一致，同时提供了额外的服务。 浏览器： 浏览器 支持的版本 Chrome 4+ Firefox 4+ Internet Explorer 10+ Opera 10+ Safari 5+ 服务端： 服务器 支持的版本 jetty 7.0.1+ tomcat 7.0.27+ Nginx 1.3.13+ resin 4+ Java 实现方法 在 Spring 端可以有以下几种方法使用 WebSocket：\n使用 Java EE7 的方式 使用 Spring 提供的接口 使用 STOMP 协议以及 Spring 的 MVC 本文使用Spring提供的接口，实现起来比较简单。\n适用场景 客户端和服务器需要 高频率 低延迟 交换事件的时候。基本的候选包括但不限于，金融、游戏、合作、以及其他应用。这些应用对时间延迟很敏感，还需要以高频率交换大量的消息。\nSpring MVC的WebSocket开发实战 Nginx配置 我们知道，WebSocket握手需要在HTTP请求头里增加Upgrade和Connection字段，以便向服务申请将连接升级为WebSocket。\n但如果tomcat服务器使用了Nginx作为反向代理，那么默认是不会转发这两个请求头的，所以需要手动设置这两个HTTP请求头。\n应在nginx.conf对应域名server配置里面的location配置中增加：\nproxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; Maven依赖 在pom.xml文件中增加以下依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-websocket\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; WebSocket相关的类 实现WebSocketConfigurer 对Spring WebSocket进行配置，可以通过xml配置文件的方式，也可以通过实现WebSocketConfigurer接口进行配置：\n@Configuration @EnableWebSocket public class WebSocketConfig extends WebMvcConfigurerAdapter implements WebSocketConfigurer { @Override public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) { registry.addHandler(SystemWebSocketHandler.getInstance(),\u0026#34;/webSocketServer\u0026#34;) //注册WebSocket处理的类的、及监听/映射路径 .addInterceptors(new WebSocketHandshakeInterceptor()); //注册WebSocket握手的拦截器 registry.addHandler(SystemWebSocketHandler.getInstance(), \u0026#34;/sockjs/webSocketServer\u0026#34;) //注册WebSocket处理的类的、及监听/映射路径 .addInterceptors(new WebSocketHandshakeInterceptor()) //注册WebSocket握手的拦截器 .withSockJS(); //设定支持SockJS } } 我们设置了两个监听的路径，第一个是传统的WebSocket，第二个是支持SockJS的。SockJS是一个JavaScript库，提供跨浏览器JavaScript的API，创建了一个低延迟、全双工的浏览器和web服务器之间通信通道。SockJS的API的命名方式基本上也和 WebSocket 一样，并且支持自动降级到AJAX轮询（降级顺序依次为：websocket -\u0026gt; html strea m -\u0026gt; long polling -\u0026gt; ajaxjsonp），因此可以很好地跨浏览器工作。 在配置文件里，我们设定了SystemWebSocketHandler类（实现WebSocketHandler接口，类似Controller）作为WebSocket各种事件的处理器，以及设定WebSocketHandshakeInterceptor类（实现HandshakeInterceptor接口）作为WebSocket协议握手的拦截器，这两个类时我们自己实现的，将在下文细述。\n实现WebSocketHandler接口 WebSocketHandler接口为WebSocket事件处理器接口，有以下方法需要实现：\npublic interface WebSocketHandler { //WebSocket连接建立后的回调方法 void afterConnectionEstablished(WebSocketSession var1) throws Exception; //接收到WebSocket消息后的处理方法 void handleMessage(WebSocketSession var1, WebSocketMessage\u0026lt;?\u0026gt; var2) throws Exception; //WebSocket传输发生错误时的处理方法 void handleTransportError(WebSocketSession var1, Throwable var2) throws Exception; //WebSocket连接关闭后的回调方法 void afterConnectionClosed(WebSocketSession var1, CloseStatus var2) throws Exception; //是否处理WebSocket分段消息 boolean supportsPartialMessages(); } 对于业务逻辑而言，我们主要关注afterConnectionEstablished()方法（进行一些初始化工作），以及handleMessage()方法（处理页面发出的消息）。其余方法的实现内容相对固定，发生错误和连接关闭应该响应地关闭一些资源，至于分段消息，暂时用不到，可以直接返回false。 下面给出一个简单的实现：\n/** * 实现WebSocketHandler接口,作为WebSocket各种事件的处理器 * * @author leibniz */ public class SystemWebSocketHandler implements WebSocketHandler { private static Logger LOG; /** * 维护所有已创建的WebSocket Session，key为用户ID（OpenID或管理员的名字） * 考虑到一个用户可能打开多个页面（如管理员可能在手机和PC登录，且多个人用同一个账号），这里使用Guava的Multimap来缓存 */ private static Multimap\u0026lt;String, WebSocketSession\u0026gt; WS_SESSION_MAP; /** * 单例的对象 */ private static final SystemWebSocketHandler INSTANCE = new SystemWebSocketHandler(); public static SystemWebSocketHandler getInstance(){ return INSTANCE; } /** * WebSocketSession中保存当前用户ID的Attribute key */ static final String WEBSOCKET_USERID = \u0026#34;WS_USERID\u0026#34;; /** * 默认构造器，初始化日志对象和Session缓存Map */ private SystemWebSocketHandler(){ LOG = LoggerFactory.getLogger(SystemWebSocketHandler.class); WS_SESSION_MAP = HashMultimap.create(); } /** * 建立连接后，将用户ID和WebSocketSession对象的映射保存到WS_SESSION_MAP * * @param session WebSocketSession 对象 * @throws Exception 接口方法声明的异常 * * @author Leibniz */ @Override public void afterConnectionEstablished(WebSocketSession session) throws Exception { LOG.debug(\u0026#34;connect to the websocket success......\u0026#34;); String userId = (String) session.getAttributes().get(WEBSOCKET_USERID); WS_SESSION_MAP.put(userId, session); } /** * 接收到WebSocket消息后的处理方法vvvvv * 暂不处理 * * @param webSocketSession WebSocketSession对象 * @param webSocketMessage 页面发送的WebSocketMessage消息对象 * @throws Exception 接口方法声明的异常 * @author Leibniz */ @Override public void handleMessage(WebSocketSession webSocketSession, WebSocketMessage\u0026lt;?\u0026gt; webSocketMessage) throws Exception { } /** * WebSocket传输发生错误时，关闭WebSocketSession，并从WS_SESSION_MAP中删除 * * @param session WebSocketSession对象 * @param exception 异常 * @throws Exception 接口方法声明的异常 * * @author Leibniz */ @Override public void handleTransportError(WebSocketSession session, Throwable exception) throws Exception { if(session.isOpen()){ session.close(); } LOG.debug(\u0026#34;websocket connection closed......\u0026#34;); for(Map.Entry\u0026lt;String, WebSocketSession\u0026gt; entry : WS_SESSION_MAP.entries()){ if(entry.getValue().equals(session)){ WS_SESSION_MAP.remove(entry.getKey(), session); } } } /** * WebSocket连接关闭后，从WS_SESSION_MAP中删除对应WebSocketSession * * @param session WebSocketSession对象 * @param closeStatus 关闭状态 * @throws Exception 接口方法声明的异常 * * @author Leibniz */ @Override public void afterConnectionClosed(WebSocketSession session, CloseStatus closeStatus) throws Exception { LOG.debug(\u0026#34;websocket connection closed......\u0026#34;); for(Map.Entry\u0026lt;String, WebSocketSession\u0026gt; entry : WS_SESSION_MAP.entries()){ if(entry.getValue().equals(session)){ WS_SESSION_MAP.remove(entry.getKey(), session); } } } /** * 不支持分段消息 * @return false */ @Override public boolean supportsPartialMessages() { return false; } /** * 给所有在线用户发送消息 * * @param message 需要发送的消息对象 * * @author Leibniz */ public void sendMessageToUsers(TextMessage message) { for (WebSocketSession user : WS_SESSION_MAP.values()) { try { if (user.isOpen()) { user.sendMessage(message); } } catch (IOException e) { e.printStackTrace(); } } } /** * 给某个用户发送消息 * * @param userId 用户ID（OpenID或管理员的名字） * @param message 需要发送的消息对象 * * @author Leibniz */ public void sendMessageToUser(String userId, TextMessage message) { Collection\u0026lt;WebSocketSession\u0026gt; users = WS_SESSION_MAP.get(userId); try { for(WebSocketSession user : users){ if (user != null \u0026amp;\u0026amp; user.isOpen()) { user.sendMessage(message); } } } catch (IOException e) { e.printStackTrace(); } } } 值得注意的是，使用WebSocketSession.sendMessage()方法可以向指定用户页面发送消息。\n实现HandshakeInterceptor接口 HandshakeInterceptor接口为WebSocket握手拦截器接口，包含以下方法：\npublic interface HandshakeInterceptor { //建立WebSocket连接、握手前的处理方法 boolean beforeHandshake(ServerHttpRequest request, ServerHttpResponse response, WebSocketHandler wsHandler, Map\u0026lt;String, Object\u0026gt; attributes) throws Exception; //建立WebSocket连接、握手后的处理方法 void afterHandshake(ServerHttpRequest request, ServerHttpResponse response, WebSocketHandler wsHandler, Exception exception); } 在前面的WebSocket配置类里面，这个接口的实现类用于拦截WebSocket连接握手，在握手前后都可以拦截。 我们的应用里用到这个握手拦截器，主要是因为在WebSocketHandler接口的方法中，只能拿到WebSocketSession对象，无法直接与用户请求的HttpSession建立关联。 而在握手拦截器中，通过ServerHttpRequest对象可以拿到关于当前用户、当前连接的很多相关信息，包括HttpSession及其属性；同时通过attributes参数可以设置最终生成的WebSocketSession对象的属性；从而WebSocketSession和HttpSession就可以建立起关联。 从一个简单的实现类中就可以清晰看到这一点：\npublic class WebSocketHandshakeInterceptor implements HandshakeInterceptor { private static Logger LOG = LoggerFactory.getLogger(HandshakeInterceptor.class); /** * 建立WebSocket连接、握手前的处理方 * 从HttpSession读取当前用户的用户ID（OpenID或管理员的名字），写入attributes * * @param request Http请求对象 * @param response Http响应对象 * @param wsHandler WebSocketHandler实现类的实例，这里是SystemWebSocketHandler类 * @param attributes 握手生成的WebSocketSession对象的属性 * @return 是否成功 * @throws Exception 接口方法声明的异常 * * @author Leibniz */ @Override public boolean beforeHandshake(ServerHttpRequest request, ServerHttpResponse response, WebSocketHandler wsHandler, Map\u0026lt;String, Object\u0026gt; attributes) throws Exception { if (request instanceof ServletServerHttpRequest) { ServletServerHttpRequest servletRequest = (ServletServerHttpRequest) request; HttpSession session = servletRequest.getServletRequest().getSession(false); if (session != null) { //使用userName区分WebSocketHandler，以便定向发送消息 String userId = (String) session.getAttribute(RegisterLoginController.OPENID_KEY);//普通用户是OpenID String adminUserId = (String) session.getAttribute(AdminUserController.USER_NAME);//管理员用户是用户名 if( null != adminUserId){ //优先保存管理员的用户ID1 attributes.put(WEBSOCKET_USERID, adminUserId); } else { attributes.put(WEBSOCKET_USERID, userId); } } } return true; } /** * 建立WebSocket连接、握手后的处理方法 * * @param request Http请求对象 * @param response Http响应对象 * @param wsHandler WebSocketHandler实现类的实例，这里是SystemWebSocketHandler类 * @param exception 抛出的异常 * * @author Leibniz */ @Override public void afterHandshake(ServerHttpRequest request, ServerHttpResponse response, WebSocketHandler wsHandler, Exception exception) { } } 业务层的封装 实际的使用中，我们封装了一些类，包括WebSocket消息内容的实体类，以及发送消息的Service类（在Controller层触发了相应的事件时进行调用），以下代码仅供参考，请根据实际业务需求进行封装。\n/** * WebSocket消息的统一封装 * * @author Leibniz */ public class WebSocketMessage { private ROLE role;//接受消息的角色，枚举，NORMAL=普通用户，ADMIN=客服/管理员 private String id;//用户ID（OpenID或管理员的名字） private String event;//事件，实际上是数字，10001=客服确认借出，10002=客服确认归还，20001=用户申请租赁，20002=用户申请归还 private String msg;//事件消息，具体的文字描述，英文 public WebSocketMessage(ROLE role, String id, String event, String msg) { this.role = role; this.id = id; this.event = event; this.msg = msg; } @Override public String toString() { JSONObject json = new JSONObject(); json.put(\u0026#34;role\u0026#34;, this.role.toString()); json.put(\u0026#34;id\u0026#34;, this.id); json.put(\u0026#34;event\u0026#34;, this.event); json.put(\u0026#34;msg\u0026#34;, this.msg); return json.toString(); } enum ROLE { NORMAL { @Override public String toString() { return \u0026#34;normal\u0026#34;; } }, ADMIN { @Override public String toString() { return \u0026#34;admin\u0026#34;; } } } } /** * 发送WebSocket消息的Service类 * * @author Leibniz */ @Service public class WebSocketMessageService { @Autowired private AdminUserDao adminUserDao; private List\u0026lt;AdminUserEntity\u0026gt; adminUserList; /** * 向普通用户发送客服确认借出消息 * * @param openId 用户OpenID * @author Leibniz */ public void sendBorrowConfirm(String openId){ WebSocketMessage msg = new WebSocketMessage(NORMAL, openId, \u0026#34;10001\u0026#34;, \u0026#34;admin_borrow_confirm\u0026#34;); SystemWebSocketHandler.getInstance().sendMessageToUser(openId, new TextMessage(msg.toString())); } /** * 向普通用户发送客服确认归还消息 * * @param openId 用户OpenID * @author Leibniz */ public void sendReturnConfirm(String openId){ WebSocketMessage msg = new WebSocketMessage(NORMAL, openId, \u0026#34;10002\u0026#34;, \u0026#34;admin_return_confirm\u0026#34;); SystemWebSocketHandler.getInstance().sendMessageToUser(openId, new TextMessage(msg.toString())); } /** * 向超级管理员及指定柜子对应的客服发送用户申请租赁的消息 * * @param boxId 柜子ID * @author Leibniz */ public void sendBorrowApply(int boxId){ //检查管理员列表是否已加载 if(null == adminUserList){ adminUserList = adminUserDao.selectAllAdminUser(); } for(AdminUserEntity adminUser : adminUserList) { if(adminUser.getBoxId()== null || adminUser.getBoxId().equals(boxId)){ //当前遍历到的是超级管理员或指定柜子对应的客服 String adminName = adminUser.getUserName(); WebSocketMessage msg = new WebSocketMessage(ADMIN, adminName, \u0026#34;20001\u0026#34;, \u0026#34;user_borrow_apply\u0026#34;); SystemWebSocketHandler.getInstance().sendMessageToUser(adminName, new TextMessage(msg.toString())); } } } /** * 向超级管理员及指定柜子对应的客服发送用户申请归还的消息 * * @param boxId 柜子ID * @author Leibniz */ public void sendReturnApply(Integer boxId){ //检查管理员列表是否已加载 if(null == adminUserList){ adminUserList = adminUserDao.selectAllAdminUser(); } for(AdminUserEntity adminUser : adminUserList) { if(adminUser.getBoxId() == null || adminUser.getBoxId().equals(boxId)) { //当前遍历到的是超级管理员或指定柜子对应的客服 String adminName = adminUser.getUserName(); WebSocketMessage msg = new WebSocketMessage(ADMIN, adminName, \u0026#34;20002\u0026#34;, \u0026#34;user_return_apply\u0026#34;); SystemWebSocketHandler.getInstance().sendMessageToUser(adminName, new TextMessage(msg.toString())); } } } } 页面\u0026amp;\u0026amp;js 页面需要引入SockJS，js中需要初始化WebSocket并建立链接（前面在WebSocketConfig类中配置的映射路径）。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html xmlns=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34; xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;msgcount\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;../js/libs/jquery-2.0.2.min.js\u0026#34; th:src=\u0026#34;@{/js/libs/jquery-2.0.2.min.js}\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;../js/sockjs.min.js\u0026#34; th:src=\u0026#34;@{/js/sockjs.min.js}\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; if(typeof $basePath === \u0026#34;undefined\u0026#34;){ window.$basePath = \u0026#34;/breo/\u0026#34;; } var websocket; //根据当前浏览器支持的WebSocket对象类型进行初始化 if (\u0026#39;WebSocket\u0026#39; in window) { //浏览器内置WebSocket API websocket = new WebSocket(\u0026#34;ws://\u0026#34;+window.location.host+$basePath+\u0026#34;webSocketServer\u0026#34;); } else if (\u0026#39;MozWebSocket\u0026#39; in window) { //Firefox浏览器 websocket = new MozWebSocket(\u0026#34;ws://\u0026#34;+window.location.host+$basePath+\u0026#34;webSocketServer\u0026#34;); } else { //其他浏览器，或不支持WebSocket websocket = new SockJS(\u0026#34;http://\u0026#34;+window.location.host+$basePath+\u0026#34;sockjs/webSocketServer\u0026#34;); } //WebSocket连接打开的回调方法 websocket.onopen = function (evnt) { }; //页面接收到WebSocket消息的回调方法 websocket.onmessage = function (evnt) { var msg = JSON.parse(evnt.data); console.log(msg); if(msg.role === \u0026#34;normal\u0026#34; \u0026amp;\u0026amp; msg.event === \u0026#34;10001\u0026#34;){ $(\u0026#34;#msgcount\u0026#34;).html(\u0026#34;\u0026lt;font color=\u0026#39;red\u0026#39;\u0026gt;\u0026#34;+JSON.stringify(msg)+\u0026#34;\u0026lt;/font\u0026gt;\u0026#34;) } }; //WebSocket发生错误的回调方法 websocket.onerror = function (evnt) { }; //WebSocket连接关闭的回调方法 websocket.onclose = function (evnt) { } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","date":"2017-07-29T15:26:32+08:00","image":"https://leibnizhu.github.io/p/SpringMVC%E4%B8%AD%E7%9A%84WebSocket%E5%BC%80%E5%8F%91/snow_hu68df5ce305526b4a5a8177697281f671_85226_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/SpringMVC%E4%B8%AD%E7%9A%84WebSocket%E5%BC%80%E5%8F%91/","title":"SpringMVC中的WebSocket开发"},{"content":"ArrayList源码阅读笔记 继承结构 成员变量 transient Object[] elementData; 存储List数据，用了transient关键字避免被持久化。\nprivate int size; List长度。 在父类AbstractList中modCount成员变量：\nprivate static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; 有些JVM会在数组保留一些头部信息，为了防止报内存不足Error，在int最大值的基础上预留8个元素。\nprotected transient int modCount = 0; 表示已从结构上修改此列表的次数。从结构上修改是指更改列表的大小，或者打乱列表，从而使正在进行的迭代产生错误的结果。此字段由iterator和listiterator方法返回的迭代器和列表迭代器实现使用。如果意外更改了此字段中的值，则迭代器（或列表迭代器）将抛出ConcurrentModificationException来响应next、remove、previous、set或add操作。子类是否使用此字段是可选的。如果子类希望提供快速失败迭代器（和列表迭代器），则它只需在其 add(int,e)和remove(int)方法（以及它所重写的、导致列表结构上修改的任何其他方法）中增加此字段，否则可以忽略此字段。对add(int, e)或remove(int)的单个调用向此字段添加的数量不得超过 1，否则迭代器（和列表迭代器）将抛出虚假的 ConcurrentModificationException。\n构造方法 ArrayList(int initialCapacity) initialCapacity大于0的话elementData赋值为对应长度的Object数组；为0的话赋值为EMPTY_ELEMENTDATA（空数组）；负数的情况抛IllegalArgumentException异常。\nArrayList() 默认构造方法，elementData赋值为DEFAULTCAPACITY_EMPTY_ELEMENTDATA（空数组）。\nArrayList(Collection\u0026lt;? extends E\u0026gt; c) c先转成数组赋值给elementData，如果长度不为0且数组不为Object数组，则通过 Arrays.copyOf()转换为Object数组；如果长度为0则c先转成数组赋值给elementData，如果长度不为0且数组不为Object数组，则通过重新赋值为EMPTY_ELEMENTDATA（空数组）。\n常规方法 void trimToSize(); modCount++后，调用Arrays.copyOf()将elementData复制到size大小的数组里并赋值给elementData。这样是因为容量常常会大于实际元素的数量。内存紧张时，可以调用该方法删除预留的位置，调整容量为元素实际数量。\nvoid ensureCapacity(int minCapacity); 如果数组为空，容量预取0，否则去默认值(DEFAULT_CAPACITY = 10)；若参数大于预设的容量，则使用该参数调用私有方法void ensureCapacityInternal(int minCapacity)设置数组容量；后者会在当前数组长度不足时调用grow()进行扩容。\nprivate void grow(int minCapacity); 容量为：当前容量增加一半、及minCapacity中的较大者。如果超过数组最大长度则调用hugeCapacity(int minCapacity)处理，抛出错误。最后调用Arrays.copyOf()复制旧数据到新数组并赋值给element。\npublic int size(); //返回size属性 public boolean isEmpty() ; //size是否为0 public boolean contains(Object o); //调用indexOf()是否大于等于0。 public int indexOf(Object o); //下面详解 public int lastIndexOf(Object o);// indexOf()查找元素索引，遍历数组逐个对比（如果o为null则直接判断== null，否则调用o.equals()进行判断），返回第一次找到的索引值，找不到则返回-1。lastIndexOf()与之类似，只是遍历的方向相反。\npublic Object clone(); 调用Arrays.copyOf()进行浅拷贝，每个元素只拷贝引用。\npublic Object[] toArray(); public \u0026lt;T\u0026gt; T[] toArray(T[] a); 前者转换为Object数组，调用Arrays.copyOf()方法。\n后者转换为指定类型的数组，返回的数组容量由参数和本数组中较大值确定；如果a参数的长度小于当前size，则调用带Class参数的Arrays.copyOf()方法进行复制并返回；否则a的长度足够，则调用System.arraycopy()将elementData复制到a，并将第一个实际没有数据的设为null（a[size]）。\n增删改查方法 public E get(int index); 调用rangeCheck(index);检查下标是否越界后（越界则抛出IndexOutOfBoundsException异常），直接从elementData数组根据下标拿值，速度比较快。\npublic E set(int index, E element); 调用rangeCheck(index);检查下标是否越界后，读取索引对应的旧值，然后改为新值，并将旧的值返回；注意用到泛型。\npublic boolean add(E e); public void add(int index, E element); 前者调用ensureCapacityInternal(size + 1)确认是否需要扩容后，直接设定数组对应下标的值。\n后者先调用rangeCheckForAdd()确认index参数是否越界，然后同样调用ensureCapacityInternal(size+1)确认是否需要扩容，将index之后的元素通过System.arraycopy()方法复制到index+1位置之后，最后将element赋值到下标为index处。\nP.S.： System.arraycopy()的参数：src:源数组； srcPos:源数组要复制的起始位置； dest:目的数组； destPos:目的数组放置的起始位置； length:复制的长度。\npublic boolean addAll(Collection\u0026lt;? extends E\u0026gt; c); public boolean addAll(int index, Collection\u0026lt;? extends E\u0026gt; c); 前者先调用c.toArray()将参数c转换为数组，然后进行扩容的判断，接着调用System.arraycopy()将数组的内容复制到elementData后面，并修改size的值，最后c的长度不为0则返回true，为零则返回false。\n后者先判断index是否越界及是否需要扩容，然后计算需要移动的元素个数并调用System.arraycopy()移动index以后的元素，接着调用System.arraycopy()将c转换的数组复制到index后面，最后修改size的值，根据c的长度返回true/false。\npublic E remove(int index); public boolean remove(Object o); protected void removeRange(int fromIndex, int toIndex); 前者按下标进行删除，先检查下标越界后，读取index对应的旧值，计算index下标往后的（需要移动的）元素个数，调用System.arraycopy()进行移动，随后将原来最后一位改为null（方便GC回收），最后将旧值返回。\n中者基本逻辑与前者类似，只是移动数组元素的范围不一样，而且需要遍历将数组尾部多个元素（toIndex - fromIndex个）赋值为null。\n后者删除指定对象，遍历数组，找到与o相等的元素（o为null则判断==null，否则调用o.equals()判断相等），调用fastRemove()删除，并返回true；如果找不到对应相等的元素，则返回false。private void fastRemove(int index)方法的实现与E remove(int index);基本一直，区别嘛，方法如其名，快就快在不需要进行越界判定，及不需要返回旧值，直接删除。\npublic void clear(); 清空List，遍历数组将每个元素赋值为null，最后将size设为0.\npublic boolean removeAll(Collection\u0026lt;?\u0026gt; c); //batchRemove(c, false); public boolean retainAll(Collection\u0026lt;?\u0026gt; c); //batchRemove(c, true); /** * @param complement true时从数组保留指定集合中元素的值，为false时从数组删除指定集合中元素的值。 * @return 数组中重复的元素都会被删除(而不是仅删除一次或几次)，有任何删除操作都会返回true */ private boolean batchRemove(Collection\u0026lt;?\u0026gt; c, boolean complement) { final Object[] elementData = this.elementData; int r = 0, w = 0; boolean modified = false; try { //遍历数组，并检查这个集合是否包含对应的值，移动要保留的值到数组前面，w最后值为要保留的元素的数量 //简单点：若保留，就将相同元素移动到前段；若删除，就将不同元素移动到前段 for (; r \u0026lt; size; r++) if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; }finally { //确保异常抛出前的部分可以完成期望的操作，而未被遍历的部分会被接到后面 //r!=size表示可能出错了：c.contains(elementData[r])抛出异常 if (r != size) { System.arraycopy(elementData, r,elementData, w,size - r); w += size - r; } //如果w==size：表示全部元素都保留了，所以也就没有删除操作发生，所以会返回false；反之，返回true，并更改数组 //而w!=size的时候，即使try块抛出异常，也能正确处理异常抛出前的操作，因为w始终为要保留的前段部分的长度，数组也不会因此乱序 if (w != size) { for (int i = w; i \u0026lt; size; i++) elementData[i] = null; modCount += size - w;//改变的次数 size = w; //新的大小为保留的元素的个数 modified = true; } } return modified; } 前两个方法调用后面的batchRemove()方法（具体看上面的注释），第一个方法删除c里面的元素，第二个方法保留c里面的元素，有任何删除操作都会返回true，返回false表示没有删除任一元素。不难看出batchRemove()方法的时间复杂度是O(n^2)，大致逻辑是遍历数组，根据complement条件判断要保留的元素依次复制到数组的前面，w保存要保留的最大下标，如果期间发生异常则将未处理的元素都保留，遍历后将w之后的元素设置为null等待GC。\nIO方法 private void writeObject(java.io.ObjectOutputStream s); private void readObject(java.io.ObjectInputStream s); 前者将List序列化到输出流，先写入size，再逐个元素写入，写入过程数组被更改会抛出ConcurrentModificationException异常。\n后者读取输入流反序列化到List，先从输入流读取一个int，判断这个int作为size的话是否需要扩容，然后从输入流中读取Object从下标0开始逐个写入到elementData数组中（就是说读取的数量小于原来size的话，读取最后的下标之后的元素不会变动）。\n迭代器方法 public ListIterator\u0026lt;E\u0026gt; listIterator(int index); //new ListItr(index);开始位置为指定参数 public ListIterator\u0026lt;E\u0026gt; listIterator(); //return new ListItr(0);开始位置为0 public Iterator\u0026lt;E\u0026gt; iterator(); //return new Itr(); 迭代器方法分别返回ListItr和Itr内部类的实例，而ListItr继承了Itr。\n迭代器类Itr private class Itr implements Iterator\u0026lt;E\u0026gt; {} 成员变量 int cursor; //游标，下一个元素的索引，默认初始化为0 int lastRet = -1; //上次返回的元素的位置 int expectedModCount = modCount;//迭代过程不运行修改数组，否则就抛出异常 Iterator接口方法 public boolean hasNext(); } 返回cursor != size;，如果游标到了size那也就是没有下一个了。\npublic E next(); 先调用checkForComodification()检查是否被修改过，然后判断如果游标已经越界则抛出NoSuchElementException异常，否则lastRet改为游标值，游标前进，返回原来游标指向的元素。\npublic void remove(); lastRet \u0026lt; 0时抛IllegalStateException异常，然后检查是否被修改过，随后调用ArrayList.remove(int index)移除元素，游标后退，lastRet重设为-1，expectedModCount赋值为新的modCount。\n基本上ArrayList采用size来维护自已状态，而Iterator采用cursor维护自已状态。当size出现变化时，cursor并不一定能够得到同步，除非这种变化是Iterator主动导致的，比如调用remove()方法导致ArrayList列表发生变化时，迭代器会更新cursor来同步这一变化，但其他方式导致的ArrayList变化Iterator无法感知，ArrayList也不会主动通知Iterator们。Iterator为了防止状态不一致可能引发的后果经常做checkForComodification检查，以防有变。如果有变，则以异常抛出。\n所以在循环里删除元素的话要用迭代器的remove()方法。\npublic void forEachRemaining(Consumer\u0026lt;? super E\u0026gt; consumer); JDK8开始有的方法，遍历数组，分别传入调用consumer.accept((E) elementData[i++]);，即对每个元素执行同一方法。\n其他方法 final void checkForComodification(); modCount != expectedModCount则抛出ConcurrentModificationException异常。\n迭代器类ListItr private class ListItr extends Itr implements ListIterator\u0026lt;E\u0026gt; {} 构造器 ListItr(int index); 将游标设置为index。\n接口方法 接口 Iterator的方法在Itr中实现了，该类实现了ListIterator接口中其他的方法。\npublic boolean hasPrevious() { return cursor != 0; //根据游标判断是否有前一个元素 } public int nextIndex() { return cursor; //游标即为下一个要迭代到的元素的下标 } public int previousIndex() { return cursor - 1;//游标减一即上一个元素（当前已迭代的）的下标 } 这三个方法比较简单就不具体讲了。\npublic E previous(); 返回上一个元素同时游标回退，与next()方法类似，先检查是否被修改以及是否下标越界，然后lastRet和游标减一，返回索引减一之后对应的元素。\npublic void set(E e); 用指定元素替换 next 或 previous 返回的最后一个元素。先判定lastRet不小于0，否则抛出IllegalStateException异常。lastRet初始化为-1，之后只有调用ListIterator.add(E e)和remove()之后lastRet会恢复到-1。因为修改的是下标=lastRet的元素，所以不允许为-1，也就是说，只有在最后一次调用 next 或 previous 后既没有调用 ListIterator.remove 也没有调用 ListIterator.add 时才可以进行该调用。\npublic void add(E e); 将指定的元素插入列表。在当前游标处调用List.add()方法插入一个元素e，然后游标前进，lastRet恢复-1，同时更新expectedModCount。\n新元素被插入到游标前：不影响对 next 的后续调用，并且对 previous 的后续调用会返回此新元素。\nsubList()方法与SubList内部类 //检查下标越界后，返回SubList的实例（用fromIndex、toIndex参数构造）作为子列表。 public List\u0026lt;E\u0026gt; subList(int fromIndex, int toIndex); //用于subList()方法的下标越界检查方法 static void subListRangeCheck(int fromIndex, int toIndex, int size); //SubList类 private class SubList extends AbstractList\u0026lt;E\u0026gt; implements RandomAccess //注意在AbstractList.java源文件里也有一个SubList，然而并不是AbstractList的内部类，AbstractList.subList()方法会返回这个SubList的实例 class SubList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; {} SubList继承AbstractList抽象类，是List的实现类，主要用于返回ArrayList的视图，这个视图是原ArrayList对象中的一部分，确实是一部分，直接将原ArrayList对象引用到新的子视图的ArrayList，对子视图进行改变，原ArrayList对象也会随之改变。\n成员变量 private final AbstractList\u0026lt;E\u0026gt; parent; //保存母ArrayList的引用 private final int parentOffset; //subList方法的fromIndex参数 private final int offset;//subList方法的fromIndex参数 int size;//subList方法的参数相减：toIndex - fromIndex 同时，modCount赋值为母ArrayList的modCount。\n接口方法 public E set(int index, E e); public E get(int index); public int size(); public void add(int index, E e); public E remove(int index); protected void removeRange(int fromIndex, int toIndex); public boolean addAll(Collection\u0026lt;? extends E\u0026gt; c) ; public boolean addAll(int index, Collection\u0026lt;? extends E\u0026gt; c); public Spliterator\u0026lt;E\u0026gt; spliterator(); 这些方法的实现与ArrayList的实现都大同小异，差异在与下标多了偏移量parentOffset，而且最后调用parent（ArrayList）的同名方法进行处理。\npublic List\u0026lt;E\u0026gt; subList(int fromIndex, int toIndex); 返回自己的类（SubList），构造方法的offset参数为offset（母ArrayList的subList方法的fromIndex参数）。\n迭代器方法 public Iterator\u0026lt;E\u0026gt; iterator(); public ListIterator\u0026lt;E\u0026gt; listIterator(final int index); 前者返回的是AbstractList的内部类ListItr，实现了ListIterator接口，具体不表。\n后者返回ListIterator接口的一个匿名内部类，与ArrayList的ListItr的区别也是在于对ArrayList的elementData数组进行操作时，使用的下标加上offset（母ArrayList的subList方法的fromIndex参数）。\n其他方法（检查方法） private void rangeCheck(int index); private void rangeCheckForAdd(int index); private String outOfBoundsMsg(int index); private void checkForComodification(); 检查下标越界、并行修改等，与ArrayList类似，具体不表。\nJDK8新增方法 //forEach：for循环遍历elementData，每个元素分别调用action.accept(elementData[i])方法进行处理，如果处理过程中ArrayList被并行修改了，那么抛出ConcurrentModificationException异常 public void forEach(Consumer\u0026lt;? super E\u0026gt; action); //spliterator：返回一个内部类 static final class ArrayListSpliterator\u0026lt;E\u0026gt; implements Spliterator\u0026lt;E\u0026gt;的实例。 //为了并行遍历数据源中的元素，Stream.isParallel()实现调用。下面专门一小节讲Spliterator接口 //这个实现类在private int getFence()方法里面初始化了fence = ArrayList的size，expectedModCount = ArrayList的modCount public Spliterator\u0026lt;E\u0026gt; spliterator(); //删除指定条件的元素。先遍历数组，将符合条件（filter.test(element) == true）的元素的下标设置到一个BitSet的对应下标元素中，即这个BitSet哪一位为true，对应下标在ArrayList的元素就符合filter规则，并使用removeCount记录满足规则的个数。 //遍历后如果有满足filter规则的元素，则遍历数组，将被删除的元素后面的元素往前移，最后将末尾无效数据设为null。 //若有删除的元素则返回true，否则返回false public boolean removeIf(Predicate\u0026lt;? super E\u0026gt; filter); //与forEach()类似，检查operator不为空，然后遍历数组，每个元素调用operator.apply((E) elementData[i])进行处理，最后判断是否发生了并发修改，并增加modCount。 public void replaceAll(UnaryOperator\u0026lt;E\u0026gt; operator); //调用Arrays.sort(T[] a, int fromIndex, int toIndex, Comparator\u0026lt;? super T\u0026gt; c)进行排序，最后判断是否发生了并发修改，并增加modCount。 public void sort(Comparator\u0026lt;? super E\u0026gt; c); Spliterator接口提供的方法 boolean tryAdvance(Consumer\u0026lt;? super T\u0026gt; action); Spliterator\u0026lt;T\u0026gt; trySplit(); long estimateSize(); int characteristics(); tryAdvance()就是顺序处理每个元素，类似Iterator，如果还有元素要处理，则返回true，否则返回false trySplit()，这就是为Spliterator专门设计的方法，区分与普通的Iterator，该方法会把当前元素划分一部分出去创建一个新的Spliterator作为返回，两个Spliterator变会并行执行，如果元素个数小到无法划分则返回null estimateSize()，该方法用于估算还剩下多少个元素需要遍历 characteristics()，其实就是表示该Spliterator有哪些特性，用于可以更好控制和优化Spliterator的使用。 ","date":"2017-07-11T21:48:32+08:00","image":"https://leibnizhu.github.io/p/ArrayList%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0JDK8/city_hud8d69787ae494c030da2db7909294360_43124_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/ArrayList%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0JDK8/","title":"ArrayList源码阅读笔记（JDK8）"},{"content":"String源码阅读笔记 成员变量 value，存放String数据，不可变。\nprivate final char value[]; hash，存放String的哈希值\nprivate int hash; // Default to 0 构造器部分 String(String original); 将参数的value和hash复制给当前对象。\nString(char value[]); 调用Arrays.copyOf()复制到value属性中。\nString(char value[], int offset, int count); 先判断offset和count与value的长度对比是否可用，然后调用Arrays.copyOfRange()复制到value属性中。\nString(int[] codePoints, int offset, int count); 类似上面，判断完之后，再遍历codePoints排除非法int，最后遍历codePoints强转char[]，赋值给value属性。\nString(byte bytes[], int offset, int length, String charsetName); 检查参数没有越界后，调用StringCoding.decode()生成char[]赋值给value属性。\nString(byte bytes[], int offset, int length, Charset charset); 类似上面，只是代表编码的参数改了类型。\nString(byte bytes[], String charsetName); String(byte bytes[], Charset charset); String(byte bytes[], int offset, int length); String(byte bytes[]); 分别调用上面两个构造方法。\nString(StringBuffer buffer); String(StringBuilder builder); 调用Arrays.copyOf()利用参数的value和length生成char[]赋值给value属性。两者区别在于前者多了synchronized修饰。\n而关于StringCoding.decode()，从源码可以看出来，默认的情况下会调用Charset.defaultCharset()获取默认编码，默认情况下获取到UTF-8编码，如果获取失败或者获取到的并不支持的话则使用ISO-8859-1编码：\n/* StringCoding类 */ static char[] decode(byte[] ba, int off, int len) { String csn = Charset.defaultCharset().name(); try { // use charset name decode() variant which provides caching. return decode(csn, ba, off, len); } catch (UnsupportedEncodingException x) { warnUnsupportedCharset(csn); } try { return decode(\u0026#34;ISO-8859-1\u0026#34;, ba, off, len); } catch (UnsupportedEncodingException x) { // If this code is hit during VM initialization, MessageUtils is // the only way we will be able to get any kind of error message. MessageUtils.err(\u0026#34;ISO-8859-1 charset not available: \u0026#34; + x.toString()); // If we can not find ISO-8859-1 (a required encoding) then things // are seriously wrong with the installation. System.exit(1); return null; } } /* Charset类 */ public static Charset defaultCharset() { if (defaultCharset == null) { synchronized (Charset.class) { String csn = AccessController.doPrivileged( new GetPropertyAction(\u0026#34;file.encoding\u0026#34;)); Charset cs = lookup(csn); if (cs != null) defaultCharset = cs; else defaultCharset = forName(\u0026#34;UTF-8\u0026#34;); } } return defaultCharset; } 还有值得注意的时，String还提供了一个保护类型的构造方法String(char[] value, boolean share)，与String(char[] value)区别在于多了一个没用的参数，以便重载构造方法，而且实现时直接将参数的数组赋值给当前String对象的value属性，而不是复制数组，也就是说这个方法构造出来的String和参数传过来的char[]共享同一个数组，并不安全，这样的设计是出于性能和节约内存的考虑，因此这个方法是包私有的。\nString(char[] value, boolean share) { // assert share : \u0026#34;unshared not supported\u0026#34;; this.value = value; } 其他方法 静态工厂方法 String valueOf(Object obj); 调用Object的toString()方法。\nString valueOf(char data[]); String copyValueOf(char data[]); 调用String(char value[])构造器。\nString valueOf(char data[], int offset, int count); String copyValueOf(char data[], int offset, int count); 调用String(char value[], int offset, int count)构造器。\nString valueOf(boolean b); 返回\u0026quot;true\u0026quot;或\u0026quot;false\u0026quot;。\nString valueOf(char c); 调用String(char[] value, true)构造器。\nString valueOf(int i); String valueOf(long l); String valueOf(float f); String valueOf(double d); 调用参数对应包装类的toString()方法。\nintern()方法 public native String intern(); 该方法返回一个字符串对象的内部化引用。 众所周知：String类维护一个初始为空的字符串的对象池，当intern方法被调用时，如果对象池中已经包含这一个相等的字符串对象则返回对象池中的实例，否则添加字符串到对象池并返回该字符串的引用。\n对“+”的重载 String对“+”的支持其实就是使用了StringBuilder以及他的append()、toString()le两个方法。\n常规方法 boolean isEmpty(); 判断length是否为0。\nchar charAt(int index); 判断越界，然后直接从value数组取值。\nint codePointAt(int index); int codePointBefore(int index); int codePointCount(int beginIndex, int endIndex); int offsetByCodePoints(int index, int codePointOffset); 判断越界然后调用Character对应静态方法。\nbyte[] getBytes(String charsetName); byte[] getBytes(Charset charset); byte[] getBytes(); 调用StringCoding.encode()编码返回。\nString substring(int beginIndex); String substring(int beginIndex, int endIndex); CharSequence subSequence(int beginIndex, int endIndex); 调用String的构造方法String(char value[], int offset, int count)，将会将原来的char[]中的值逐一复制到新的String中，两个数组并不是共享的，虽然这样做损失一些性能，但是有效地避免了内存泄露。\nString concat(String str); 先将原来数据用Arrays.copyOf()复制到一个char数组中，然后调用getChars()将str的值复制到char数组后面，最后调用共享char[]的构造方法将char数组构造成新的String对象并返回。\nboolean matches(String regex); 调用Pattern.matches()方法。\nboolean contains(CharSequence s); 调用indexOf()进行判断，只要返回索引大于-1即包含。\nString[] split(String regex, int limit); String[] split(String regex); 后者调用前者；如果regex长度为1而且不包含\u0026quot;.$|()[{^?*+\\\\\u0026quot;，或者regex长度为2而且以\u0026quot;\\\\\u0026quot;开头且第二个字符非数字字母（总而言之分割的正则其实只有一个字符），则创建一个List，遍历value，读取匹配到regex的时候，切取分隔符前面的子字符串，放入List中，最后一段也放入List，最后根据limit创建一个子List转换为String[]并返回；否则调用Pattern.compile(regex).split()进行计算并返回。\nString join(CharSequence delimiter, CharSequence... elements); String join(CharSequence delimiter, Iterable\u0026lt;? extends CharSequence\u0026gt; elements) 调用StringJoiner的add()和toString()方法进行拼接。\nString toLowerCase(Locale locale); String toLowerCase(); String toUpperCase(Locale locale); String toUpperCase(); 涉及到多语言的实现，实现起来比较复杂，没仔细看。\nString trim() 分别从头和尾开始遍历找到首次不为空字符的位置，取子字符串返回。\nchar[] toCharArray() 创建一个同样长度的char数组，调用System.arraycopy()复制并返回，避免安全性问题。\nString format(String format, Object... args); String format(Locale l, String format, Object... args); 调用Formatter的format()方法进行计算并返回。\n替代方法 String replace(char oldChar, char newChar); 如果新旧字符一样则直接返回this好了，否则先遍历，找到第一次出现oldChar的下标，如果没找到也是返回this，找到则将该下标之前的值循环复制到新数组，此下标之后的值复制到新数组的时候先判断是否oldChar，是的话复制newChar到新数组；最后用新数组构造一个String并返回。\nString replaceFirst(String regex, String replacement); 调用Pattern.matcher()找到匹配之后，再调用Matcher.replaceFirst()来替换首次出现。\nString replaceAll(String regex, String replacement); 调用Pattern.matcher()找到匹配之后，再调用Matcher.replaceAll()来替换全部。\nString replace(CharSequence target, CharSequence replacement); 同样调用Pattern.matcher().replaceAll()，支持单个字符。\n比较方法 boolean equals(Object anObject); 先判断是否this，再判断是否String对象，再判断长度是否相等，最后逐个char进行对比。\nboolean contentEquals(StringBuffer sb); 调用contentEquals(CharSequence cs)，该方法判断如果是StringBuffer则加同步去执行nonSyncContentEquals(AbstractStringBuilder sb)，否则（StringBuilder的情况）不加同步直接执行。 而nonSyncContentEquals(AbstractStringBuilder sb)中具体的比较流程与equals基本一致。\nboolean equalsIgnoreCase(String anotherString); 先后判断是否this、是否null、长度是否相同，然后调用boolean regionMatches()。\nboolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len); 比较this和other是否相等，先判断越界，再逐个字符比较，相同则继续，不同则根据ignoreCase参数，如果true则先将比较双方转成大写进行相等判断，还不相等则转成小写（针对格鲁吉亚语）进行判断。\nboolean startsWith(String prefix, int toffset), boolean startsWith(String prefix)``` 后者调用前者，从指定偏移量开始，逐个字符进行判断是否相等，判断次数为prefix的长度。 ```java boolean endsWith(String suffix); 调用startsWith(suffix, value.length - suffix.value.length)，判断this的后面N（suffix的长度）个字符是否与suffix相等。\n哈希方法 int hashCode(); hash属性初始化为0，如果调用hashCode()的时候发现hash为0则开始计算哈希值（懒加载）；由于String不可变，则hash计算一次即可。哈希算法核心为h = 31 * h + val[i];，遍历所有字符，循环地加上乘以31的哈希值作为新的哈希值，相当于val[0]*31^(n-1) + val[1]*31^(n-2) + ... + val[n-1]；而选用31，可能时出于i*31== (i\u0026lt;\u0026lt;5)-1的考虑。\n查找方法 int indexOf(int ch); int indexOf(int ch, int fromIndex); 前者调用后者（fromIndex=0），先判断越界，然后有两种情况就是ch对应单字节和双字节，单字节则直接从fromIndex开始遍历对比查找，双字节则调用int indexOfSupplementary(int ch, int fromIndex)遍历查找的时候同时判断两个字节。\nint lastIndexOf(int ch); int lastIndexOf(int ch, int fromIndex); int lastIndexOfSupplementary(int ch, int fromIndex); 与indexOf()系列类似，只是遍历查找的起点和方向不同。\nint indexOf(String str); int indexOf(String str, int fromIndex); int indexOf(char[] source, int sourceOffset, int sourceCount, String target, int fromIndex); int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex); 前三者调用最后一个方法。处理完越界和特殊情况后，开始遍历，遍历过程中每次先找到this中出现target的第一个字符（减少判断），找到后开始从当前下标开始，this的值与target的值逐个比较，判断到不相等的值或者到target的结尾则退出判断，然后如果退出判断时的下标等于开始判断下标+target长度，那么就是找到了，返回开始判断的下标，否则继续外面的循环。并没有用KMP算法。\nint lastIndexOf(String str); int lastIndexOf(String str, int fromIndex); int lastIndexOf(char[] source, int sourceOffset, int sourceCount, String target, int fromIndex); int lastIndexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex); 与indexOf()系列类似，只是查找的方向以及起始位置不一样了。\n","date":"2017-07-09T20:54:16+08:00","image":"https://leibnizhu.github.io/p/String%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0JDK8/cup_hu7e3a3189d4a581c062a0f72ed1d4499d_36680_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/String%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0JDK8/","title":"String源码阅读笔记（JDK8）"},{"content":"JVM笔记系列索引\n《深入理解Java虚拟机》 学习笔记(一)——JVM内存结构\n《深入理解Java虚拟机》 学习笔记(二)——垃圾回收\n《深入理解Java虚拟机》 学习笔记(三)——类文件结构\n《深入理解Java虚拟机》 学习笔记(四)——类加载机制与JVM优化\n《深入理解Java虚拟机》 学习笔记(五.终章)——Java内存模型与线程安全/优化\nJava内存模型 JVM定义了一种Java内存模型以消除各种硬件和操作系统的内存访问差异，保证Java程序的跨平台性。\n主内存和工作内存 所有的变量存储在主内存，每个线程有自己的工作内存，保存了被这个线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都在对应工作内存进行。\n主内存和工作内存与第一篇的内存结构不是一个层面上的概念，主内存主要对应Java堆的对象实例数据，工作内存主要对应虚拟机栈的部分区域，而且工作内存可能优先存储与寄存器和高速缓存中。\n内存操作 Java内存模型规定了8种内存操作：\nlock锁定，将变量表示为某线程独占； unlock解锁，将变量的lock状态借出，执行后才能被其他线程lock； read读取，将变量的值从主内存中读取，准备load； load载入，必须先进行read，将read到的数据放入工作内存的变量副本； use使用，将工作内存的变量的值传递给执行引擎； assign赋值，接收执行引擎的值付给工作内存的变量； store存储，将变量的值从工作内存读取，准备write； write写入，必须先进行store，将store到的数据放入主内存对应变量。 此外规定了对应的一些规则，如read、load和store、write不能单独出现，assign后的变量必须同步回主内存，没有assign的变量不能同步回注内存等等。\nvolatile变量 volatile声明能保证变量对所有线程可见性，即一个线程修改了变量的值，其他线程立即得知新的值。但由于Java操作并非全是原子操作，所以volatile变量在并发下不能保证线程安全。例如++操作，对应字节码为4个指令，大致可分为读值/加一/写入等步骤，volatile关键值只能保证在取值放入操作栈顶的时候是最新的值，此后的操作之前可能其他线程已经修改了变量的值，就会导致线程安全问题。\n适合使用volatile的情况有：\n运算结果不依赖变量的当前值，或者只有单一线程修改变量的值； 变量不需要与其他状态变量共同参与不变约束。 volatile关键字还能禁止指令重排优化，普通变量只能保证依赖赋值结果的地方都获得正确结果，但不能保证赋值操作的顺序与代码一致。volatile关键字可以避免多线程情况下，代码执行顺序被重排导致的一些错误。\nvolatile变量的读操作性能与普通变量基本无差别，写操作可能慢一些，总体开销比锁小。\n在Java内存模型层面上来看，volatile变量要求load操作和use操作必须连续一起出现，assign和store操作也必须连续一起出现，即每次使用volatile变量前必须从主内存刷新最新的值，每次修改volatile变量之后必须立刻同步到主内存。\nlong和double变量 没有被volatile修饰的64位数据（long和double）的读写操作划分位两次32位操作，不能保证其操作的原子性。但实际目前商用JVM基本都把64位数据的读写操作作为原子操作。\nJava线程 Java线程实现 一般来说线程有3种实现方法：\n使用内核线程：使用操作系统内核的轻量级进程（LWP），每个轻量级进程与一个内核线程一一对应，一对一。缺点是代价较高，在内核态和用户态之间来回切换，并消耗内核资源，操作系统支持的轻量级进程数量有限； 使用用户线程：用户线程建立在用户空间的线程库上，其建立、同步、销毁、调度完全在用户态完成，不需要内核帮助，一对多。因此快速且低消耗，支持更大的线程数量。缺点是实现复杂，很多问题需要考虑实现； 用户线程+轻量级进程：以上两者的混合，多対多。 JDK1.2之前使用用户线程实现，JDK1.2开始替换为基于操作系统原生线程模型实现。对Sun JDK而言，Windows和Linux版都是一对一线程模型，一条Java线程映射到一条轻量级进程，Solaris中支持一对一和多対多。\nJava线程调度 线程调度方式分为协同式和抢占式。协同式(Cooperative)指线程执行时间由线程自身控制，执行完完成后主动通知系统切换线程；抢占式(Preemptive)指由系统统一分配每个线程的执行时间，线程自身不能决定线程切换。Java使用抢占式。\nJava线程安全 线程安全等级 按由强至弱分为以下5种：\n不可变：基本数据类型加final修饰，对象则保证其行为不影响其状态。注意AtomicInteger和AtomicLong并非不可变，这样的设计应该是考虑到线程访问外部变量需要final，但有时候需要可变的数，于是有了这些类； 绝对线程安全：不需要任何额外的同步措施，即可实现线程安全。Java API标注线程安全的类很多并不是绝对线程安全； 相对线程安全：通常意义上的线程安全，指保证对象单独的操作是线程安全的，但不保证任何顺序连续调用都能保证线程安全/正确性； 线程兼容：通常意义上的线程不安全，指对象本身并不线程安全，但可以通过同步手段保证在并发环境下安全、准确； 相互层对立：无论是否采取同步手段，都无法在多线程环境下并发使用。极少出现，比如Thread类的suspend()和resume()方法，如果两个线程同时持有同一个线程对象，同时分别去中断及恢复线程，中断的是进行恢复操作的线程，那么就会产生死锁。 线程安全实现方法 包括以下几种：\n互斥同步：保证共享数据同一时刻制备一个线程使用。Java最基本的互斥同步手段是sychronized关键字，编译后在同步块前后形成monitorenter和monitorexit两个字节码指令，需要一个reference类型参数来指定锁定/解锁的对象；执行monitorenter指令时，先尝试获取锁，如果锁对象没被锁定或者当前线程已经拥有这个锁，那么锁的计数器加一，并进入代码块，到monitorexit指令执行时则计数器减一，到计数器为0时释放锁。也就是说sychronized对同个线程而言是可重入的，不会自己把自己死锁。但阻塞或唤醒线程开销都比较大，需要切换用户态/内核态，因此sychronized是重量级操作。还可以使用ReentrantLock实现同步，相比sychronized，有等待可中断、公平锁、绑定多条件(Condition)等功能。JDK1.6之后sychronized与ReentrantLock性能基本持平。 非阻塞同步：先进行操作，没有其他线程争用共享数据则操作成功，否则产生冲突，则采取补偿措施（比如不断重试），基于处理器的一些新指令实现，如Compare-and-Swap（比较并交换，CAS），用户程序不能直接调用，但AtomicInteger等类使用到了。 无同步方案：如可重入代码（可以在代码执行的任何时刻中断，去执行别的代码，再返回继续执行而不出现错误），线程本地存储（Thread Local Storage）。 Java锁优化 JDK1.6开始引入了许多高效并发优化，实现了各种锁优化技术：\n自旋锁与自适应锁：自旋锁即多个线程请求锁，若持有锁的线程很快会释放锁的话，让其他线程在其他CPU内核执行忙循环（自旋），而不是来回切换挂起/恢复线程，减少开销。自适应锁就是自旋时间由前一次在同一个锁上自旋时间、以及锁的拥有者状态决定自旋时间的自旋锁。 锁消除：根据逃逸分析结果，判定代码对应堆中数据都不会逃逸的话，可以认为是线程私有的数据，就可以不加同步锁，提高效率。 锁粗化：如果一系列连续操作都对同一个对象反复加锁解锁，甚至是循环体内加锁，那么频繁进行互斥同步操作会导致不必要的性能损耗，可以将锁的范围扩大，即称为锁粗化。 轻量级锁：在无竞争的情况下使用CAS操作消除同步的互斥量 偏向锁：锁偏向第一个获取该锁的线程，如果执行过程中锁没有被其他线程获取，那么持有偏向锁的线程无需同步；如果有其他线程尝试获取该锁，那么结束偏向模式。 ","date":"2017-06-20T12:55:14+08:00","image":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94.%E7%BB%88%E7%AB%A0Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/%E4%BC%98%E5%8C%96/touhou_yuyuko_hu55a8f4d087e1038e70c3ad67a6d35744_50690_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94.%E7%BB%88%E7%AB%A0Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/%E4%BC%98%E5%8C%96/","title":"《深入理解Java虚拟机》 学习笔记(五.终章)——Java内存模型与线程安全/优化"},{"content":"JVM笔记系列索引\n《深入理解Java虚拟机》 学习笔记(一)——JVM内存结构\n《深入理解Java虚拟机》 学习笔记(二)——垃圾回收\n《深入理解Java虚拟机》 学习笔记(三)——类文件结构\n《深入理解Java虚拟机》 学习笔记(四)——类加载机制与JVM优化\n《深入理解Java虚拟机》 学习笔记(五.终章)——Java内存模型与线程安全/优化\n类加载机制 类从加载导卸载出内存的整个生命周期如上图所示。图中的7个阶段中，加载、验证、准备、初始化和卸载的顺序是确定的，而解析和使用阶段不一定，解析可能在初始化之后（动态绑定）。\n类加载时机 有且只有以下5种情况：\n遇到new、getstatic、putstatic、invokestatic等字节码，对应Java代码中的new对象、读取或者设置类的静态变量、调用类的静态方法； 使用reflect包进行反射的时候； 初始化类时，若父类未初始化，则先出发父类的初始化； JVM启动时，执行的主类（包含main()方法的类）； JDK1.7以后动态语言支持。 注意是有且仅有，其他情况，譬如数组定义引用到未加载的类、调用类的静态常量（存储在常量池中）等其他情况并不会触发类的初始化加载。\n加载 完成以下事情：\n通过类的全限定名获取类的二进制字节流（不一定从文件获取，也可能是从网络、zip包、动态代理、其他文件如jsp等途径生成）； 将字节流的静态存储结构转化为方法区的运行时数据结构，具体由虚拟机自行实现定义； 生成对应java.lang.Class对象，放在方法区。 数组类本身不通过类加载器创建，而是JVM直接创建的，如果数组类加载的时候，其组件类型（去掉最外面维度之后的类型）是引用类型，则递归地加载。\n加载阶段和连接阶段的部分内容（比如一部分字节码文件格式的验证动作）时交叉进行的，但两者的开始时间肯定是保持先后顺序的。\n验证 验证步骤是为了确保Class文件的字节流中的信息符合JVM要求，且不危害JVM安全。验证过程又细分为以下4个子过程：\n文件格式验证：验证是否符合Class文件格式，如果验证到不符合Class文件格式约束，则JVM抛出java.lang.VerifyError异常或其子类； 元数据验证:进行语义分析，保证其符合Java语言规范； 字节码验证：通过数据流和控制流分析，确认语义合法且符合逻辑。JDK1.6之后的javac在Code属性的属性表里面增加了一项“StackMapTable”属性，描述了方法提中所有基本块（按控制流拆分的代码块）开始时本地变量表和操作栈应有的状态，字节码验证过程中秩序检查StackMapTable属性的记录是否合法即可； 符号引用验证：在连接的第三阶段——解析阶段中，JVM将符号引用转化为直接引用进行符号引用验证，对类自身以外的信息进行匹配行校验，比如符号引用的类是否能找到，类、方法、字段的访问性是否能被当前类访问。 准备 为类的静态变量分配内存，并初始化其值（初始化为零值）。如果有定义其取值，且非final变量，比如\npublic static String test = \u0026#34;test\u0026#34;; test变量不是final变量，会被初始化为零值null，在初始化阶段调用\u0026lt;clinit\u0026gt;方法时才会赋值\u0026quot;test\u0026quot;。\n而一个final的静态变量，即常量，如：\npublic final static String test = \u0026#34;test\u0026#34;; 是会通过ConstantValue属性在准备阶段就初始化为\u0026quot;test\u0026quot;。\n解析 JVM将常量池的符号引用替换为直接引用的过程。JVM规范要求在调用符号引用操作的字节码指令之前必须先对其所使用的符号引用进行解析。JVM会将第一次解析结果进行缓存，避免解析动作重复进行。\n初始化 初始化是类加载过程的最后一步，执行类构造器\u0026lt;clinit\u0026gt;()方法。\n\u0026lt;clinit\u0026gt;方法由编译器自动收集类中所有静态变量的赋值动作以及静态代码块合并生成的，按源文件中出现的顺序（即静态代码块对于其后的静态变量，可以赋值，但不能访问）。\u0026lt;clinit\u0026gt;方法不需要调用父类的类构造器，JVM会保证\u0026lt;clinit\u0026gt;执行前父类的\u0026lt;clinit\u0026gt;方法已经执行，所以JVM第一个执行的\u0026lt;clinit\u0026gt;方法是Object的。JVM会保证多线程环境中\u0026lt;clinit\u0026gt;方法执行的安全性，保证只有一个线程去执行。\n\u0026lt;clinit\u0026gt;方法不是必须的，比如没有静态变量的接口，或没有静态代码块和静态变量的类。\n编译期优化 javac对代码的运行效率几乎没有任何优化措施（JDK1.3之后-O优化参数没有意义了），性能优化主要集中在运行期（后端的即时编译期），javac主要进行了一些针对Java语言编码过程的优化，如语法糖。\n编译过程 编译过程大概分为3个过程： javac中的代码是这样的： 解析与填充符号表过程：首先进行词法、语法分析，将源代码的字符流转换为Token集合，然后根据Token序列构造抽象语法树AST，对应parseFiles()方法；然后填充符号表（记录符号地址和符号信息映射关系），符号表中记录的信息在编译的不同阶段都会用到，对应enterTress()方法； 插入式注解处理器的注解处理过程：处理代码中的注解，这个过程中可能影响到语法树的元素，如果影响到了，则要重新回到解析及填充符号表的过程，这样一个循环称作一个Round，直到注解处理器没有对语法树进行修改； 分析与字节码生成过程：具体又分为标注检查（检查变量使用前是否已生命、变量与赋值之间类型是否匹配等问题，以及常量折叠，如\u0026quot;1\u0026quot;+\u0026ldquo;2\u0026quot;优化为\u0026quot;12\u0026rdquo;）、数据及控制流分析（检查局部变量使用前是否赋值、每条路径是否都有返回值、异常是否都处理了等问题）、解语法糖（由desugar()方法完成）、字节码生成（收敛生成\u0026lt;clinit\u0026gt;()方法he \u0026lt;init\u0026gt;()方法，将所有生成的信息转换成字节码写入磁盘）等子过程。 语法糖 泛型 Java的泛型是伪泛型，只在源码中存在，编译时进行类型擦除变成原生类型（Raw Type），并在调用的地方加上强转类型代码，这是为了兼容旧版本。对于重载方法，如果泛型参数的泛型类型不同而其他参数以及返回类型相同，是不允许重载的，比如以下方法1和方法2不能重载；而如果泛型参数的泛型类型不同，且返回类型不同，则可以重载，比如方法1和方法3（JVM本来就允许）。\n//方法1和方法2不能重载，方法1和方法3可以重载 //方法1 public String test(List\u0026lt;Integer\u0026gt;); //方法2 public String test(List\u0026lt;String\u0026gt;); //方法3 public int test(List\u0026lt;String\u0026gt;); 自动装箱/拆箱、遍历循环、变长参数 遍历循环（增强for）的实现是编译时还原为迭代其的实现，因此需要实现Iterable接口。\n条件编译 java的条件编译通过条件为常量的if语句实现。如下面代码中，编译后的字节码不会包含调用B()方法的指令。\nif(true){ A(); } else { B(); } 运行期优化 解释器与编译器 许多主流商用JVM包括HotSpot采用解释器与编译器并存的结构，启动的时候使用解释器，保证启动速度，随着运行时间推移，编译器发挥作用，编译为本地代码，提高执行效率。在JVM中这种模式被称为混合模式，可以用-Xint强制JVM运行于解释模式，或用-Xcomp强制JVM运行于编译模式。HotSpot包含两个及时编译器Client Compiler和Server Compiler，一般简称为C1和C2。\n热点探测 运行过程中被即时编译器编译的热点代码包括被多次调用的 方法 或 循环体，对于后者编译器还是会以整个方法作为编译对象。\n判断方法或循环体是否热点代码的行为被称为热点探测，目前主要的热点探测方法有两种：\n基于采样的热点探测。JVM周期性的检查各个线程的栈顶，如果发现某些方法经常出现在栈顶，那么就是热点方法。缺点是容易收到线程阻塞或其他外界因素影响，优点是简单高效； 基于计数器的热点探测。为每个方法甚至代码块建立计数器，统计执行次数，超过一定阈值就认为是热点方法。缺点是不能获取导方法的调用关系，优点是精确且严谨。 HotSpot使用第二种，准备了方法调用计数器和回边计数器。前者统计方法被调用的次数，默认的阈值：Client模式1500次，Server模式10000次，可以通过-XX:CompileThreshold来设定；后者统计循环体被执行的次数，字节码遇到控制流向后跳转的指令称为回边(Back Edge)，通过-XX:BackEdgeThreshold来手动设置阈值。\n对于方法调用计数器，一个方法执行时先判断存不存在JIT编译过的版本，存在的话执行编译后版本，不存在的话计数器加一，再判断是否超过阈值，超过的话向即时编译器提交编译申请。其统计的并不是方法被调用的绝对次数，而是一段时间内的调用次数，如果超过一定时间计数器仍不足阈值，则计数值会减少一半，这被成为热度衰减(Counter Decay)，这段时间被称为半衰期。热度衰减的动作时在GC时顺便进行的。\n回边计数器没有计数热度衰减的过程，记录循环体被调用的绝对次数。\n默认配置下，编译是在后台的编译线程进行的，除非用-XX:-BackgroundCompilation来禁止后台编译，这样提交编译请求的线程会一直等待编译完成。\n编译优化技术 JVM几乎所有的优化措施都集中在及时编译器中。\n方法内联 方法内联（Method Inlining）指的是将调用的方法代码替换掉调用者的调用语句。目的：\n取出调用方法的成本，如建立栈帧； 为其他优化建立良好基础，比如内联可以发现更多的无用代码。 考虑到多态，方法内联的实现并不简单，在编译器无法得出调用的方法是哪个版本的结论（父类还是子类），需要在运行期确定。\nJVM引入了类型继承关系分析（Class Hierarchy Analysis，CHA）技术，用于确定目前加载的类中某个接口是否有多于一种的实现、某类是否存在子类、子类是否抽象等信息。进行内联时：\n如果目标方法是非虚方法（私有方法、实力构造器、父类方法、静态方法等），那么直接进行内联； 对于虚方法，向CHA查询该方法是否有多个版本可选，如果只有一个版本，则直接进行内联，此时属于激进优化，需要预留逃生门（守护内联），此后如果JVM没有加载到改变方法接受者的继承关系的类，则可以继续使用内联优化的版本，否则抛弃已编译的代码、退回到解释状态进行或重新编译； 如果虚方法有多个版本，则尝试内联缓存（Inline Cache）。发生方法调用前，内联缓存状态为空；第一次调用后，缓存记录下方法接受者的版本信息，每次进行方法调用的时候都比较接受者版本，如果方法接受者版本一样，则继续调用内联缓存进行内联，否则取消内联。 冗余访问消除 冗余访问消除（Redundant Loads Elimination）指的是如果能保证一个方法的两次调用之间的代码不会引起其返回值的更改，那么这第二次调用的结果可以直接用第一次调用结果去赋值，比如一下代码：\npublic void foo1(){ y=b.value; //其他调用，不会影响b.value的返回值 z=b.value(); } 以上代码可以优化为：\npublic void foo1(){ y=b.value; //其他调用，不会影响b.value的返回值 z=y; } 复写传播 复写传播（Copy Propagation）指的是去掉重复的变量。\n无用代码消除 无用代码消除（Dead Code Elimination），无用代码指的是永远不会izhixing的代码，或者完全没有意义的代码。\n公共子表达式消除 Common Subexpression Elimination定义：一个表达式E已经计算过，且计算后到现在E的变量全部没有变化，那么E这次出现成为了公共子表达式，无需重复计算，直接用前面计算结果替换即可。如果消除优化仅限于程序基本块内，则成为局部公共子表达式消除，如果覆盖范围涵盖多个基本块，则成为全局公共子表达式消除。\n数组边界检查消除 Java访问数组元素时，会对下标进行上下界范围检查，不满足上下界时会抛出ArrayIndexOutOfBoundsException异常。\n编译器根据数据流分析确定数组长度，并判断下标有无越界；在循环中进行数组访问时，也是可以通过数据流分析判定循环变量的取值是否越界，如果能保证循环体中不越界的话循环体中访问数组的语句可以消除边界检查。\n还有一种思路时隐式异常处理，将空指针检查和除数为零检查消除，注册一个Segment Fault信号的异常处理器，放在异常处理里面，在这个异常处理器里面再转换为对应的异常并抛出。\n还有一些其他的消除操作，比如自动装箱消除、安全点消除、消除反射等等。\n逃逸分析 逃逸分析不能直接优化代码，而是为其他优化手段提供优化的依据。逃逸分析指的是分析对象动态作用域：一个对象在方法中被定义后，被外部方法引用，则称为方法逃逸，被外部线程引用访问到的话，被称为线程逃逸。如果能证明一个对象不会逃逸到方法或线程外，则可以进行以下优化：\n栈上分配（Stack Allocation）：若确认对象没有方法逃逸，可以将其在栈上分配内存，则其占用内存会随着栈帧出栈而被销毁，减少GC压力，而一般应用中不逃逸的局部对象占很大比例； 同步消除（Synchronization Elimination）：若确认对象没有线程逃逸，可以对该变量实时的同步措施消除； 标量替换（Scalar Replacement）：标量指一个数据无法再分解为更小的数据来表示，如基础数据类型，反之称之为聚合量（Aggregate），如对象。若确认一个对象没有逃逸，则可以不创建对象，改为直接创建它会被使用到的成员变量来代替，同时可以保存在栈上，提高读写效率，并为进一步优化创造条件。 ","date":"2017-06-09T12:43:44+08:00","permalink":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9B%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E4%B8%8EJVM%E4%BC%98%E5%8C%96/","title":"《深入理解Java虚拟机》 学习笔记(四)——类加载机制与JVM优化"},{"content":"JVM笔记系列索引\n《深入理解Java虚拟机》 学习笔记(一)——JVM内存结构\n《深入理解Java虚拟机》 学习笔记(二)——垃圾回收\n《深入理解Java虚拟机》 学习笔记(三)——类文件结构\n《深入理解Java虚拟机》 学习笔记(四)——类加载机制与JVM优化\n《深入理解Java虚拟机》 学习笔记(五.终章)——Java内存模型与线程安全/优化\nJVM多语言支持 Java规范分为Java语言规范（The Java Language Specification）和Java虚拟机规范（The Java Virtual Machine Specification），因此JVM支持多种语言，只要该语言编译后的类文件符合JVM规范。比如我们常用的Scala、Kotlin、Clojure、Groovy等等。\n类文件结构 基础原则：多字节的数据，高位在前。JVM加载Class文件的时候进行动态连接。\nClass文件结构类似C的结构体，包含无符号数（u1/u2/u4/u8表示1/2/4/8字节的无符号数）和表（由多个无符号数或表组成的结构体，class文件本身就是一个大的表），有多个同类的无符号数或者表并数量不确定的时候，一般先用一个无符号数记录数量，后面接上一系列连续的这种无符号数或者表。Class文件没有分隔符号，所以整个数据结构都是被严格规定的。\n魔数与Class文件版本 Class文件头4个字节是固定的0xCAFEBABE（咖啡宝贝），显然与Java语言命名的历史相关。\n紧接着4个字节存储Class文件版本号，5-6字节是子版本号，7-8字节是主版本号（比如1.7.0是0x0033）。JVM读取Class文件的时候，搞版本JDK可以兼容旧版本Class文件，就是通过这4个字节进行判定的。\n常量池 一般来说常量池占Class文件空间最大，由于长度不定，所以入口有u2类型的常量池容量计数器（8-9位），计数从1开始（Class文件中其他容量计数器都是从0开始的）。\n常量池存储两类常量：字面量（类似Java语言中的常量）和符号引用，后者包括类和接口全名、字段名称和描述符、方法名称和描述符。JVM运行时从常量池获取符号引用再在类创建时解析到具体内存地址，没有C语言的“连接”步骤。\n常量池每一个常量都是一个表。，一共有14种表，表的开头都是一个u1类型的标志位代表当前常量的类型（浮点整形之类），后面的结构与具体的常量类型有关，各自不同。\n使用javap -verbose 类名可以解析类的结构，输出结构大概这样：\njavap -verbose com.turingdi.breorent.user.controller.RentAndReturnController Classfile /home/leibniz/workspace/BreoRent/target/classes/com/turingdi/breorent/user/controller/RentAndReturnController.class Last modified 2017-6-6; size 9126 bytes MD5 checksum 26ed5594f39cfc9d6b109637ad76bf12 Compiled from \u0026#34;RentAndReturnController.java\u0026#34; public class com.turingdi.breorent.user.controller.RentAndReturnController minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPER Constant pool: #1 = Methodref #111.#203 // java/lang/Object.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #2 = Class #204 // org/springframework/web/servlet/ModelAndView #3 = Methodref #2.#203 // org/springframework/web/servlet/ModelAndView.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V #4 = Class #205 // com/turingdi/breorent/common/wechatApi/process/WechatJdk #5 = Methodref #4.#206 // com/turingdi/breorent/common/wechatApi/process/WechatJdk.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:(Ljavax/servlet/http/HttpServletRequest;)V #6 = Methodref #4.#207 // com/turingdi/breorent/common/wechatApi/process/WechatJdk.getMap:()Ljava/util/Map; #7 = String #208 // appId #8 = String #209 // wechat #9 = String #210 // APP_ID …… 常见的两种常量举例：\nCONSTANT_Class_info，类常量，标志为0x07，紧接着是1个u2类型的类名索引，指的是类名（字符串常量）在常量池中的索引（如上面所说，从1开始数）。 CONSTANT_Utf8_info，字符串变量，标志为0x01，紧接着是1个u2类型的字符串长度，然后是字符串内容的bytes，u1类型，数量等于前面u2定义的。 从上面可以推导：类名（全限定名）是字符串常量，长度用u2类型表示，也就是最大长度是65535，换言之就是Java类全名最长65535，超过的无法编译。\n访问标志 常量池结束之后，有两个字节为访问标志，代表当前Class文件是否public、是否类/接口/枚举、是否抽象、是否注解等等。\n类索引、父类索引及接口索引集合 类索引和父类索引分别为u2类型数据，接口索引集合结合为u2类型数据的集合（只能有一个父类，可以实现多个接口），分别用于记录当前类、父类、实现的接口的类描述符（CONSTANT_Class_info）在常量池中的索引。\n类索引和父类索引紧接在访问标志后面，再后面是接口索引集合，入口是一个数量计数器，0表示没有实现任何接口，再后面就是具体的接口类描述符索引。\n字段表集合 描述类或接口中定义的字段，包括静态和非静态的。结构如下：\n类型 名称 数量 含义 u2 access_flags 1 访问标志，public/private/final/static/enum等描述符 u2 name_index 1 字段简单名称在常量池中的索引，即变量名或方法名 u2 descriptor_index 1 字段和方法的描述符在常量池的索引，描述字段类型或方法参数列表/返回类型 u2 attibutes_count 1 属性表计数器 attribute_info attributes attibutes_count 属性额外描述，比如描述变量初始化值在常量池中的索引 描述符描述方法的时候，先是参数列表，然后是返回值类型。而方法\npublic String toString(int test) 对应的描述符是\n(I)Ljava/lang/String; 其中L是表示对象类型。\n此外，字段表集合不会列出从父类或接口中继承的字段，但可能会有代码中不存在的字段，比如内部类对外部类实例的引用之类。\n方法表集合 方法表集合的入口同样也是一个u2类型的计数器，紧接着是各个具体的方法。方法表的结构与字段表基本一样，不列出来了。区别：\n首先是access_flags的取值范围不同，比如没有ACC_TRANSIENT、有ACC_SYNCHRONIZED等值； name_index表示方法名索引，descriptor_index表示方法描述符索引，跟字段表一样； 而编译后的方法代码，放在属性表里面名为“Code”的属性中； 没有Override的父类方法，不会出现在子类的方法表集合中； 同样可能出现代码中原本没有的方法，比如\u0026lt;clinit\u0026gt;（类构造器）、\u0026lt;init\u0026gt;（实例对象构造器）。 两个方法名字相同，参数列表相同，返回值类型不同，是允许共存在一个Class文件中的，但Java语言不允许这样。\n属性表集合 Class文件、字段表、方法表都可以有自己的属性表，Java7里面定义了21种属性。\nCode属性 并非所有方法表都有Code属性，比如接口和抽象类的方法就没有。结构如下：\n类型 名称 数量 含义 u2 attribute_name_index 1 属性名的索引，对Code属性而言恒为\u0026quot;Code\u0026quot; u4 attribute_length 1 属性值长度，相当于整个属性表长度长度减6(u2+u4) u2 max_stack 1 操作数栈深度最大值。JVM运行时根据此值分配栈桢的操作栈深度 u2 max_locals 1 局部变量表所需存储空间，单位是Slot，double和long占用2个Slot、其他基本类型1Slot，Slot空间可以重用(变量作用域问题) u4 code_length 1 编译后的字节码长度，理论上最长2^32-1，实际上JVM规定一个方法不允许超过65535条字节码指令 u1 code code_length 代码编译后的字节码 u2 exception_table_length 1 异常表长度 exception_info exception_table exception_table_length 异常表，记录字节码在start_pc到end_pc行之间如果出现类型为catch_type或其子类的异常则跳转到handler_pc行继续处理 u2 attibutes_count 1 属性表计数器 attribute_info attributes attibutes_count 属性额外描述，比如描述变量初始化值在常量池中的索引 字节码值得注意的一个地方是，javac编译时将this关键字作为一个普通方法参数由JVM调用时自动传入。\nExceptions属性 描述方法可能抛出的受检异常。\nLineNumberTable属性 描述Java远吗行号与字节码行号之间映射关系，也就是为什么抛异常的时候可以显示源码哪一行抛出的。\nLocalVariableTable属性 描述栈桢中局部变量表与Java源码中变量的关系，以保证编译后的代码被其他代码调用时，IDE可以显示参数名（否则被arg0、arg1之类的变量名代替）\nSourceFile属性 描述生成当前Class文件的源文件名称，也是抛异常时可以显示源文件名字的原因。但内部类不会生成这个属性。\nConstantValue属性 static关键字修饰的变量可以使用这个属性。对于Sun javac编译器，final static的变量采用ConstantValue属性初始化，其他static变量在\u0026lt;clinit\u0026gt;（类构造器）中初始化。\nInnerClasses属性 记录内部类和宿主类的关联。内部类和宿主类的Class文件都会有这个属性。\nSignature属性 记录泛型签名信息。Java的泛型是使用擦除式实现的伪泛型，编译后擦除泛型，这个属性为了弥补此缺陷，方便反射API可以拿到泛型类型。\n字节码指令 字节码指令由一个字节的操作码（代表具体操作）及跟随其后的0个或多个操作数（操作所需的参数）组成。JVM大多数指令不含操作数只有操作码。\nClass文件放弃了操作数对齐，因此省略很多填充和分割符，因此体积可以尽量小；缺点是损失一些解析字节码的性能。\nJVM的指令大多数包含了操作的数据类型信息，但因为只有一个字节，也就是说最多只有256种指令，所以不是所有命令对所有数据类型都有独立的指令（非完全独立），同时提供一些指令将指令不支持类型的操作数转换为可支持的类型。\nJVM的浮点数运算，舍入模式是最低有效位向下（0）取整。操作溢出时用有符号的无穷大表示（INF），如果操作结果没有明确数学意义则得到NaN（非数字，比如0/0，∞×0之类）\n","date":"2017-06-06T12:46:22+08:00","image":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/classfile_hu5d2ad4b4cdcfbb03e8908b4d8a621019_20422_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/","title":"《深入理解Java虚拟机》 学习笔记(三)——类文件结构"},{"content":"JVM笔记系列索引\n《深入理解Java虚拟机》 学习笔记(一)——JVM内存结构\n《深入理解Java虚拟机》 学习笔记(二)——垃圾回收\n《深入理解Java虚拟机》 学习笔记(三)——类文件结构\n《深入理解Java虚拟机》 学习笔记(四)——类加载机制与JVM优化\n《深入理解Java虚拟机》 学习笔记(五.终章)——Java内存模型与线程安全/优化\n判断对象可回收 有以下方法：\n引用计数法 维护引用计数的Map，对象被引用时计数加1，引用失效时计数减1，简单粗暴。\n实现简单，判断效率高。但主流JVM没有用这个方法的，因为无法解决循环引用的问题。\n可达性分析算法 JVM主流实现时可达性分析。\n通过一系列GC Root的对象作为起点，开始向下搜索，搜索经过的路径为引用链，当一个对象到GC Root没有任何引用链项链，则引用不可达，可以GC掉。\nGC Root通常包括以下几种：\n虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即一般说的Native方法）引用的对象； ………… 引用分类 引用分为强引用（平常用的引用，只要还存在有效，就不会被GC），软引用（SoftReference类，将要发生内存溢出前GC回收），弱引用（WeakReference类，生存到下一次GC，只被弱引用关联的对象会被回收！），虚引用（PhantomReference类，不影响实例生存时间，无法取得实例，唯一作用是被GC时收到一个系统通知）。\nfinalize()方法 可达性分析中发现不可达的对象，会被标记，如果没有覆盖finalize()方法，或者其finalize()方法已经被JVM调用过，则不会执行finalize()方法。\n否则需要执行finalize()方法，此对象放入F-Queue队列中，由JVM的低优先级Finalizer线程去执行。\n稍后，GC会对F-Queue的对象进行第二次标记，如果对象在finalize()方法中拯救了自己（重新被引用），则将会被移出要回收的集合。\n一般不覆盖finalize()方法，该方法设计之初是为了迎合C++语法的析构函数，应该用try-finally取代之。\n方法区/永生代的GC 永生代也会进行GC，主要收集废弃的常量和无用的类。\n其中无用的类的判定比较严格，要求：\n该类所有实例已被回收； 加载该类的ClassLoader已被回收； 该类的Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。 然而满足了以上条件的类，也不一定会被回收，仅仅是可以回收。\n垃圾收集算法 包括以下：\n标记-清除算法(Mark-Sweep) 先标记出所有需要回收的对象，然后统一回收被标记需要回收的对象。\n缺点：标记和清除的效率都不高，且清除后产生大量不连续的内存碎片。\n复制算法(Copying) 内存划分成等大小的两块，只有一块在使用，GC时将存活对象复制到另一块区域中，清除另一半。\n缺点：有一半内存浪费，复制操作效率低，不适合老年代。\n实际使用中并不是一半一半的内存分配，因为大部分对象生命周期很短，所以划分成一块很大的Eden区和两块小的Survivor区，一般默认8:1:1的比例。每次使用一块Eden和一块Survivor区，GC时讲Eden和Survivor中存活的对象复制到另一块Survivor区中。当Survivor区不够大时，使用老年代进行分配担保，存活对象放入老年代。\n标记-整理算法(Mark-Compact) 类似标记-清除算法，标记之后存活的对象向一端移动，最后清理掉边界以外的内存，保证内存的规整。\nHotSpot的算法实现 以下是HotSpot中垃圾回收的几个关键点技术实现方法。\n枚举GC Root 可达性分析要求在一个能保证一致性的快照中进行工作，即GC时必须停止所有线程，即Stop The World~~（JOJO里面Dio的世界 ザ・ワールド？）~~。\n在HotSpot里面，通过一个叫OopMap的数据结构来维护哪些地方存放着对象引用，记录栈上本地变量与堆中对象的引用关系，方便枚举GC Root。\n安全点 并不是所有指令都会生成/改变OopMap，这样效率太低。线程必须到达安全点（SafePoint）才会生成OopMap，然后开始GC。安全点的选定是以“有让程序长时间执行的特征”的原则进行的（如方法调用，循环跳转，异常跳转等）。\n考虑多线程，必须所有线程都跑到安全点才能开始GC。方法有二：\n抢先式中断：GC时先中断所有线程，对于还没跑到安全点的线程，让其恢复并等它跑到安全点上再停。——商业JVM几乎没有这样实现的； 主动式中断：GC需要中断线程时，设置一个标志，各个线程执行时，跑到安全点的时候轮询这个标志，发现标志为真的时候自己中断挂起。 安全区域 线程执行的时候，可能很久都不会跑到安全点（比如执行了Thread.Sleep()的情况），导致GC不能马上执行。因此引入安全区域的概念（Safe Region），表示这段代码片段中引用关系不会发生变化。\n线程执行到安全区域的时候，标识自己进入了安全区域；则发动GC的时候，可以忽略标识为安全区域的线程；而线程在离开安全区域的时候，需要检查是否已完成GC Root枚举（或者是整个GC过程），没完成的话要GC发出等待离开的信号。\n实际的垃圾收集器 上图给出了HotSpot的7个垃圾收集器，中间有连线的标识可以搭配使用。\nSerial收集器 收集时，暂停所有其他工作线程（Stop The World），开一个线程进行GC。对新生代采用复制算法，对老年代采取标记-整理算法。\nJVM在client模式下默认新生代垃圾收集器还在用Serial。\n优点：简单高效。\n缺点：GC时的停顿时间长。\nSerial Old收集器 Serial收集器的老年代版本，单线程，标记-整理算法。\nParNew收集器 Serial收集器的多线程版本，可以配合CMS收集器工作，不能配合Parallel Scavenge收集器工作。使用-XX+UseParNewGC指定使用之。\n澄清垃圾收集器的两个概念：\n并行Parallel：多条垃圾回收线程并行工作，用户线程在等待； 并发Concurrent：垃圾收集线程和用户线程同事执行（CMS）。 Parallel Scavenge收集器 新生代收集器，多线程、复制算法，与ParNew收集器的区别在于，关注点在于吞吐量（Thoughput，=运行用户代码时间/(运行用户代码时间+GC时间)），适合在后台运算而不需要提案多交互的任务。\n用-XX:MaxGCPauseMillis参数指定保证GC消耗时间的最大值（毫秒），减少GC时间是以牺牲吞吐量及新生代空间来获取的。\n用-XX:GCTimeRatio参数指定GC占总时间的比例，0-100，默认99，即允许1/(1+99)=1%的GC时间。\n使用-XX:UseAdaptiveSizePolicy参数之后，JVM根据当前系统情况动态调整新生代大小、Eden与Survivor比例等参数以保证最佳的吞吐量和设定的最长GC时间。\nParallel Old收集器 Parallel Scavenge收集器的老年代版本，多线程，标记-整理算法，JDK1.6开始提供。\nCMS收集器 老年代收集，设计目标是获取最短回收停顿时间，基于标记-清除算法设计，包括以下步骤：\n初始标记：标记GC Root可以直接关联到的对象，速度很快，需要Stop The World； 并发标记：可达性分析枚举，耗时较长； 重新标记：修正并发标记期间因用户程序继续运行而导致的引用变动，需要Stop The World； 并发清除：耗时较长。 CMS收集器缺点：\n对CPU资源敏感，并发阶段总吞吐量降低，CPU数量少的时候对用户程序影响大； 无法处理浮动垃圾（Floating Garbage），即并发清理阶段新产生的垃圾，要等到下一次GC；因此需要预留内存空间给用户线程使用，不能等内存快满才进行收集；当 CMS运行期间预留的内存不够，会出现“Concurrent Mode Failure”失败，JVM会临时启用Serial Old收集器重新进行老年代垃圾收集，导致停顿时间变长。可以通过-XX:CMSInitiatingOccupancyFraction参数调整老年代内存占用比例触发GC的阈值。 收集后产生内存碎片。 G1收集器 面向服务端，全称Garbage-First，停顿时间可控、可预测，不会产生内存碎片。\nG1收集器将整个Java堆划分成多个大小相等的独立区域（Region），跟踪各个Region里面垃圾堆积的价值（根据回收所获得的空间大小及回收耗费时间的经验值），维护优先列表，每次收集的时候，根据允许的收集时间，优先回收价值更大的Region。\n此外每个Region维护一个Remembered Set来避免全堆扫描，引用型数据进行写操作的时候，会产生中断写操作，检查引用的对象是否处于不同的Region，如果是，则记录到被引用对象所属的Remember Set中；那么在GC时，GC Root的枚举范围加入Remembered Set，保证不进行全堆扫描也不会有遗漏。\nG1收集器的回收步骤包括：\n初始标记：与CMS一样； 并发标记：与CMS一样，包含Remembered Set； 最终标记：并发标记期间对象变化记录到Remembered Set Logs中，合并到Remembered Set； 筛选回收：先根据回收价值和回收成本进行排序，再根据用户期望GC停顿时间制定回收计划。 内存分配、回收策略 优先在Eden分配：大多数情况下，对象在新生代Eden区分配，当Eden区空间不够时，发起Minor GC； 大对象直接进入老年代：提供-XX:PretenureSizeThreshold参数，大于此值的对象直接在老年代分配，避免在Eden区和Survivor区之间大量内存复制； 长期存活的对象进入老年代：JVM为对象定义年龄计数器，经过Minor GC依然存活且被Survivor区容纳的，移动到Survivor区，年龄加1，每经历一次Minor GC不被清理则年龄加1，增加到一定年龄则移动到老年区（默认15岁，通过-XX:MaxTenuringThreshold设置）； 动态对象年龄判定：若Survivor区中同年龄所有对象大小总和大于Survivor空间一半，则年龄大于等于该年龄的对象可以直接进入老年代； 空间分配担保：Minor GC之前，JVM检查老年代最大可用连续空间大于新生代所有对象总空间，成立的话Minor GC确认是安全的；否则检查老年代最大可用连续空间大于历次晋升到老年代对象的平均大小，大于的话进行Minor GC；小于的话进行Full GC。 ","date":"2017-06-04T13:57:44+08:00","image":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/2_huccc5a32b9972667086692466cd3686f0_65721_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","title":"《深入理解Java虚拟机》 学习笔记(二)——垃圾回收"},{"content":"最近一个月把经典Java书籍《深入理解Java虚拟机》读了一遍，受益匪浅，接下来几篇博客里将会总结一些学习笔记，或许会跟很多现有的博文重复，但主要是为了自己总结一下。\nJVM笔记系列索引\n《深入理解Java虚拟机》 学习笔记(一)——JVM内存结构\n《深入理解Java虚拟机》 学习笔记(二)——垃圾回收\n《深入理解Java虚拟机》 学习笔记(三)——类文件结构\n《深入理解Java虚拟机》 学习笔记(四)——类加载机制与JVM优化\n《深入理解Java虚拟机》 学习笔记(五.终章)——Java内存模型与线程安全/优化\nJVM内存结构 JVM内存结构不光是只有堆内存和栈内存，实际情况要复杂很多，主要包含以下结构。\n程序计数器 每个线程都有独立的程序计数器，各线程的互不影响，用于存储正在执行的虚拟机指令地址（对于Native方法则为空undefined）.\nJVM栈 JVM栈是线程私有的，每个方法执行的时候都会建立栈帧，栈帧包含以下内容：\n局部变量表：存放编译期可知的基本数据类型数据、对象引用和returnAddress，亦即运行期不会改变局部变量表大小; 操作数栈； 动态链接； 方法出口，等等。 该区域可能抛出以下异常：\n当线程请求的栈深度超过最大值，会抛出StackOverflowError异常； JVM栈动态扩展时无法申请导足够内存，抛出OutOfMemoryError异常。 本地方法栈 类似JVM栈，区别只在于本地方法栈用于执行本地(Native)方法。\nJava堆 所有线程共享的内存区域，用于存放对象实例（但现在不一定全部对象都在堆里了，栈上分配/标量替换等技术）。在GC的概念中还可以分为Eden区、FromSurvivor区及ToSurvivor区。也可能会划分出线程私有的分配缓冲区TLAB。\n方法区 线程共享，用于存放已加载的类、常量、静态变量、JIT编译后的代码等数据。\n对于HotSpot虚拟机用户而言，经常将方法区称为永生代（Permanent Generation），是因为HotSpot虚拟机用永生代实现方法区，用GC管理方法区\n运行时常量池 运行时常量池是方法区的一部分，类文件被加载后，常量部分就会被放到运行时常量池里。运行期期间也可以将新的常量放入常量池，比如String.intern()方法。\n直接内存 NIO里面引入直接内存的API，可以使用本地方法分配堆外内存，在某些情况下可以提高IO性能。\n创建对象过程 遇到new关键字的时候，检查对应类是否能在常量池定位到类的符号引用，并检查是否已加载、解析、初始化。没有的话线加载类； 分配内存。加载类之后一个对象所需的内存大小就确定了；使用Serial、ParNew等收集器时，堆内存是整齐的，使用指针碰撞划分内存，即在空闲内存的分界点开始分配指定大小的内存空间；如果用CMS等给予Mark-Sweep算法的收集器时，使用空闲列表划分内存，即JVM维护了一个记录可用内存的表，从改变中找一块足够大小的内存空间用于分配。 考虑到多线程同时创建对象的情况，会使用到前面说的TLAB，每个线程在自己的TLAB上分配内存，TLAB用完并重新分配新TLAB的时候才需要同步锁定。 申请内存后，进行初始化零值（可以在TLAB分配时进行）； 设置对象的对象头（Object Header）； 执行\u0026lt;init\u0026gt;方法。 ","date":"2017-05-27T21:31:01+08:00","image":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/java_hu734bbb3372dd7695b703eb3e94d9cd0c_7496_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","title":"《深入理解Java虚拟机》 学习笔记(一)——JVM内存结构"},{"content":"背景 昨天发现本地HDP集群的HBase连不上了，解决了这个问题后，又发现HBase同步脚本一直执行失败，进YARN看了下，几天前开始堆积了很多ACCEPT了的任务，但没一个在执行，先用hadoop job kill 干掉了旧的任务，重新执行脚本，还是不行，日志上面却没报错。\n于是进Ambari一看，HDFS的DataNode全都满了……看来要扩容了。\n集群是部署在几台VirtualBox的CentOS虚拟机上的（起始我一直想转移到Docker上，但一直没时间+懒）。网上关于VirtualBox虚拟机磁盘扩容的文章很多，试了下，很多是行不通的，而且不太完整，于是把今天的经验记下来吧。\n具体步骤 磁盘映像扩容 下面的命令以名为CentOS05的虚拟机为例进行。\n关闭虚拟机。\nvboxmanage controlvm CentOS05 poweroff 因为原来的磁盘是vmdk的不能直接扩展容量，需要先转换成vdi(这个步骤相当缓慢，视乎电脑配置和原磁盘映像大小，请耐心等待)：\ncd /path/to/虚拟机vmdk磁盘映像存储位置 vboxmanage clonehd CentOS05-disk1.vmdk CentOS05-disk1.vdi --format VDI 再扩容，注意默认单位是MB：\nvboxmanage modifyhd CentOS05-disk1.vdi --resize 122880 修改虚拟机挂载磁盘 查看虚拟机原来配置\nVBoxManage showvminfo CentOS05 注意类似以下的信息，包括名称（IDE），端口号，设备ID（如IDE(1,0)的端口号是1，设备号0），下面要用到：\nStorage Controller Name (0): IDE Storage Controller Type (0): PIIX4 Storage Controller Instance Number (0): 0 Storage Controller Max Port Count (0): 2 Storage Controller Port Count (0): 2 Storage Controller Bootable (0): on IDE (0, 0): /home/turing/VirtualBox VMs/CentOS03/CentOS03-disk1.vdmk (UUID: 2e2d32dc-e66b-42c2-8c9c-4cbb7abb6182) IDE (1, 0): Empty\n修改虚拟机挂载的磁盘：\nVBoxManage storageattach CentOS05 --storagectl \u0026#39;IDE\u0026#39; --port 0 --device 0 --type hdd --medium CentOS05-disk1.vdi 参数说明：\nstoragectl：存储控制器的名称。必须。 port：介质将被连接／断开／修改的端口号。必须。 device：介质将被连接／断开／修改的设备号。必须。 type：定义 介质将被连接／断开／修改的驱动器类型。 medium：允许指定DVD／软盘驱动器是完全断开的（none）或仅是需要被连接的空的DVD／软盘驱动器（emptydrive）。如果指定了uuid，filename或host:，将连接到存储控制器的指定端口和设备号。\n再次查看配置确认修改成功：\nVBoxManage showvminfo CentOS05 修改CentOS分区配置 因为在系统启动后根目录的卸载之类的动作旧就不能执行了，所以只能在LiveCD里面改，建议使用gparted图形化工具修改分区大小，比较简单，此处不表。\n修改好启动虚拟机：\nvboxmanage startvm CentOS05 --type headless ssh CentOS05 查看分区情况：\nfdisk -l /dev/sda Disk /dev/sda: 128.8 GB, 128849018880 bytes 255 heads, 63 sectors/track, 15665 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000636d5 Device Boot Start End Blocks Id System /dev/sda1 * 1 64 512000 83 Linux Partition 1 does not end on cylinder boundary. /dev/sda2 64 15666 125316096 8e Linux LVM\ndf -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/vg_centos03-lv_root 50G 44G 3.1G 94% / tmpfs 1.9G 0 1.9G 0% /dev/shm /dev/sda1 477M 30M 422M 7% /boot /dev/mapper/vg_centos03-lv_home 6.4G 82M 6.0G 2% /home\n发现fdisk正确识别分区容量，但df还是旧容量。仔细观察df的输出，原来LVM管理的，需要执行修改逻辑卷的命令。先看情况：\npvdisplay -v -m Using physical volume(s) on command line. Wiping cache of LVM-capable devices Finding all volume groups. \u0026mdash; Physical volume \u0026mdash; PV Name /dev/sda2 VG Name vg_centos03 PV Size 119.51 GiB / not usable 2.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 30594 Free PE 15360 Allocated PE 15234 PV UUID Eq5aod-THPs-vdcM-T40A-fCQW-dqQ1-NERrdH \u0026mdash; Physical Segments \u0026mdash; Physical extent 0 to 12799: Logical volume /dev/vg_centos03/lv_root Logical extents 0 to 12799 Physical extent 12800 to 14477: Logical volume /dev/vg_centos03/lv_home Logical extents 0 to 1677 Physical extent 14478 to 15233: Logical volume /dev/vg_centos03/lv_swap Logical extents 0 to 755 Physical extent 15234 to 30593: FREE\n结合df -h的输出，可知我们要改的是就是vg_centos03/lv_root逻辑卷，修改容量为110G：\nlvresize -L 110G -r vg_centos05/lv_root 再次查看容量发现OK。\n进入Ambari，重启服务，也看到空间警告消除了。\n","date":"2017-05-10T12:54:24+08:00","image":"https://leibnizhu.github.io/p/%E8%B0%83%E6%95%B4VirtualBox-CentOS%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E5%A4%A7%E5%B0%8F/vbox_hub32001874590c99b7acc51af4b05bb88_5952_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/%E8%B0%83%E6%95%B4VirtualBox-CentOS%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E5%A4%A7%E5%B0%8F/","title":"调整VirtualBox CentOS虚拟机磁盘大小"},{"content":"需求 对于类似上图所示的树状结构数据，统计每个节点的总子孙数、每个节点在各层的子孙数。\n比如B的总子孙数就是7（D/E/F/G/H/I/J），F的一级子孙数为2（G/H），F的二级和三级子孙数都为1（分别对应I和J）。\n注：此处用字母代替节点ID，是为了与级别区分，方便描述和理解，实际的节点ID并非如此。\n数据结构 树状关系在采集初期，是通过Netty搭建的微服务保存在HBase中，再通过Spark定期计算，保存到关系型数据库MySQL中。在MySQL中的表结构及保存的数据如下（经过精简，只保留与本文相关的字段）：\nID 父节点ID 父链 等级 A null - 1 B A -A- 2 C A -A- 2 D B -A-B- 3 E B -A-B- 3 F B -A-B- 3 G F -A-B-F- 4 H F -A-B-F- 4 I G -A-B-F-G- 5 J I -A-B-F-G-I- 6 ID、父节点ID、等级这几个字段比较好理解，父链稍加思考也可以看出来是从根节点一直到当前节点的链，以减号分割，父链字段前后都有减号，方便like查询的精准性。\n难点 可以看出来，单纯计算我们需要的指标并不困难，比如总子孙数，根据父链中包含当前节点这个条件去查询、再count就能拿到；而各级子孙数也是，父链条件结合等级的条件就能查出来。\n但这些查询都需要遍历整个树，而每个节点会产生一次查询，相当于复杂度是O(N^2)；而且每次需要查询MySQL，计算速度被IO开销限制，Spark批量计算的优势丝毫无存，可以预见计算是相当缓慢的。\n解决方案 思路 父链这个字段是为了将数据保存到MySQL，方便数据分析员进行查询而作出的妥协，而我们可以从父链下手，减少计算时间复杂度。\n总子孙数 上面的分析已经提到，总子孙数，根据父链中包含当前节点这个条件去查询、再count就能拿到，而考虑到树中没有环路，也就是说一个节点在树中只出现一次，那么在父链中也只会出现一次；也就是说，在整棵树里，一个节点的ID在所有父链中出现多少次（每次对应一个子节点的父链），就有多少个子孙节点。\n在SQL中查询父链包含某节点ID用的是like操作，查询速度很慢，我们在Spark中可以进行优化：\n每个节点的父链依据减号进行split，一个节点对应多个父祖节点（父链上所有节点）； 所有节点父链split的结果进行Word-Count，一个节点ID在父链split结果中出现多少次，意味着它有多少个总子孙数。 以开始的图为例，所有父链拆分后拿到的结果是：(A,A,A,B,A,B,A,B,A,B,F,A,B,F,A,B,F,G,A,B,F,G,I)，进行Word-Count的结果（Spark中可以直接用countByValue方法）是((A,9),(B,7),(F,4),(G,2),(I,1))。这里已经出来每个节点的总子孙数了，结果中不存在的节点就是总子孙数为0。\n各层子孙数 从总子孙数的计算中进行扩展就可以计算各层的子孙数了。\n在父链split的时候加上当前节点的等级构成RDD，以节点D为例，D的父链为-A-B-，级别为3，split后就是(A,3),(B,3); 当计算N级子孙数的时候，构造一个(节点ID，节点等级+N)的RDD； 两个RDD在join之后（以节点ID为key），过滤出两个RDD的value（对应父链中节点所在等级，和需要计算的等级）相等的数据； 过滤后map掉无用信息，再count就是所要求的等级上的子孙数。 以开始的图为例，计算2级子孙数，所有父链拆分后拿到的结果是：\n(A,2), (A,2), (A,3),(B,3), (A,3),(B,3), (A,3),(B,3), (A,4),(B,4),(F,4), (A,4),(B,4),(F,4), (A,5),(B,5),(F,5),(G,5), (A,6),(B,6),(F,6),(G,6),(I,6) 构造(节点ID，节点等级+2)的RDD：\n(A,3), (B,4), (C,4), (D,5), (E,5), (F,5), (G,6), (H,6), (I,7), (J,8) 两个RDD在join之后（join的结果太多，不列出了），过滤出两边等级相等的数据：\n(A,3,3),(A,3,3),(A,3,3), (B,4,4),(B,4,4), (F,5,5), (G,6,6) map掉无用的等级，再count得到：\n(A,3),(B,2),(F,1),(G,1) 即只有这些节点有二级子孙，二级子孙个数也拿到了。\nSpark代码(scala) //全部MySQL数据读到RDD中，格式为(用户ID,(*,*,*,*,等级,父节点ID,父链,*)) val rsRDD = sc.makeRDD(readAllMySQLData()).cache() //统计总子孙数 val totalList = rsRDD.flatMap(r =\u0026gt; r._2._7.split(\u0026#34;-\u0026#34;)).filter(p =\u0026gt; p != null \u0026amp;\u0026amp; p != \u0026#34;\u0026#34;).countByValue() //统计二级子孙数 //切分父链 val secondList = rsRDD.flatMap(r =\u0026gt; { val values = r._2._7.split(\u0026#34;-\u0026#34;, -1) values.filter(p =\u0026gt; p!=null \u0026amp;\u0026amp; p.length \u0026gt; 0).map(userid =\u0026gt; { (Integer.parseInt(userid), r._2._5) }) }) //构造(节点ID，节点等级+2)的RDD val secondUser = rsRDD.map(p =\u0026gt; (p._1, p._2._5 + 2)) //join之后，过滤出两边等级相等的数据，map掉无用信息，再count val secondShare = secondUser.join(secondList).filter(p =\u0026gt; p._2._1 == p._2._2).map(_._1).countByValue() ……………… ","date":"2017-04-26T15:29:19+08:00","permalink":"https://leibnizhu.github.io/p/Spark%E5%BF%AB%E9%80%9F%E7%BB%9F%E8%AE%A1%E6%A0%91%E7%8A%B6%E5%85%B3%E7%B3%BB%E5%90%84%E5%B1%82%E6%95%B0%E9%87%8F%E7%9A%84%E4%B8%80%E7%A7%8D%E7%AE%97%E6%B3%95/","title":"Spark快速统计树状关系各层数量的一种算法"},{"content":"安装Gitlab-CI-Runner 下载 根据官方的说法，Gitlab-CI-Runner9.0以后的版本需要GitLab9.0以上版本支持，我们目前部署的GitLab是8.x，所以需要下载旧版。\n最新版：\nsudo wget -O /usr/local/bin/gitlab-runner https://gitlab-ci-multi-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-ci-multi-runner-linux-amd64 旧版（如v1.11.0）：\nsudo wget -O /usr/local/bin/gitlab-runner https://gitlab-ci-multi-runner-downloads.s3.amazonaws.com/v1.11.0/binaries/gitlab-ci-multi-runner-linux-amd64 安装配置 sudo chmod +x /usr/local/bin/gitlab-runner sudo useradd --comment \u0026#39;GitLab Runner\u0026#39; --create-home gitlab-runner --shell /bin/bash sudo gitlab-runner register 执行上面最后一句命令的时候，会要求输入网址和秘钥，此时打开Gitlab页面，进项目，右上角配置按钮-Runners，再按此时页面上给出的来填。\n还会要求输入名称和标签之类的信息，到最后会提示输入运行环境之类，我们Gitlab是在docker上，但Runner和Gitlab在同一个docker容器中的，就是在Gitlab调用的角度上来看，Runner并不是在docker中，所以运行环境那里选shell就行。\n运行 sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner sudo gitlab-runner start 配置项目的CI脚本 在项目根目录创建文件.gitlab-ci.yml，写入CI执行的脚本，具体参考官方文档\n此处以最简单的maven打包部署tomcat为例：\nstages: - deploy deploy: stage: deploy only: - dev script: - mvn clean - mvn package - cp target/FissionSales.war /var/opt/gitlab/webapps/ 此处限定了dev分支的提交才会触发CI任务，打包后复制到指定文件夹里。\n此外，我们将打包成功的war包复制到/var/opt/gitlab/webapps/中，这样做是因为，Gitlab在docker中，而Tomcat在宿主机里，因为权限方面的问题，我只好让Gitlab的CI任务将war包放在docker的volume（已经配置了/var/opt/gitlab/的volume）中，然后在宿主机中通过定时任务，检查war包的版本，检查到新版本时复制到宿主机的Tomcat中进行部署，具体的检查脚本如下：\n#!/bin/bash a=`sudo stat -c %Y /var/lib/docker/volumes/gitlab-data/_data/webapps/***.war` b=`date +%s` b=$[b-a] echo $b if [ $b -le 60 ]; then sudo cp /var/lib/docker/volumes/gitlab-data/_data/webapps/***.war /enviroment/apache-tomcat-8.0.33/webapps/ else echo \u0026#34;No new war package...\u0026#34; fi 这个脚本通过crontab定时每分钟执行，所以检查war包的修改时间与当前时间相差小于60秒就会复制war包到tomcat的webapps中。\n配置Pipeline邮件通知 Gitlab默认CI Pipeline任务成功失败都会发邮件通知，这样或许会困扰到大家，所以修改Gitlab的源码，只让部署不成功的时候才发邮件通知。\n进入docker，编辑/opt/gitlab/embedded/service/gitlab-rails/app/services/notification_service.rb，在pipeline_finished方法的开头添加return if pipeline.status == \u0026quot;success\u0026quot; ，如：\ndef pipeline_finished(pipeline, recipients = nil) return if pipeline.status == \u0026#34;success\u0026#34; email_template = \u0026#34;pipeline_#{pipeline.status}_email\u0026#34; ………………………… ","date":"2017-04-26T15:02:26+08:00","image":"https://leibnizhu.github.io/p/Docker%E4%B8%ADGitlab%E7%9A%84%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/docker-gitlab_hu81c9fe789874452904ea5af57acbdd42_12216_120x120_fill_box_smart1_3.png","permalink":"https://leibnizhu.github.io/p/Docker%E4%B8%ADGitlab%E7%9A%84%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","title":"Docker中Gitlab的持续集成安装与配置"},{"content":"问题 我们用多台云服务器搭建了HDP的hadoop集群，为了方便测试，在本地用virtualbox虚拟机搭建了一个架构完全一样的集群。为了测试程序，训练模型，本地的集群的数据也需要与云服务器上集群的一样。\nMySQL数据库可以通过主从备份实时同步，而HBase数据在配置同步的过程中就遇到了问题，常规的方法无法完成同步。\n常规方法 HBase可以设置备份，然而只能在同一个内网，而我们不想搭建vpn。\n然后是借助sqoop之类的工具，我们尝试对zookeeper等相关使用的端口开启内网映射，然而还是无法从外网访问，检查了iptables，没发现问题，原因未知。\n最终方案 最后采用了最暴力的方案，就是使用HBase自带的export和import功能，把云端HBase整个表导出到文件再导入到本地集群HBase，缺点是慢，因为是通过mapReduce操作完成的，数据量大的时候尤其慢。而且不知为何，无法直接导出到本地文件系统，只能通过HDFS文件系统中转，也就是说，整个同步流程是：\n云端HBase数据库表export到云端HDFS 云端HDFS的导出文件导出到云端Linux文件系统 本地集群通过scp下载云端的导出文件（因为安全问题，这里还分了两部，先scp下载到我的电脑，再scp上传到本地集群） 本地集群HDFS导入下载到的备份文件 本地集群HBase数据库import备份文件 整个过程比较麻烦，所以我写了个脚本，通过crontab每15分钟定时执行，脚本内容如下：\n#!/bin/bash ssh 服务器名 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026lt;\u0026lt; eeooff set HADOOP_USER_NAME=hdfs rm -rf /root/share hadoop fs -rm -r -f -skipTrash /backup/表名 whoami /usr/hdp/current/hbase-client/bin/hbase org.apache.hadoop.hbase.mapreduce.Driver export \u0026#39;表名\u0026#39; /backup/表名 hadoop fs -get /backup/表名 ~/bakcup exit eeooff rm -rf /home/***/tmp/backup scp -r 服务器名:/root/backup/ /home/***/tmp/ scp -r /home/***/tmp/backup 本地集群主机名:/home/hdfs/ ssh 本地集群主机名 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026lt;\u0026lt; eeooff set HADOOP_USER_NAME=hdfs su - hdfs hadoop fs -rm -r -f -skipTrash /backup/表名 hadoop fs -put /home/hdfs/backup /test /usr/hdp/current/hbase-client/bin/hbase org.apache.hadoop.hbase.mapreduce.Driver import \u0026#39;SHARE_CHAIN\u0026#39; /backup/表名 eeooff ","date":"2017-04-22T19:34:10+08:00","permalink":"https://leibnizhu.github.io/p/HBase%E5%A4%96%E7%BD%91%E5%90%8C%E6%AD%A5%E8%84%9A%E6%9C%AC/","title":"HBase外网同步脚本"},{"content":"简介 最近使用Spark的GraphX进行一些图计算，GraphX要求每个节点都有唯一的ID；但我们的数据并没有包含唯一的ID，所以需要使用Hash函数将每条数据的信息进行摘要生成唯一ID。\nHash中，String自己的hashCode()自然是很烂的(字符串每个字符乘以质数再与之前hash相加，如此迭代)，MD5/SHA1之类的算法开销又比较大，Spark图运算的时候节点较多，hash的开销还是蛮可观的。\n经过搜索，发现了MurmurHash算法，具体参考 维基百科 ，总而言之就是高效低碰撞，hadoop/memcached之类都在用。\nScala API自身是有MurmurHash算法的实现的（scala.util.hashing.MurmurHash3），但返回值是int，32位。对于海量数据而言，显然不够用，我们需要64位的。\n于是就写了个scala实现的64位MurmurHash函数，详见下文（没有测试过具体的碰撞性能）。\nscala实现 object MurmurHash64 { def stringHash(str: String): Long = { val data = str.getBytes val length = data.length val seed = 0xe17a1465 val m = 0xc6a4a7935bd1e995L val r = 47 var h = (seed \u0026amp; 0xffffffffL) ^ (length * m) val length8 = length / 8 for (i \u0026lt;- 0 until length8) { val i8 = i * 8 var k = (data(i8 + 0) \u0026amp; 0xff).toLong + ((data(i8 + 1) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 8) + ((data(i8 + 2) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 16) + ((data(i8 + 3) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 24) + ((data(i8 + 4) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 32) + ((data(i8 + 5) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 40) + ((data(i8 + 6) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 48) + ((data(i8 + 7) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 56) k *= m k ^= k \u0026gt;\u0026gt;\u0026gt; r k *= m h ^= k h *= m } if (length % 8 \u0026gt;= 7) h ^= (data((length \u0026amp; ~7) + 6) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 48 if (length % 8 \u0026gt;= 6) h ^= (data((length \u0026amp; ~7) + 5) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 40 if (length % 8 \u0026gt;= 5) h ^= (data((length \u0026amp; ~7) + 4) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 32 if (length % 8 \u0026gt;= 4) h ^= (data((length \u0026amp; ~7) + 3) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 24 if (length % 8 \u0026gt;= 3) h ^= (data((length \u0026amp; ~7) + 2) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 16 if (length % 8 \u0026gt;= 2) h ^= (data((length \u0026amp; ~7) + 1) \u0026amp; 0xff).toLong \u0026lt;\u0026lt; 8 if (length % 8 \u0026gt;= 1) { h ^= (data(length \u0026amp; ~7) \u0026amp; 0xff).toLong h *= m } h ^= h \u0026gt;\u0026gt;\u0026gt; r h *= m h ^= h \u0026gt;\u0026gt;\u0026gt; r h } } ","date":"2017-01-19T11:20:52+08:00","permalink":"https://leibnizhu.github.io/p/Scala%E5%AE%9E%E7%8E%B064%E4%BD%8D%E7%9A%84MurmurHash%E5%87%BD%E6%95%B0/","title":"Scala实现64位的MurmurHash函数"},{"content":"背景 同一个项目分配了多个域名，在其中一个域名（下称域名A）的一级域名上放了Cookie。\n使用其他域名(下面统称域名B*)去访问某些页面时，需要使用js读取域名A下的那个Cookie。\n已有的解决方案 网上已经有一些解决方案，如：JS跨域（ajax跨域、iframe跨域）解决方法及原理详解（jsonp）、JS 获取跨域的cookie。\n其中document.domain的方法已确认不可用于我的需求，iframe跨域可以实现，但是跨域后的页面在iframe中，此时js变量的作用域只在iframe中，需要通过一个中间元素的值或者属性来写入需要传递的值，比较麻烦；document.name的方法虽然可以跨域，但同时也跨页面了，处理起来要小心点。\n本文的解决方案 原理 在域名B*的页面上，js的确不能直接获取到域名A的cookie，但显然，域名B*的页面如果发请求到域名A，会带上域名A的Cookie。\n针对这一点，只要我们在域名A上面部署一个微服务，域名B*的页面发AJAX请求到这个服务，返回域名A的Cookie中我们感兴趣字段的值。域名B*的页面就能接收到域名A的Cookie，可以各种利用了。\nNetty实现 域名A的服务器端选用Netty实现。实现代码很简单，这里只给出核心Handler部分代码：\npublic class RPCMissionHandler extends SimpleChannelInboundHandler\u0026lt;FullHttpRequest\u0026gt; { private static Logger LOG = Logger.getLogger(RPCMissionHandler.class); @Override public void channelRead0(ChannelHandlerContext ctx, FullHttpRequest req) throws Exception { if(!req.decoderResult().isSuccess()){ sendError(ctx, BAD_REQUEST); return; } if(req.method() != GET){ sendError(ctx, METHOD_NOT_ALLOWED); return; } String uri = req.uri(); String paramUri = uri.substring(uri.indexOf(\u0026#34;?\u0026#34;) + 1); List\u0026lt;NameValuePair\u0026gt; params = URLEncodedUtils.parse(paramUri, Charset.forName(\u0026#34;UTF-8\u0026#34;)); if(uri.startsWith(\u0026#34;/getid\u0026#34;)){ String refererDomain = null; for(NameValuePair pair : params){ if(\u0026#34;domain\u0026#34;.equals(pair.getName())){ refererDomain = pair.getValue(); } } if(refererDomain != null){ if(req.headers().get(\u0026#34;Cookie\u0026#34;) != null) { Set\u0026lt;Cookie\u0026gt; cookies = CookieDecoder.decode(req.headers().get(\u0026#34;Cookie\u0026#34;)); for (Cookie cookie : cookies) { if (\u0026#34;ID\u0026#34;.equals(cookie.name())) { LOG.info(\u0026#34;Request ID = \u0026#34; + cookie.value()); responseWithString(ctx, cookie.value(), \u0026#34;http://\u0026#34; + refererDomain); return; } } } else { responseWithString(ctx, \u0026#34;\u0026#34;, \u0026#34;http://\u0026#34; + refererDomain); return; } } } responseWithImage(ctx); } private void responseWithString(ChannelHandlerContext ctx, String data, String referer) { FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, OK); /*配置不缓存*/ resp.headers().set(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache\u0026#34;); resp.headers().set(\u0026#34;Pragma\u0026#34;, \u0026#34;no-cache\u0026#34;); resp.headers().set(\u0026#34;Expires\u0026#34;, \u0026#34;Wed, 31 Dec 1969 23:59:59 GMT\u0026#34;); /*配置跨域允许*/ resp.headers().set(\u0026#34;Access-Control-Allow-origin\u0026#34;, referer); resp.headers().set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;GET, POST\u0026#34;); resp.headers().set(\u0026#34;Access-Control-Allow-Credentials\u0026#34;, \u0026#34;true\u0026#34;); /*响应类型*/ resp.headers().set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html;charset=UTF-8\u0026#34;); ByteBuf buf = Unpooled.copiedBuffer(new StringBuffer(data), CharsetUtil.UTF_8); resp.content().writeBytes(buf); buf.release(); ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE); } private void sendError(ChannelHandlerContext ctx, HttpResponseStatus status) { FullHttpResponse resp = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, status, Unpooled.copiedBuffer(\u0026#34;Failure:\u0026#34; + status, CharsetUtil.UTF_8)); resp.headers().set(CONTENT_TYPE, \u0026#34;text/plain; charset=UTF-8\u0026#34;); ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.writeAndFlush(Unpooled.EMPTY_BUFFER); super.channelReadComplete(ctx); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { LOG.error(\u0026#34;抛出异常\u0026#34;, cause); super.exceptionCaught(ctx, cause); } } 唯一需要注意的是，在请求的时候，通过domain参数带上了域名B*，然后在响应的时候，配置跨域相关的一些HTTP响应头：\nresp.headers().set(\u0026#34;Access-Control-Allow-origin\u0026#34;, referer); resp.headers().set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;GET, POST\u0026#34;); resp.headers().set(\u0026#34;Access-Control-Allow-Credentials\u0026#34;, \u0026#34;true\u0026#34;); 其中referer就是域名B*。\n页面js调用 js发AJAX请求这块就更简单了。唯一注意的是，我们这个Cookie字段可能在页面加载后一段时间才能获取到，所以这里设置了重试机制，获取不到ID的时候等待1秒后重新发送一次请求，一共最多请求5次。\n这里发送AJAX请求也涉及到一些跨域的配置，详见注释\n$(window).load(function(){ setTimeout(function(){getID(5);}, 1000); function getID(times){ $.ajax({ url: \u0026#39;http://A.com/getid?domain=\u0026#39;+document.domain, type: \u0026#39;get\u0026#39;, crossDomain: true, /*允许跨域*/ xhrFields: { withCredentials: true }, /*允许跨域*/ error: function(data){alert(data);}, success: function (data) { if(data != \u0026#34;\u0026#34;){ ID = data; } else { if(--times \u0026gt; 0) setTimeout(function(){getID(times);}, 1000); } } }); } }); ","date":"2017-01-12T14:01:25+08:00","permalink":"https://leibnizhu.github.io/p/js%E8%B7%A8%E5%9F%9F%E8%8E%B7%E5%8F%96Cookie%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%96%B9%E6%B3%95/","title":"js跨域获取Cookie的一种新方法"},{"content":"put方法： put方法调用私有方法\nputVal(hash(key),key,value,false,true); 先去计算hash。 key非空的时候返回：\nh=key.hashCode())^(h\u0026gt;\u0026gt;\u0026gt;16) 即高16位不变，低16位与高16位做异或运算，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的(使用\u0026amp;位操作，而非%求余)：(n - 1) \u0026amp; hash 。设计者认为这方法很容易发生碰撞。在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。因此，综合考虑了速度、作用、质量，把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。\n然后进入putVal()。 首先判断table是否为空（null或长度为0），为空的话进行resize()。\nresize() 如果原来table大小已经超过上限，则不resize，直接返回原来table； 原来table大小不为0且未超上限则容量增倍，threshold（扩容阈值）也增倍； 原来table大小为0，则大小设为DEFAULT_INITIAL_CAPACITY=16，threshold设为DEFAULT_LOAD_FACTOR*DEFAULT_INITIAL_CAPACITY=12。 然后按新的table大小，new一个Node数组。此时如果旧的table不为空，则需要进行原有数据的转移。\nif(oldTab!=null){ for(intj=0;j\u0026lt;oldCap;++j){ Node\u0026lt;K,V\u0026gt;e; if((e=oldTab[j])!=null){ oldTab[j]=null; if(e.next==null) newTab[e.hash\u0026amp;(newCap-1)]=e; elseif(e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this,newTab,j,oldCap); else{//preserveorder Node\u0026lt;K,V\u0026gt;loHead=null,loTail=null; Node\u0026lt;K,V\u0026gt;hiHead=null,hiTail=null; Node\u0026lt;K,V\u0026gt;next; do{ next=e.next; if((e.hash\u0026amp;oldCap)==0){ if(loTail==null) loHead=e; else loTail.next=e; loTail=e; }else{ if(hiTail==null) hiHead=e; else hiTail.next=e; hiTail=e; } }while((e=next)!=null); if(loTail!=null){ loTail.next=null; newTab[j]=loHead; } if(hiTail!=null){ hiTail.next=null; newTab[j+oldCap]=hiHead; } } } } } 大致意思就是遍历旧表的元素： 1.如果旧表元素的next为空（没有发生冲突而放入链表），则计算hash放进新表对应位置：newTab[旧表元素.hash\u0026amp;(newCap-1)]=旧表元素（newCap为新的table长度）; 2.若旧表元素为TreeNode实例，即该节点使用了红黑树进行存储的（JDK8开始引入），则执行split方法去处理； 3.否则遍历旧表元素的对应链表，重新计算位置。 resize的时候，因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂（oldCap）的位置，例如我们从16扩展为32时，具体的变化如下所示： 因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色，0或1)。 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了（通过(e.hash\u0026amp;oldCap)==0进行判断），是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 既省去了重新计算hash值的时间，而且由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 上面代码中else{//preserveorder之后的部分即完成了上述过程，将一个节点上的链表拆分成用loHead,loTail和hiHead,hiTail 描述的两个链表，分别对应新表中位置不变的节点，和移动oldCap之后的节点。\n新建节点或更新value 然后判断当前hash值对应table的节点是否为空，为空的话直接新建节点即可：new Node\u0026lt;\u0026gt;(hash,key,value,null);最后一个参数是next，因为table原节点为空，为链表第一个元素，所以next设为null即可； 如果hash值对应table节点不为空，则判断原节点和当前插入的数据key及value是否都一致，如果一致，证明是同一个节点，无需重新插入； 否则进入table节点的链表，遍历，如果找到与待插入节点一样的节点，则直接退出，否则一直找到链表末端节点还没找到相同的，则增加新节点插入当前数据，如果当前链表长度大于TREEIFY_THRESHOLD-1，还需要进行treeifyBin()操作，将链表转换为红黑树，提高查询效率（O(n)变为O(logn)，JDK8之后引入的优化）。 如果以上操作中找到待插入节点的key在map中已存在，则用新数据覆盖之，最后size++, 判断table尺寸，看是否需要进行resize()。\nget方法： get时调用getNode(int hash, Object key)方法。 首先table为null或长度为0或对应hash位置的元素为null均返回null。 否则先判断hash位置上的元素key和get方法的key相同，如果相同则直接返回hash位置的元素；否则判断hash位置节点是否为TreeNode，若是则调用getTreeNode方法进行处理并返回；对于非TreeNode节点，且hash位置节点的key不等于get方法dekey的话，则遍历hash位置节点的链表，直到找到key相同的节点并返回节点的value。\n","date":"2016-12-24T16:48:04+08:00","permalink":"https://leibnizhu.github.io/p/HashMap%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"HashMap源码阅读笔记"},{"content":"背景 最近给以前写的一个刷流量的爬虫/工具增加动态切换代理的功能。\n这个工具基于Java开发，使用了HtmlUnit库模拟浏览器进行访问，并加载页面中所有的JS以触发第三方监控。其实可以抓第三方监控的请求，然后再直接模拟这些请求，但这样就缺乏通用性，即不同的网站需要单独针对抓请求分析，所以选择了直接模拟浏览器加载JS的方法。\n而最近新需求要求增加使用代理模拟多地访问的功能，所以：\n先是写了个简单的Python爬虫爬各种免费代理的网站，一一验证可用性并放入数据库中备用； 而后发现这样抓到的代理很少，一直维持50个左右可用的，而且经常返回403错误，不能满足刷流量工具的需求，所以购买了某收费代理； 在使用收费代理的过程中，发现通过很多宣称高可靠的代理进行访问时依然返回403错误码，结果是大量代理不可用。 问题分析 首先，Python抓回来的代理是经过Python的Requests库检验的，确认可以通过其进行网页访问才放入数据库的；而购买的代理宣称高可靠的也是有自己的检测的，而且可以看到他们的检验时间跟我的使用时间一般只相差几分钟，应该不会有大量代理在这几分钟之内失效。\n所以怀疑是HtmlUnit的问题。HtmlUnit库可以选择使用不同的浏览器类型/版本（根据官网文档，这些不同的浏览器的区别是JS的解析方法和User-Agent、HTTP请求头），我们首先尝试了所有HtmlUnit支持的浏览器类型/版本，发现还是有大量的代理在使用时返回403错误码。\n再思考为什么会返回403，要么就是请求头有问题，要么我们的IP被封了。但我们的IP应该没干过什么坏事，不会轻易被列入黑名单吧，所以还是往请求头的方向去想，毕竟爬虫的话出问题也经常是因为请求头有问题，异于常规的访问。所以我们尝试了很多User-Agent，发现还是不行。\n最后考虑到抓包分析。直接用WireShark进行抓包，筛选HTTP包，分别截取Python的Requests库和Java的HtmlUnit去使用同一个代理访问同一个网页的请求包，如下。 Python的Requests库使用代理访问产生的HTTP请求包：\nHypertext Transfer Protocol GET http://www.***.com/test/index.htm HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): GET http://www.***.com/test/index.htm HTTP/1.1\\r\\n] [GET http://www.***.com/test/index.htm HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: GET Request URI: http://www.***.com/test/index.htm Request Version: HTTP/1.1 Host: www.***.com\\r\\n Connection: keep-alive\\r\\n Accept-Encoding: gzip, deflate\\r\\n Accept: */*\\r\\n User-Agent: python-requests/2.12.2\\r\\n \\r\\n [Full request URI: http://www.***.com/test/index.htm] [HTTP request 1/1] 而HtmlUnit产生的包：\nHypertext Transfer Protocol GET http://www.***.com/test/index.htm HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): GET http://www.***.com/test/index.htm HTTP/1.1\\r\\n] [GET http://www.***.com/test/index.htm HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: GET Request URI: http://www.***.com/test/index.htm Request Version: HTTP/1.1 Host: www.***.com\\r\\n User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36\\r\\n Accept: */*\\r\\n Accept-Language: en-US\\r\\n Accept-Encoding: gzip, deflate\\r\\n Proxy-Connection: keep-alive\\r\\n \\r\\n [Full request URI: http://www.***.com/test/index.htm] [HTTP request 1/1] 可以看到，Requests使用的User-Agent是python-requests，竟然也没被拦截，而HtmlUnit使用真实浏览器的User-Agent却返回403，看来跟User-Agent无关；事实上，我们尝试将HtmlUnit的User-Agent设置为与Requests一样，再去访问，一样得到403。\n然后仔细看HTTP请求头，不难发现，唯一的区别在于python-requests使用了一个Connection的字段，这个大家都比较熟悉，是要求远程服务器启用长连接的；而HtmlUnit没有Connection字段，取而代之的是Proxy-Connection字段，明显与代理有关。那么问题出在这个HTTP请求头字段上面？？？\nProxy-Connection是什么 通过Google我们发现Proxy-Connection是为了兼容不同HTTP协议版本而产生的一个字段，可以参考这篇文章：\nHttp 请求头中的 Proxy-Connection\n大概意思就是如果使用代理服务的时候，还是发送Connection字段，那么一些旧的代理服务器不能处理这个字段而直接转发给目标服务器，最后目标服务器可能同意开启长连接，而代理服务器却没有正确地响应建立长连接，最后导致不能正常访问；所以才在使用代理的时候，使用Proxy-Connection替代Connection，让远程服务器区分对待。\n总而言之，使用代理服务器的时候，请求头使用Proxy-Connection字段是符合HTTP协议的。也就是说其实HtmlUnit的处理是正确的，而Requests的处理反而是不符合HTTP协议的？\nHtmlUnit中去除Proxy-Connection字段 不管谁对谁错，就目前状况而言，似乎使用了Proxy-Connection就是会让代理服务器返回403，或许是因为这些代理服务器不希望开启长连接？（毕竟很多代理服务器隔一小段时间就要切换端口之类的，比较被人长时间占用），所以我们的目标变成使用HtmlUnit发送请求的时候去除HTTP请求头的Proxy-Connection字段。\nremoveRequestHeader() HtmlUnit的核心类WebClient有方法\nremoveRequestHeader(String name) 可以删除请求头的字段。不过经过尝试，调用了\nremoveRequestHeader(\u0026#34;Proxy-Connection\u0026#34;) 之后，发出的请求还是带有Proxy-Connection字段。\nWebRequest.removeAdditionalHeader() 于是尝试别的方法，在访问页面的时候再删除请求头：\nWebRequest request = new WebRequest(new URL(\u0026#34;**********\u0026#34;)); request.removeAdditionalHeader(\u0026#34;Proxy-Connection\u0026#34;); request.setAdditionalHeader(\u0026#34;Connection\u0026#34;, \u0026#34;Keep-Alive\u0026#34;); HtmlPage page1 = webClient.getPage(request); 依然不行。\n查看源码 查看了HtmlUnit源码（在此不贴出来了），发现我们调用其方法处理Header，所处理的只不过是一个额外的Map，不管增加和删除都是在这个Map里面处理的，最后的Header是这个额外的Map和预定义的一些Header结合在一起。但是在源码中也没有发现Proxy-Connection字段的处理（如判断当前使用代理，则用Proxy-Connection字段取代Connection字段等逻辑）。\n仔细观察，其实HtmlUnit底层是调用了JDK的sun.net.www.protocol.http.HttpURLConnection 进行访问的，我们再看HttpURLConnection的源码，发现有两处：\nprivate void writeRequests() throws IOException { ………… if(this.http.usingProxy \u0026amp;\u0026amp; this.tunnelState() != HttpURLConnection.TunnelState.TUNNELING) { this.requests.setIfNotSet(\u0026#34;Proxy-Connection\u0026#34;, \u0026#34;keep-alive\u0026#34;); } else { this.requests.setIfNotSet(\u0026#34;Connection\u0026#34;, \u0026#34;keep-alive\u0026#34;); } ………… } private void sendCONNECTRequest() throws IOException { ………… if(this.http.getHttpKeepAliveSet()) { this.requests.setIfNotSet(\u0026#34;Proxy-Connection\u0026#34;, \u0026#34;keep-alive\u0026#34;); } ………… } 原来是在这里面处理的！！！！\n编译rt.jar sun.net.www.protocol.http.HttpURLConnection 是在JRE的rt.jar中的核心类，不能随便修改。 所以我们提取出HttpURLConnection，反编译，注释掉Proxy-Connection相关语句，重新编译，替换rt.jar中的响应class文件：\n重新测试，发现发出的请求还是带有Proxy-Connection字段！！！！\nPython验证 从另一个角度来验证：对于HtmlUnit使用会返回403的代理，在Python的Requests中使用，并在HTTP请求头加上Proxy-Connection字段。Python代码比较简单，执行时输入需要验证的IP和端口就行：\n#coding:utf-8 import requests import json import sys ip = sys.argv[1] port = sys.argv[2] header ={\u0026#39;Proxy-Connection\u0026#39;:\u0026#39;Keep-Alive\u0026#39;} proxies={ \u0026#39;http\u0026#39;:\u0026#39;http://%s:%s\u0026#39;%(ip,port), \u0026#39;https\u0026#39;:\u0026#39;http://%s:%s\u0026#39;%(ip,port) } r = requests.get(\u0026#39;http://www.***.com/test/index.htm\u0026#39;,proxies=proxies,headers=header) r.encoding=\u0026#39;utf-8\u0026#39; print r.text ~ 果然，控制台输出“Access not allowed!”。将“headers=header”去掉之后就能正常输出页面内容。\n因此可以确认是Proxy-Connection字段导致的问题。\n目前结论 对于某些代理服务器，使用时请求头不能有Proxy-Connection字段； 在家里跑这个工具，却基本没有返回403错误…………看来还有可能是办公室IP被列入黑名单了…… 对于Java开发，要选择不适用sun的HttpUrlConnection的html测试工具/模拟浏览器了；看了Selenium的源码，没有用这个类，准备试下。 ","date":"2016-12-04T09:39:02+08:00","permalink":"https://leibnizhu.github.io/p/HtmlUnit%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E6%97%B6%E5%AF%B9HTTP%E8%AF%B7%E6%B1%82%E5%A4%B4Proxy-Connection%E5%AD%97%E6%AE%B5%E7%9A%84%E6%8E%A2%E7%A9%B6/","title":"HtmlUnit爬虫使用代理时对HTTP请求头Proxy-Connection字段的探究"},{"content":"日志系统 简介 Spring Boot默认使用的Apache的Common Logging日志系统，但同时也提供了Java Util Logging, Log4J, Log4J2和Logback等日志系统的支持（无需额外增加依赖）。\n日志格式 Spring Boot默认输出的日志各列含义如下：\n日期和时间 - 精确到毫秒，且易于排序。 日志级别 - ERROR, WARN, INFO, DEBUG 或 TRACE。 Process ID。 一个用于区分实际日志信息开头的\u0026mdash;分隔符。 线程名 - 包括在方括号中（控制台输出可能会被截断）。 日志名 - 通常是源class的类名（缩写）。 日志信息。 如：\n2014-03-05 10:57:51.112 INFO 45469 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat/7.0.52 2014-03-05 10:57:51.253 INFO 45469 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2014-03-05 10:57:51.253 INFO 45469 --- [ost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1358 ms 2014-03-05 10:57:51.698 INFO 45469 --- [ost-startStop-1] o.s.b.c.e.ServletRegistrationBean : Mapping servlet: \u0026#39;dispatcherServlet\u0026#39; to [/] 2014-03-05 10:57:51.702 INFO 45469 --- [ost-startStop-1] o.s.b.c.embedded.FilterRegistrationBean : Mapping filter: \u0026#39;hiddenHttpMethodFilter\u0026#39; to: [/*] 可以在resources/application.properties中使用logging.pattern.console和logging.pattern.file属性进行配置。\n日志配置 可以在resources/application.properties 中进行配置，如下：\n###日志配置### #日志输出级别 logging.level.root=INFO logging.level.com.turingdi.dmp=DEBUG #检查终端是否支持ANSI，是的话就采用彩色输出 spring.output.ansi.enabled=DETECT #设置文件，可以是绝对路径，也可以是相对路径 #logging.file= #设置目录，会在该目录下创建spring.log文件，并写入日志内容 #logging.path= #定义输出到控制台的样式（不支持JDK Logger） logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} [%-5level] %logger{36}[%line]=\u0026gt; %msg%n #定义输出到文件的样式（不支持JDK Logger） #logging.pattern.file= 日志API调用 类似Log4j：\nimport org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; public class TemplateController { private static Log log = LogFactory.getLog(TemplateController.class); public String index(ModelMap map) { …… log.info(\u0026#34;hahahaha\u0026#34;);//还有warn\\error\\debug等方法 …… } } 其他相关网站 Spring Boot相关博客：\nhttp://blog.didispace.com/categories/Spring-Boot/\nThymeleaf相关博客：\nhttp://www.cnblogs.com/vinphy/p/4674247.html\nhttp://www.jianshu.com/p/ed9d47f92e37\n","date":"2016-11-26T16:00:08+08:00","permalink":"https://leibnizhu.github.io/p/Spring-Boot%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%9B%9B%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/","title":"Spring Boot快速入门（四）——日志系统"},{"content":"使用Thymeleaf模板引擎 Thymeleaf简介 Thymeleaf是一个XML/XHTML/HTML5模板引擎，可用于Web与非Web环境中的应用开发。它是一个开源的Java库，基于Apache License 2.0许可。\nThymeleaf提供了一个用于整合Spring MVC的可选模块，在应用开发中，你可以使用Thymeleaf来完全代替JSP或其他模板引擎，如Velocity、FreeMarker等。Thymeleaf的主要目标在于提供一种可被浏览器正确显示的、格式良好的模板创建方式，因此也可以用作静态建模 。你可以使用它创建经过验证的XML与HTML模板。相对于编写逻辑或代码，开发者只需将标签属性添加到模板中即可。\nThymeleaf主要通过HTML的标签属性渲染标签内容，浏览器在解析html时，当检查到Thymeleaf的属性时候会忽略，所以Thymeleaf的模板可以通过浏览器直接打开展现，这样非常有利于前后端的分离 。\n添加依赖 添加模板引擎的依赖，可以在IntelliJ IDEA创建Spring Boot项目的时候选择对应的依赖，也可以在后期手动修改pom.xml文件增加依赖。\nIntelliJ IDEA创建Spring Boot项目时增加 在选择依赖的界面，点击左边的“Template Engine”，在中间选择所需的模板引擎即可。\n手动修改pom.xml 以Thymeleaf为例，修改pom.xml增加以下依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-thymeleaf\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 可在resources/application.properties中对Thymeleaf进行配置。\n目录结构 Thymeleaf默认将模板放在resources/templates目录下（可以通过application.properties文件进行配置，但建议保持默认值方便管理）；同时，Spring Boot默认将静态资源放在resources/static（从根路径访问），于是，经典的目录结构是这样的：\n其中640-100.jpg的访问路径为http://127.0.0.1:8080/640-100.jpg。\n模板Demo 在resources/templates目录下新建temp.html，内容如下：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 th:text=\u0026#34;${host}\u0026#34;\u0026gt;Hello World\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 其中h1的th:text=\u0026quot;${host}\u0026quot; 即为Thymeleaf的属性，表示获取ModelMap中的host属性赋值给h1的文本；而这个文件直接用浏览器访问的时候h1元素则显示“Hello World”。 再编写一个Controller：\n@Controller @RequestMapping(\u0026#34;/template\u0026#34;) public class TemplateController { @RequestMapping(\u0026#34;/\u0026#34;) public String index(ModelMap map) { // 加入一个属性，用来在模板中读取 map.addAttribute(\u0026#34;host\u0026#34;, \u0026#34;http://www.turingdi.com\u0026#34;); // return模板文件的名称，对应resources/templates/temp.html return \u0026#34;temp\u0026#34;; } } 注意这里使用了@Controller注解而不是之前的@RestController，若使用后者，index()方法返回的”temp”会直接以JSON格式返回，页面显示“temp”。实际上我们使用@Controller注解并加入了Thymeleaf模板引擎后，index()方法返回的”temp”会被Thymeleaf模板引擎理解为src/main/resources/templates/temp.html文件，然后解析该文档并响应返回。在浏览器中接收到的html源码如下：\n而这个模板文件直接用浏览器打开的效果：\n可以看到，前端开发人员可以直接修改html并直接观察修改后的效果，修改时并不影响Thymeleaf的代码，因此可以方便前后端协同开发。\nThymeleaf简单表达式 变量表达式 ${……} \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;userName\u0026#34; value=\u0026#34;James Carrot\u0026#34; th:value=\u0026#34;${user.name}\u0026#34; /\u0026gt; 上述代码为引用user对象的name属性值。\n选择/星号表达式 *{……} \u0026lt;div th:object=\u0026#34;${session.user}\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Nationality: \u0026lt;span th:text=\u0026#34;*{nationality}\u0026#34;\u0026gt;Saturn\u0026lt;/span\u0026gt;. \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; 选择表达式一般跟在th:object后，直接取object中的属性。\n文字国际化表达式 #{……} \u0026lt;p th:utext=\u0026#34;#{home.welcome}\u0026#34;\u0026gt;Welcome to our grocery store!\u0026lt;/p\u0026gt; URL表达式 @{……} \u0026lt;a href=\u0026#34;details.html\u0026#34; th:href=\u0026#34;@{/order/details(orderId=${o.id})}\u0026#34;\u0026gt;view\u0026lt;/a\u0026gt; @{……}支持决定路径和相对路径。其中相对路径又支持跨上下文调用url和协议的引用。 当URL为后台传出的参数时，代码如下：\n\u0026lt;img src=\u0026#34;../../static/assets/images/qr-code.jpg\u0026#34; th:src=\u0026#34;@{${path}}\u0026#34; alt=\u0026#34;二维码\u0026#34; /\u0026gt; Thymeleaf常用标签 简单数据转换（数字，日期） \u0026lt;dt\u0026gt;价格\u0026lt;/dt\u0026gt; \u0026lt;dd th:text=\u0026#34;${#numbers.formatDecimal(product.price, 1, 2)}\u0026#34;\u0026gt;180\u0026lt;/dd\u0026gt; \u0026lt;dt\u0026gt;进货日期\u0026lt;/dt\u0026gt; \u0026lt;dd th:text=\u0026#34;${#dates.format(product.availableFrom, \u0026#39;yyyy-MM-dd\u0026#39;)}\u0026#34;\u0026gt;2014-12-01\u0026lt;/dd\u0026gt; 字符串拼接 \u0026lt;dd th:text=\u0026#34;${\u0026#39;$\u0026#39;+product.price}\u0026#34;\u0026gt;235\u0026lt;/dd\u0026gt; 表单 \u0026lt;form th:action=\u0026#34;@{/bb}\u0026#34; th:object=\u0026#34;${user}\u0026#34; method=\u0026#34;post\u0026#34; th:method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; th:field=\u0026#34;*{name}\u0026#34;/\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; th:field=\u0026#34;*{msg}\u0026#34;/\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34;/\u0026gt; \u0026lt;/form\u0026gt; 循环 渲染列表数据是一种非常常见的场景，例如现在有n条记录需要渲染成一个表格\u0026lt;table\u0026gt;，该数据集合必须是可以遍历的，使用th:each标签：\n\u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;NAME\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;PRICE\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;IN STOCK\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr th:each=\u0026#34;prod : ${prods}\u0026#34;\u0026gt; \u0026lt;td th:text=\u0026#34;${prod.name}\u0026#34;\u0026gt;Onions\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${prod.price}\u0026#34;\u0026gt;2.41\u0026lt;/td\u0026gt; \u0026lt;td th:text=\u0026#34;${prod.inStock}? #{true} : #{false}\u0026#34;\u0026gt;yes\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; 条件判断If/Unless Thymeleaf中使用th:if和th:unless属性进行条件判断，下面的例子中，\u0026lt;a\u0026gt;标签只有在th:if中条件成立时才显示：\n\u0026lt;a th:href=\u0026#34;@{/login}\u0026#34; th:unless=${session.user != null}\u0026gt;Login\u0026lt;/a\u0026gt; th:unless于th:if恰好相反，只有表达式中的条件不成立，才会显示其内容。\nSwitch Thymeleaf同样支持多路选择Switch结构，默认属性default可以用*表示：\n\u0026lt;div th:switch=\u0026#34;${user.role}\u0026#34;\u0026gt; \u0026lt;p th:case=\u0026#34;\u0026#39;admin\u0026#39;\u0026#34;\u0026gt;User is an administrator\u0026lt;/p\u0026gt; \u0026lt;p th:case=\u0026#34;#{roles.manager}\u0026#34;\u0026gt;User is a manager\u0026lt;/p\u0026gt; \u0026lt;p th:case=\u0026#34;*\u0026#34;\u0026gt;User is some other thing\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; Thymeleaf配置 可在resources/application.properties 中对Thymeleaf进行配置，配置如下：\n# Enable template caching. spring.thymeleaf.cache=true # Check that the templates location exists. spring.thymeleaf.check-template-location=true # Content-Type value. spring.thymeleaf.content-type=text/html # Enable MVC Thymeleaf view resolution. spring.thymeleaf.enabled=true # Template encoding. spring.thymeleaf.encoding=UTF-8 # Comma-separated list of view names that should be excluded from resolution. spring.thymeleaf.excluded-view-names= # Template mode to be applied to templates. See also StandardTemplateModeHandlers. spring.thymeleaf.mode=HTML5 # Prefix that gets prepended to view names when building a URL. spring.thymeleaf.prefix=classpath:/templates/ # Suffix that gets appended to view names when building a URL. spring.thymeleaf.suffix=.html spring.thymeleaf.template-resolver-order= # Order of the template resolver in the chain. spring.thymeleaf.view-names= # Comma-separated list of view names that can be resolved. ","date":"2016-11-26T15:59:27+08:00","permalink":"https://leibnizhu.github.io/p/Spring-Boot%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%B8%89%E4%BD%BF%E7%94%A8Thymeleaf%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E/","title":"Spring Boot快速入门（三）——使用Thymeleaf模板引擎"},{"content":"使用Spring Boot 编写Demo的REST风格Controller 前言 在DMP项目中，我们创建了demo分支，其中的com.turingdi.dmp.demo包中存放着这个demo。 这个demo实现一个简单的REST风格的API：\n在根页面显示“Hello World” POST请求访问/demo/user可插入一条用户（用name参数指定名字），返回该用户信息的JSON（包含后台生成的用户ID）； GET请求访问/demo/user/可查询到所有用户的信息； GET请求访问/demo/user/\u0026lt;用户ID\u0026gt;可查询到对应用户的信息JSON； 不使用上述HTTP方法访问的会返回错误。 编写Controller代码 package com.turingdi.dmp.demo; import io.swagger.annotations.ApiImplicitParam; import io.swagger.annotations.ApiOperation; import org.springframework.web.bind.annotation.*; import java.util.ArrayList; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.concurrent.atomic.AtomicInteger; import java.util.concurrent.atomic.AtomicLong; /* * Created by leibniz on 16-11-23 */ @RestController @RequestMapping(\u0026#34;/demo\u0026#34;) public class UserController { private final AtomicInteger counter = new AtomicInteger(); private Map\u0026lt;Integer, User\u0026gt; users = new HashMap\u0026lt;\u0026gt;(); @ApiOperation(value=\u0026#34;获取用户详细信息\u0026#34;, notes=\u0026#34;根据url的id来获取用户详细信息\u0026#34;) @ApiImplicitParam(name = \u0026#34;id\u0026#34;, value = \u0026#34;用户ID\u0026#34;, required = true, paramType=\u0026#34;path\u0026#34;, dataType = \u0026#34;Integer\u0026#34;) @RequestMapping(value = \u0026#34;/user/{id}\u0026#34;, method = RequestMethod.GET) public User greeting(@PathVariable int id) { return users.get(id); } @ApiOperation(value=\u0026#34;获取用户列表\u0026#34;, notes=\u0026#34;获取所有用户\u0026#34;) @RequestMapping(value = \u0026#34;/user\u0026#34;, method = RequestMethod.GET) public List\u0026lt;User\u0026gt; getUserList() { return new ArrayList\u0026lt;\u0026gt;(users.values()); } @ApiOperation(value=\u0026#34;创建用户\u0026#34;, notes=\u0026#34;根据用户名创建用户\u0026#34;) @ApiImplicitParam(name = \u0026#34;name\u0026#34;, value = \u0026#34;用户名\u0026#34;, required = true, dataType = \u0026#34;String\u0026#34;, paramType=\u0026#34;body\u0026#34;) @RequestMapping(value = \u0026#34;/user\u0026#34;, method = RequestMethod.POST) public User newUser(@RequestBody String name) { System.out.println(name); User result = new User(counter.incrementAndGet(), name); users.put(result.getId(), result); return result; } @RequestMapping(\u0026#34;\u0026#34;) public String home() { return \u0026#34;Hello World!\u0026#34;; } } 其中@RestController指定在Controller上，这样就不需要在每个@RequestMapping方法上加 @ResponseBody，默认返回json格式。 该Controller逻辑比较简单：\n访问根路径返回\u0026quot;Hello World!\u0026quot;； POST请求访问/user时，生成一个自增的ID，用于生成Greeting对象，保存在一个Map中并返回该对象；Spring Boot回自动将该对象转换成JSON格式再返回； GET请求访问/user/用户ID时，从访问路径获取用户ID，然后从Map中获取对应Greeting对象并返回，同样地Spring Boot会将其转换为JSON格式返回。 注：\n一个典型的REST风格API是这样的：\n可以使用Swagger2快速构建RESTful API文档，并支持在线发送请求调试API：http://blog.didispace.com/springbootswagger2/。 启动项目 直接运行入口类 在IDE中配置、直接运行入口类com.turingdi.dmp.DMPStarter。\n打包jar运行 使用mvn package命令进行打包，并使用以下命令执行：\njava -jar target/***.jar 由于我们的入口类加上了7.@SpringBootApplication注解，并在Maven中配置了spring-boot-maven-plugin插件，因此在打jar包的时候会自动设置jar包中的META-INF/MAINFEST.MF中配置好jar包的入口类，无需我们在pom.xml文件中额外配置。\n使用Spring Boot的Maven插件运行 执行：\nmvn spring-boot:run 即可。这也是spring-boot-maven-plugin插件起到的作用。\n热交换 由于Spring Boot在打包的时候将内置tomcat一并打包，所以我们无法直接更新项目文件让tomcat重新加载。Spring Boot提供了Spring Loaded实现热交换。 在pom.xml中增加以下依赖即可:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-devtools\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; 然后项目运行时，更新源代码之后，重新编译（如IDEA中按快捷键Ctrl+F9）即可在控制台看到Spring Boot重新加载了新编译后的文件。\n","date":"2016-11-26T15:59:05+08:00","permalink":"https://leibnizhu.github.io/p/Spring-Boot%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%BA%8CREST%E9%A3%8E%E6%A0%BCAPI%E7%9A%84Controller%E7%BC%96%E5%86%99%E4%B8%8E%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8/","title":"Spring Boot快速入门（二）——REST风格API的Controller编写与项目启动"},{"content":"因为团队新项目需要，最近研究了一下Spring Boot + RESTful API + Thymeleaf，总结了一份文档，也放到博客里贡献。\nSpring Boot概述 简介 Spring Boot简化了基于Spring的应用开发，你只需要\u0026quot;run\u0026quot;就能创建一个独立的，产品级别的Spring应用。我们为Spring平台及第三方库提供开箱即用的设置，这样你就可以有条不紊地开始。多数Spring Boot应用只需要很少的Spring配置。\n你可以使用Spring Boot创建Java应用，并使用java -jar启动它或采用传统的war部署方式。我们也提供了一个运行\u0026quot;spring脚本\u0026quot;的命令行工具。\n特性 Spring Boot主要的目标是：\n为所有Spring开发提供一个从根本上更快，且随处可得的入门体验。 开箱即用，但通过不采用默认设置可以快速摆脱这种方式。 提供一系列大型项目常用的非功能性特征，比如：内嵌服务器，安全，指标，健康检测，外部化配置。 绝对没有代码生成，也不需要XML配置。 文档/API 目前Spring Boot最新的Release版本为1.4.2。\n官方文档：\nhttp://docs.spring.io/spring-boot/docs/1.4.2.RELEASE/reference/htmlsingle/\n民间翻译的中文文档：\nhttp://udn.yyuap.com/doc/Spring-Boot-Reference-Guide/I.%20Spring%20Boot%20Documentation/index.html\n官方API：\nhttp://docs.spring.io/spring-boot/docs/1.4.2.RELEASE/api/\n使用Spring Boot 本文的Demo可从git@turing:DMPWeb.git的demo分支（未公开于互联网）下载。\n创建Spring Boot项目 IntelliJ IDEA 点击菜单File \u0026ndash;\u0026gt; New \u0026ndash;\u0026gt; Project； 弹出的窗口中，左边栏选择Spring Initializr，右边选择JDK版本（通过配置也可以运行在JDK6上，但建议JDK7以上，最好JDK8）和初始化服务的URL（按默认的https://start.spring.io 即可），点击Next；\n配置项目的基本信息，与平常Maven配置差不多，点击Next：\n选择项目需要的依赖，Web项目可选择Web下的Web（会带上内置Tomcat和Spring MVC的依赖），不选也可以，后期手动修改pom.xml增加依赖是等效的：\n点击Next，等待IDEA下载并初始化项目。 注：其实IDEA这种创建Spring Boot的方法，相当于自己去访问https://start.spring.io ，按页面提示选择了版本、项目信息和插件依赖之后，下载zip，解压导入到IDEA项目中。只不过IDEA帮你完成了下载和解压导入项目这一步而已。\n通用的项目创建方法 任意一个IDE（在此省略截图），选择新建Maven项目，普通项目即可，填写项目基本信息后，修改pom.xml文件：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.turingdi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dmpweb\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;DMPWeb\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Data Manage Platform\u0026lt;/description\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4.2.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;!-- 核心模块，包括自动配置支持、日志和YAML等--\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 测试模块，包括JUnit、Hamcrest、Mockito等--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Web模块，包括Spring MVC、内置Tomcat等--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\t\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; .\t\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 然后compile等待下载依赖即可。 为提高Maven依赖下载速度，请配置本地Nexus镜像：\n\u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;public\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;public\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://172.16.99.235:8081/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;updatePolicy\u0026gt;always\u0026lt;/updatePolicy\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; 编写项目入口类 入口类的位置 Spring Boot官方文档建议将入口类（包含main方法的类）放在项目根包下，如com.turingdi.dmp.DMPStarter，下图是一个典型的Spring Boot项目结构，注意DMPStarter类在项目中的位置：\n这样做的好处是使用@ComponentScan注解时可以使用默认value、无需额外指定扫描根包名的参数；而Spring Boot还提供了一个注解，包含了无参数的@ComponentScan注解及几个常用的注解，亦即在这样的项目结构下，入口类只需要一个注解即可。\n写入口类代码 package com.turingdi.dmp; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.*; import org.springframework.context.annotation.*; @SpringBootApplication public class DMPStarter { public static void main(String[] args) { SpringApplication.run(DMPStarter.class, args); } } 其中@SpringBootApplication注解等价于以默认属性使用@Configuration，@EnableAutoConfiguration和@ComponentScan，作用分别为①@Configuration：当前类可以使用 Spring IoC 容器作为 bean 定义的来源；②@EnableAutoConfiguration：根据项目依赖的jar包自动配置；③@ComponentScan：注解自动收集所有Spring组件。\nmain()方法中调用SpringApplication的run()静态方法来启动Spring Boot，具体的工作为：\n根据classspath创建合适的ApplicationContext； 注册CommandLinePropertySource生成命令行参数； 刷新application context，载入所有bean； 运行CommandLineRunner bean。 参数中选择启动的带@SpringBootApplication注解的类，一般按上面的常规写法就可以，暂时无需深入理解。\n","date":"2016-11-26T15:58:33+08:00","permalink":"https://leibnizhu.github.io/p/Spring-Boot%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E4%B8%80%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%9F%BA%E7%A1%80Demo/","title":"Spring Boot快速入门（一）——介绍与基础Demo"},{"content":"这篇也是组内分享的文档，整理了之前两篇Netty+Redis的文章，加入了一些Redis调优相关的命令和内容。\nRedis性能瓶颈 TCP连接 Redis协议基于TCP/IP协议，受限于TCP连接建立的速度（三次握手等），及网络中数据传输的速度。\n数据包大小 Redis官方的一项测试显示，对于1k~10k以下的数据，Redis的吞吐量变化并不明显，吞吐量曲线在1k~10k左右出现拐点，如下图。\n单线程 Redis服务器为C语言编写，使用异步非阻塞IO，目前坚持使用单线程（可能出于线程锁的效率考虑）。对于高并发访问+多核CPU场景而言，并不能充分使用CPU资源，可能发生某核心占用率很高，其他核心空闲，但Redis请求阻塞在队列中的情况。\n搭建Redis集群可以解决该问题，但集群节点间访问引起的网络IO延时又带来新的问题。\nRedis性能监控/测试 info命令 redis-cli中输入info可以显示当前Redis服务器的全部状态信息。这些信息按照内容被分成了很多部分，可以用额外的参数来单独获取，如下：\n参数名 说明 server 获取 server 信息，包括 version, OS, port 等信息 clients 获取 clients 信息，如客户端连接数等 memory 获取 server 的内存信息，包括当前内存消耗、内存使用峰值 persistence 获取 server 的持久化配置信息 stats 获取 server 的一些基本统计信息，如处理过的连接数量等 replication 获取 server 的主从配置信息 cpu 获取 server 的 CPU 使用信息 keyspace 获取 server 中各个 DB 的 key 的数量 cluster 获取集群节点信息，仅在开启集群后可见 commandstats 获取每种命令的统计信息，非常有用 slowlog命令 redis.conf中配置：\nslowlog-log-slower-than 10000 slowlog-max-len 128 意为：如果一条命令的响应时间超过了 10000us (即 10ms) ，那么将会作为 \u0026ldquo;slow command\u0026rdquo; 被记录，并且将只保留最新的128条记录。\n在redis-cli中使用slowlog get N可以显示最新产生的N条慢操作：\n每条语句有四个描述字段，分别表示慢日志序号（最新的记录被展示在最前面）、这条记录被记录时的时间戳、这条命令的响应时间（单位：us 微秒）、这条命令的内容。\n可以根据slowlog的记录优化对应的语句。\nbigkeys命令 使用方法：\nredis-cli -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; --bigkeys 这条命令会从指定的 Redis DB 中持续采样，实时输出当时得到的 value 占用空间最大的 key 值，并在最后给出各种数据结构的 biggest key 的总结报告，如下图：\nlatency命令 使用方法：\nredis-cli -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; --latency-history redis-cli -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; --latency 区别仅在于：前者每隔15秒生成一条记录（这15秒内的测试结果），后者持续更新测试结果，如下图：\nredis-benchmark测试 使用方法：\nredis-benchmark -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; -c \u0026lt;并发数\u0026gt; -n \u0026lt;请求次数\u0026gt; 执行后，redis-benchmark会对各个命令分别进行测试，测试结果较长，在此截取部分如下：\n第三方统计分析工具redis-stat redis-stat采用ruby开发，利用redis-cli info 提供的原始数据，给用户提供基于文本列表或web图表方式展现的各种关键数据。 redis-stat 开源网址: https://github.com/junegunn/redis-stat\nRedis性能调优 使用Pipeline 对于Redis读写，有很大一部分的耗时是在网络IO上，尤其是Redis(集群)与应用不在一台服务器上时；此时，对于一些连续的操作，尽量使用pipeline批处理。若批量的命令使用到的key要求在执行过程中不被其他请求修改，则需要用redis事务，效率还是比pipeline低。\nJedis jedis = RedisUtils.getSingleJedis(false);//获取Jedis连接 Pipeline pl = jedis.pipelined();//获取Pipeline Response\u0026lt;String\u0026gt; resp1 = pl.get(“key1”);//Pipeline压入命令并保存Response引用 Response\u0026lt;String\u0026gt; resp2 = pl.get(“key2”); pl.sync();//Pipeline执行批处理 System.out.println(“key1’s value = ” + resp1.get());//从Response获取执行结果 System.out.println(“key2’s value = ” + resp2.get()); RedisUtils.close(pl);//关闭Pipeline RedisUtils.close(jedis);//关闭Jedis连接 要注意的是Pipeline一次传输的key或数据也不宜过多，参考本文1.2小节。\n使用Lua脚本 灵活利用Lua脚本，可减少Redis的网络IO。Redis支持在服务器上运行Lua脚本完成一些简单运算。Redis尽管对Lua脚本有很多限制，但的确能提高效率，对于一些Redis原生API不能满足的批量操作，比如读取多个key再进行简单计算，如果将这些key的值分别读取到本地，再进行计算，会发生多次网络IO，那么可以用上面的pipeline，而效率更高的方法是将这些计算写成Lua脚本。\n我们的RTB目前使用Lua脚本的流程如下：\n配置一个监听Servlet上下文初始化的Listener（com.turingdi.rtb.service. PropertiesLoadListener），执行读取配置文件、Redis连接等初始化操作； /该Listener初始化Redis时，将指定的多个Lua脚本文件读入内存（com.turingdi.rtb.utils.RedisUtils的loadScripts()）； 使用Redis的SCRIPTLOAD命令，将Lua脚本加载到Redis服务器，返回一个SHA码，保存到RedisUtils类中； 竞价过程中需要调用Lua脚本时，调用Redis的EVALSHA命令，使用初始化时拿到的SHA进行Lua脚本调用，返回计算结果。 --计算QPS，QPS这个key只保留1s，不存在的时候设置为1并设置生命周期为1，存在的时候直接加1 local isExist = redis.call(\u0026#39;EXISTS\u0026#39;, \u0026#39;QPS\u0026#39;) if isExist == 0 then redis.call(\u0026#39;INCR\u0026#39;, \u0026#39;QPS\u0026#39;) redis.call(\u0026#39;EXPIRE\u0026#39;, \u0026#39;QPS\u0026#39;, \u0026#39;1\u0026#39;) else redis.call(\u0026#39;INCR\u0026#39;, \u0026#39;QPS\u0026#39;) end --处理请求数和响应数的统计 redis.call(\u0026#39;INCR\u0026#39;, KEYS[1]) if ARGV[1] == \u0026#39;1\u0026#39; then redis.call(\u0026#39;INCR\u0026#39;, KEYS[2]) end 本文不对Lua脚本进行详细阐述，有需要的可以参照以下网页/文档：\nhttp://redisdoc.com/script/index.html https://www.oschina.net/translate/intro-to-lua-for-redis-programmers http://origin.redisbook.com/feature/scripting.html http://wiki.jikexueyuan.com/project/redis/lua.html 使用本地的Redis Redis尽量放在本地，减少网络IO时间；对相应时间要求高的，尽量不要用云服务商提供的Redis服务，读写速度比不上本地的。\n主从复制/读写分离 Redis放在本地，在服务器集群环境下就有数据同步的问题。之前尝试过很多方案，Redis自己的Ruby集群、Twitter的Twemproxy等等，都不适合RTB使用——这些集群更多地考虑可用性和数据分片、扩容性，但对一些多键操作支持很差，而且也有各种缺陷（如使用Redis自带的Ruby集群，至少3主3从，可以建好3主3从的集群之后，手动移动Slot到同一台主机，删除其他主机，变成1主3从，但这个集群一旦关闭就无法启动）。\n考虑到RTB使用的Redis读多写少，所以最后使用的方案是Redis自带的主从复制，集群的不同的服务器之间只需要一台主机作为Redis主机，其他服务器的Redis服务设置slaveof属性，作为其从机。此外，可以将从机的只读属性设为no，但往Slave写入的数据会在下一次同步的时候被Master的数据所覆盖——这样做的目的在于写入一些临时缓存变量。\nredis.conf配置如下：\nslaveof \u0026lt;Master IP\u0026gt; \u0026lt;Master端口\u0026gt; slave-read-only no 只有一台服务器的情况下，如果是多核服务器（16核及以上），由于Redis是单线程的，只能利用一个CPU内核，只开一个Redis服务实例可能压力很大（可以从CPU占用看出来），此时也可以使用上面提到的主从复制功能，在同一台服务器上开启多个Redis实例分担查询压力，提高并发性能。\nLinux系统中，可以使用：\ntaskset -cp [CPU核心号码，从0开始] [要执行的命令] 来指定要执行的命令在哪些CPU内核上运行，在多核服务器上，可以合理利用此命令来分配CPU资源，如指定多个Redis和Netty分别运行在多个内核上，并指定哪个Netty服务使用哪个Redis服务（需要自己编写Netty服务，读取配置文件，使用不同端口的Redis服务），避免资源浪费和拥挤。\n目前RTB在一台服务器上部署了一个Master节点（端口6660）和5个Slave节点（端口6661-6665），即只有一个对外可写入的Redis服务，其他Redis服务只能读，保证了读的性能。启动的脚本如下：\n#!/bin/bash kill -9 $(ps -ef | grep redis-server | grep -v grep | awk \u0026#39;{print $2}\u0026#39;) cd /usr/local/redis/6660 taskset -c 0 redis-server redis.conf cd /usr/local/redis/6661 taskset -c 1 redis-server redis.conf ………… cd /usr/local/redis/6665 taskset -c 5 redis-server redis.conf 计算缓存 Redis指令的优化及自定义计算缓存。利用SLOWLOG我们可以找到执行比较慢的命令，从而进行优化。\n比如RTB系统在测试一段时间之后，通过SLOWLOG命令得知耗时较长的都是用户人群标签的并集操作，而这个操作与请求的具体内容有关。所以后来设定了一个计算缓存，通过EXPIRE命令设置缓存的生命周期（随着时间推移，人群标签的计算结果是不一样的，需要定时更新），每次新的请求在计算这一步时，先查询缓存中是否存在计算结果，存在的话直接读取，不存在（全新的计算或旧的已过期）则重新计算并放入运算缓存。（详见com.turingdi.rtb.service.CampaignService）\n压缩key和value 在数据量大的情况下，压缩key和value的长度不管对存储还是网络传输都有利。\n","date":"2016-11-26T14:01:02+08:00","permalink":"https://leibnizhu.github.io/p/Redis%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98Pipeline%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6Lua%E8%84%9A%E6%9C%AC%E7%AD%89/","title":"Redis性能调优——Pipeline、主从复制、Lua脚本等"},{"content":"最近总结了一些技术文档，原本用于组内分享的，发到博客里备忘。\nNginx概述 简介 Nginx是一个自由、开源、高性能及轻量级的HTTP服务器及反转代理服务器，以其高性能、稳定、功能丰富、配置简单及占用系统资源少而著称。\nNginx 超越 Apache 的高性能和稳定性，使得国内使用 Nginx 作为 Web 服务器的网站也越来越多。\n基础功能 处理静态文件，索引文件以及自动索引； 反向代理加速(无缓存)，简单的负载均衡和容错； FastCGI，简单的负载均衡和容错； 模块化的结构。过滤器包括gzipping, byte ranges, chunked responses, 以及 SSI-filter 。在SSI过滤器中，到同一个proxy或FastCGI的多个子请求并发处理； SSL 和 TLS SNI 支持。 优势 Nginx专为性能优化而开发，性能是其最重要的考量, 实现上非常注重效率 。它支持内核Poll模型，能经受高负载的考验, 有报告表明能支持高达50,000个并发连接数。\nNginx作为负载均衡服务器: Nginx 既可以在内部直接支持 Rails 和 PHP 程序对外进行服务, 也可以支持作为 HTTP代理服务器对外进行服务。\nNginx具有很高的稳定性。其它HTTP服务器，当遇到访问的峰值，或者有人恶意发起慢速连接时，也很可能会导致服务器物理内存耗尽频繁交换，失去响应，只能重启服务器。例如当前apache一旦上到200个以上进程，web响应速度就明显非常缓慢了。而Nginx采取了分阶段资源分配技术，使得它的CPU与内存占用率非常低。\nnginx官方表示保持10,000个没有活动的连接，它只占2.5M内存，就稳定性而言, nginx比lighthttpd更胜一筹。\nNginx支持热部署。它的启动特别容易, 并且几乎可以做到7*24不间断运行，即使运行数个月也不需要重新启动。你还能够在不间断服务的情况下，对软件版本进行进行升级。\nNginx采用C进行编写, 不论是系统资源开销还是CPU使用效率都比 Perlbal 要好很多。\nNginx安装 下载 到官网下载最新的稳定版： nginx: download\n环境准备 准备gcc等编译环境：\nsudo apt-get install libpcre3 libpcre3-dev openssl libssl-dev make build-essential gcc 编译安装 将下载到的.tar.gz包解压，进入解压后的目录，输入以下命令进行编译：\n./configure make sudo make install 即可安装到/usr/local/nginx中。\n如果需要SSL，可安装OpenSSL，有些Linux发行版自带OpenSSL无需额外安装，需要安装的到OpenSSL官网下载.tar.gz包解压编译即可。\n启动\u0026amp;关闭 启动 执行\nsudo /usr/local/nginx/sbin/nginx 启动Nginx。\n一般make install后会安装到PATH中，可以直接执行sudo nginx。 执行nginx -v显示Nginx版本。\n重新加载配置文件 sudo nginx -s reload 关闭 Nginx没有提供关闭的方法，只能通过ps找到进程ID后，用kill命令关闭。\n如 强制退出 sudo kill-9 [PID]，或发送其他退出的指令如TERM。\n注：nginx包含worker和master两个进程，强制关闭时两者均需关闭： Nginx配置 下面以DSP业务平台的Nginx配置为例进行讲解：\n#user nobody; worker_processes 1; #根据CPU核数设定worker工作的CPU核心mask #worker_cpu_affinity 1000; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #pid logs/nginx.pid; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; log_format mylog \u0026#39;$remote_addr$time_local$request$http_referer$status$body_bytes_sent$request_time\u0026#39;; access_log logs/access.log mylog buffer=32k flush=5s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 1000; #gzip on; server { listen 80; server_name iad.turing.asia; location / { #设置真实ip proxy_set_header real_ip $remote_addr; proxy_pass http://dsp:8080; } } server { listen 80; server_name tag.turing.asia; location / { #设置真实ip proxy_set_header real_ip $remote_addr; proxy_pass http://tag:8080; } } server { listen 80; server_name cm.turing.asia; location / { #设置真实ip proxy_set_header real_ip $remote_addr; ## proxy_redirect off; proxy_pass http://cm:8080; } } } 基本配置 user 语法: user user [group] 缺省值: nobody nobody\n指定Nginx Worker进程运行用户，默认是nobody帐号。 error_log 语法: error_log file [ debug | info | notice | warn | error | crit ] 缺省值: ${prefix}/logs/error.log\n制定错误日志的存放位置和级别。 worker_processes 语法: worker_processes number 缺省值: 1\n指定工作进程数。nginx可以使用多个worker进程。 worker_cpu_affinity 语法: 3.1.4.worker_cpu_affinity cpumask\n执行Nginx在哪个/些CPU上工作，默认由系统接管（可以在所有核心上运行）。\n如，有8个CPU线程（包括超线程），指定使用第0个CPU则配置为00000001，指定使用第7和第1个CPU线程则配置为10000010。 Event模块 Nginx的默认配置中包含一个默认的Event模块，其中只包含worker_connections配置。\nworker_connections 语法：worker_connections number\n每个worker的最大连接数。通过worker_connections和worker_proceses可以计算出maxclients： max_clients = worker_processes * worker_connections。作为反向代理，max_clients为： max_clients = worker_processes * worker_connections/4，因为浏览器访问时会通过连接池建立多个连接。 HTTP模块 Nginx最常用的是HTTP模块。\n基本配置 HTTP中大部分基本配置无需修改，除了：\nkeepalive_timeout 1000;配置Keep-Alive的超时，单位为秒； gzip on;配置是否开启Gzip压缩，开启后可以提高网站访问速度； log_format 配置日志格式，access_log配置日志路径和使用的格式，在此不细述。 server作用域 HTTP模块中最常配置的是server作用域。\nserver { listen 80; server_name cm.turing.asia; location / { #设置真实ip proxy_set_header real_ip $remote_addr; proxy_pass http://cm:8080; } } listen 语法: listen address:port 默认值： listen 80 作用域: server 指定当前虚拟机的监听端口。 server_name 当前server匹配的用户请求路径的主机名，主要用于配置基于名称的虚拟主机，支持通配符和正则表达式（以波浪线~起头）。\nserver_name指令在接到请求后的匹配顺序分别为：准确的server_name匹配，以通配符开始的字符串，以通配符结束的字符串、匹配正则表达式。nginx按以上顺序依次匹配，只要有一项匹配以后就会停止搜索。\n举例： 先后配置了两个server， server_name分别配置为cm.turing.asia和.turing.asia，当用户请求cm.turing.asia时，访问第一个server，用户访问test.turing.asia时访问第二个server。\nlocation 语法：location 匹配字符串 { 操作语句 } 一个server中可以有多个location，用于匹配请求路径。\n匹配的优先级与server_name相同。\nlocation的操作语句中：\n可以用proxy_set_header设定/增加HTTP请求头。上面的例子中，用proxy_set_header real_ip $remote_addr;将远程主机的真实IP加到HTTP请求头的real_ip字段中，因为收到Nginx转发的服务器读取到的远程主机IP为Nginx的IP，丢失了真实的用户IP信息，所以在此加上。\nlocation中还可以使用if语句结合正则表达式进行请求的判断，比如根据请求中的Refer和请求资源类型，使用valid_referers禁用外站Refer的图片资源请求，以实现防盗链，在此不细述。\n更常用的是proxy_pass 作反向代理，将请求转发到指定的服务器。\n在上面的例子中，因为同一台Tomcat服务器中有多个项目，而经过Nginx转发的请求，会将请求的主机替代为proxy_pass中配置的主机，所以如果proxy_pass中填写的是Tomcat服务器的内网地址，则Tomcat无法分辨用户原本想访问的域名，亦即难以在server.xml中配置不同的域名绑定不同项目。\n因此，我们在Nginx的服务器中配置hosts，将多个自定义的主机名绑定到Tomcat服务器的IP中；同时，在Nginx的配置中，location的proxy_pass不再配置为Tomcat服务器的IP而是hosts中我们设定的server_name对应的主机名。最后Tomcat的server.xml也相应地配置Host节点的name属性：\n\u0026lt;!--Tomcat服务器的server.xml 片段：--\u0026gt; \u0026lt;Host name=\u0026#34;cm\u0026#34; appBase=\u0026#34;cmserv\u0026#34; unpackWARs=\u0026#34;true\u0026#34; autoDeploy=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Context path=\u0026#34;turingcm\u0026#34; docBase=\u0026#34;turingcm\u0026#34; reloadable=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/Host\u0026gt; 负载均衡配置 首先在HTTP模块中定义多台服务器构成的负载均衡：\nupstream rtb{ server 127.0.0.1:8080 weight=5; server 127.0.0.1:8081 weight=5; ………… server 127.0.0.1:8085 backup; } 每台服务器为一行，填写IP和端口，并在weight中填入负载均衡的权重（正整数，默认为1），注意分号。以上例子将该负载均衡配置命名为”rtb”。IP和端口后面填backup的表示备用服务器，当其他非backup的服务器均忙或宕机时，会分发请求到backup服务器上。\n然后在location中，配置proxy_pass http://[负载均衡名]; 即可，即该server和location定义的域名规则和访问路径规则匹配到的请求将按weight分发到以上几台服务器/服务中。\n动静分离 tomcat是一个比较全面的web容器，对静态网页的处理，应该是比较费资源的，特别是每次都要从磁盘读取静态页面，然后返回。这中间会消耗Tomcat的资源，可能会使那些动态页面解析性能影响。\n//静态资源 location ~ .*\\.(js|css|htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$ { //静态资源到nginx服务器下static获取 root static; expires 30d; //设置缓存期限 } //动态资源 location ~ .*$ { //动态请求转发到tomcat服务器 proxy_pass http://127.0.0.1:8080; } 动静分离的原理：Nginx配置中，通过location的配置，结合正则表达式，根据请求路径判断当前请求是否静态资源，如果是，则通过** root static;** 配置直接读取/usr/local/nginx/static下面对应相对路径的资源。如果是jsp等动态资源，则转发到Tomcat服务器。\n注意，可以使用** root [绝对路径];**， 不一定要将静态资源放在Nginx目录下。 这里将静态资源的location放在前面优先匹配（同样是正则匹配的情况下）。\n在此基础上，也可以通过不同的正则对不同的静态资源类型配置不同的缓存期限，如创意等图片可能会变动，缓存期限设小一点，而js/css等静态资源基本不变，缓存期限可以设长一点。\n","date":"2016-11-26T12:17:21+08:00","permalink":"https://leibnizhu.github.io/p/Nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E5%8F%8A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%8A%A8%E9%9D%99%E5%88%86%E7%A6%BB/","title":"Nginx常用配置及负载均衡、动静分离"},{"content":"上一篇《Netty+Redis开发高并发应用的一些思考(一)》提及到Redis的优化，最近一个月的开发调测和部署又有了一些新的想法：\nRedis尽量放在本地，减少网络IO时间；对相应时间要求高的，尽量不要用云服务商提供的Redis服务，别人的服务再好，读写速度也比不上本地的。\nRedis放在本地，在服务器集群环境下就有数据同步的问题。之前尝试过很多方案，Redis自己的Ruby集群、Twitter的Twemproxy等等，都不适合我们的应用场景——这些集群更多地考虑可用性和数据分片、扩容性，但对一些多键操作支持很差，而且也有各种缺陷（如使用Redis自带的Ruby集群，至少3主3从，可以建好3主3从的集群之后，手动移动Slot到同一台主机，删除其他主机，变成1主3从，但这个集群一旦关闭就很难启动起来。）。\n寻寻觅觅，最后发现Redis自带的原始主从复制最适合我们，集群的不同的服务器之间只需要一台主机作为Redis主机，其他服务器的Redis服务设置slaveof属性，作为其从机。 此外，可以将从机的只读属性设为no，但往Slave写入的数据会在下一次同步的时候被Master的数据所覆盖——这样做的目的在于写入一些临时缓存变量。\n只有一台服务器的情况下，如果是多核服务器（16核及以上），由于Redis是单线程的，只能利用一个CPU内核，只开一个Redis服务实例可能压力很大（可以从CPU占用看出来），此时也可以使用上面提到的主从复制功能，在同一台服务器上开启多个Redis实例分担查询压力，提高并发性能。\nLinux系统中，可以使用\ntaskset -cp [CPU核心号码，从0开始] [要执行的命令] 来指定要执行的命令在哪些CPU内核上运行，在多核服务器上，可以合理利用此命令来分配CPU资源，如指定多个Redis和Netty分别运行在多个内核上，并指定哪个Netty服务使用哪个Redis服务（需要自己编写Netty服务，读取配置文件，使用不同端口的Redis服务），避免资源浪费和拥挤。 Redis指令的优化及自定义计算缓存。在Redis官网文档和很多微博都对SLOWLOG命令有介绍，在此不细述细节。利用SLOWLOG我们可以找到执行比较慢的命令，从而进行优化。比如我们的系统在测试一段时间之后，通过SLOWLOG命令得知耗时较长的都是某一步并集操作，而这个操作与请求的具体内容有关，所以后来我为之设了一个缓存，通过EXPIRE命令设置缓存的生命周期（随着时间推移这个计算的结果是不一样的，需要更新），每次新的请求在计算这一步时，先查询缓存中是否存在计算结果，存在的话直接读取，不存在（全新的计算或旧的已过期）则重新计算并放入运算缓存。 ","date":"2016-09-11T15:48:06+08:00","permalink":"https://leibnizhu.github.io/p/Netty-Redis%E5%BC%80%E5%8F%91%E9%AB%98%E5%B9%B6%E5%8F%91%E5%BA%94%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%E4%BA%8C/","title":"Netty+Redis开发高并发应用的一些思考(二)"},{"content":"工作电脑是Linux系统，没有什么顺手的Email客户端（ThunderBird会反复接收同样的邮件、最后收件箱里一大堆重复的邮件真不想再吐槽了），而公司邮箱是疼讯企业邮箱，WebUI不忍直视，所以选择了Gmail的POP3代收，Chrome长期开着web版，打开消息通知。\n问题就来了，Gmail会根据POP3代收邮箱的来件频率动态调整其收件频率，这也是可以理解的，因为POP3代收没法推送只能主动收件，如果可以随便设收件频率，那大家都设成1分钟一收，那Gmail服务器的负载也太大了。虽然这样的设定合理，而我的邮箱收件频率也不算高，最终大概是半小时收一次，有时候甚至是40-50min，这样突发的紧急邮件就很容易错过。\nGoogle了一下，解决方案有三：\n订阅大量邮件，刺激Gmail提高收件频率；同时在Gmail里设置过滤器将这些订阅的邮件删除。但是要定期去被代收的邮箱里去清理，也要小心被代收的邮箱容量问题。 与解决方案1类似，自己写脚本定时给被代收邮箱发邮件，但容易被被代收邮箱拉黑，其余缺点也一样有。 Chrome安装Gmail POP3 Checker插件（需要先安装TemperMonkey）,可以设置POP3代收频率。 前两种方法显然太烂，选择第3种。安装插件后发现免费版不能自由设定收件频率，只能是默认的12分钟，虽然比Gmail自己的快了不少，但还是不满足。而捐献的话至少5刀，太贵了，1刀的话我就给了。于是想办法自己弄吧。\n我的解决方案很简单，在Chrome控制台写入一段JS代码，定期进入POP3代收管理页面，触发手动收件的点击事件，最后再返回到收件箱页面即可，代码如下：\nwindow.setInterval(function(){ window.location.href=\u0026#34;https://mail.google.com/mail/u/0/#settings/accounts\u0026#34;; var exitTime = new Date().getTime() + 5000; //5秒后再执行，以免页面加载慢 while (new Date().getTime() \u0026lt; exitTime) {;} var checkEmails = document.getElementsByClassName(\u0026#39;rP sA\u0026#39;); for(var i =0; i \u0026lt; checkEmails.length; i++){ checkEmails[i].click(); } window.location.href=\u0026#34;https://mail.google.com/mail/u/0/#inbox\u0026#34;; }, 3*60*1000); 其实可以写成Chrome扩展的，我就懒了，毕竟Chrome一直开着，很少要重新开Gmail页面。\n","date":"2016-08-12T15:02:43+08:00","permalink":"https://leibnizhu.github.io/p/%E6%8F%90%E9%AB%98Gmail%E4%BB%A3%E6%94%B6POP3%E9%82%AE%E4%BB%B6%E7%9A%84%E9%A2%91%E7%8E%87/","title":"提高Gmail代收POP3邮件的频率"},{"content":"一个开发中的高并发应用原来部署在tomcat上，但这个应用基于HTTP协议，但并非tomcat所擅长的web服务；在启用了tomcat自带的nio模式后，效率还是不高，所以选择了尝试Netty。\n在缓存方面，一直以来都是使用Redis，为了满足高并发的需求，Redis也需要作一些优化。\n下面就简单总结一下在开发过程中的一些想法：\n对于Redis读写，有很大一部分的耗时是在网络IO上，尤其是Redis(集群)与应用不在一台服务器上时；此时，对于一些连续的操作，尽量使用pipeline批处理，当然前提是这一系列操作对先后顺序没有要求，因为pipeline是将命令打包一起发送，执行顺序可能没有保证的。若批量的命令对执行顺序有要求，建议用redis事务，效率还是比pipeline低很多。\n灵活利用lua脚本，减少Redis的网络IO。Redis尽管对Lua脚本有很多限制，但的确能提高效率，对于一些Redis原生API不能满足的批量操作，比如读取多个key再进行简单计算，如果将这些key的值分别读取到本地，再进行计算，会发生多次网络IO，那么可以用上面的pipeline，而效率更高的方法是将这些计算写成Lua脚本，使用其SHA（可以在应用初始化的时候加载所有用到的Lua脚本，保存SHA，在线计算时直接拿SHA）调用直接返回计算结果。\n对于我们的应用，Netty相比Tomcat更为轻量化，毕竟只是一个NIO框架，省去了不必要的中间层。值得注意的是，协议处理和业务逻辑应该尽量解耦，协议处理由Netty完成，包括TCP拆包粘包处理、HTTP协议处理、业务应用的底层协议处理，都可以编写成Netty的Handler进行处理；但业务逻辑本身的处理不建议放在Handler中，一来逻辑上架构上不清晰，耦合度太高，二来一些耗时长的业务逻辑（往往需要数据库IO）会阻塞Eventloop，阻塞后面的channel。\n对于Netty中的业务逻辑，我的做法是在Handler中将解析出来的请求以及一个DefaultPromise实例封装成对象，压入业务处理的等待队列中，并在Handler中增加Promise的Listener监听器监听业务处理完成的情况，完成则写入响应；\n@Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { if (msg instanceof BidRequest) { //创建一个Promise DefaultPromise\u0026lt;BidResponse\u0026gt; promise = new DefaultPromise\u0026lt;BidResponse\u0026gt;(ctx.executor()) ; //打包成任务对象并加入处理队列 bidQueueStack.offer(new BidMission((BidRequest)msg, promise)); //增加监听器，等任务处理完成之后将BidResponse写入响应 promise.addListener(new PromiseNotifier\u0026lt;BidResponse,DefaultPromise\u0026lt;BidResponse\u0026gt;\u0026gt;(){ @Override public void operationComplete(DefaultPromise\u0026lt;BidResponse\u0026gt; future) throws Exception { if(future.isSuccess()){ ctx.writeAndFlush(future.get()); } ctx.channel().close(); } }); } } 另外使用线程池管理CPU内核数个业务处理线程，从业务等待队列中获取任务对象，进行业务逻辑处理；处理完成之后通过Promise通知任务完成，并放入任务处理结果（响应）：\npublic class BidHandleThread implements Runnable { private LinkedBlockingQueue\u0026lt;BidMission\u0026gt; bidQueueStack; private static final int DEFAULT_RANGE_FOR_SLEEP = 50; // 随机休眠时间 public BidHandleThread(LinkedBlockingQueue\u0026lt;BidMission\u0026gt; bidQueueStack) { super(); this.bidQueueStack = bidQueueStack; } @Override public void run() { try { while (true) { Random r = new Random(); // 从队列弹出数据 BidMission mission = null; if (bidQueueStack.size() \u0026gt; 0) { mission = bidQueueStack.poll(); } else { Thread.sleep(r.nextInt(DEFAULT_RANGE_FOR_SLEEP)); continue; } if (null != mission) { /** * 此处为具体的业务处理过程 */ //通过Promise通知任务完成 mission.getPromise().setSuccess(adxResp); // 打印数据 System.out.println(\u0026#34;队列剩余数据数量：\u0026#34; + bidQueueStack.size()); } } } catch (Exception e) { e.printStackTrace(); } } 至于业务处理的线程池内，线程之间对数据库的访问应该还有进一步优化的空间。之前的一个设想是一个业务线程发起Redis访问的时候，把当前线程休眠，让其他线程进行数据库访问以外的业务处理（计算）；等待Redis响应后才苏醒，参与到其他线程之间对时间片的争夺。这样保证数据库IO是饱和的（应该也是业务逻辑处理中耗时最多的部分）。但还没实现。 或者将所有数据库访问都放在一个任务队列中，也是通过Promise监听-通知的方法，实现数据库的异步访问。 ","date":"2016-07-27T21:40:22+08:00","permalink":"https://leibnizhu.github.io/p/Netty-Redis%E5%BC%80%E5%8F%91%E9%AB%98%E5%B9%B6%E5%8F%91%E5%BA%94%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%E4%B8%80/","title":"Netty+Redis开发高并发应用的一些思考(一)"},{"content":"最近给公司的git服务器从http协议升级为ssh协议的，同时增加了gitolite作为权限管理。但gitolite在增删用户公钥、增删用户对项目的权限、以及新增repository时的操作说不上繁杂但也是蛮机械的；又不想用gitlab之类重型的解决方案，所以选择了编写Shell脚本来实现。\n增加用户公钥的脚本 #!/bin/bash echo \u0026#34;增加/修改 $1 的公钥 $1.pub……\u0026#34; cd /home/git/gitolite-admin echo $2 \u0026gt; keydir/$1.pub git add . git commit -m \u0026#34;增加/修改 $1 的公钥 $1.pub……\u0026#34; git push 删除用户公钥的脚本 #!/bin/bash echo \u0026#34;删除 $1 的公钥 $1.pub……\u0026#34; cd /home/git/gitolite-admin rm keydir/$1.pub git add . git commit -m \u0026#34;删除 $1 的公钥 $1.pub……\u0026#34; git push 增加用户对指定代码仓库的读写权限 #!/bin/bash #增加指定用户对指定代码仓库的读写权限 #第1个参数为用户名，对应公钥pub文件的文件名 #第2个参数为repository名，对应/home/git/repositories下的文件夹名（不包含.git后缀） echo \u0026#34;增加 $1 对项目 $2 的权限……\u0026#34; cd /home/git/gitolite-admin isTarget=0 row=0 #找到要修改的行 cat conf/gitolite.conf | while read line do row=`expr $row + 1` if [[ \u0026#34;$isTarget\u0026#34; == 1 ]]; then echo \u0026#34;$line\u0026#34; if [[ \u0026#34;$line\u0026#34; =~ \u0026#34;$1\u0026#34; ]]; then echo \u0026#34;$1 已经拥有 $2 项目的读写权限\u0026#34; exit 0 else #正式修改 reg=\u0026#34;${row}s/$/\u0026amp;$1 /g\u0026#34; sed -i \u0026#34;${reg}\u0026#34; conf/gitolite.conf break fi isTarget=0 fi if [[ $line =~ \u0026#34;$2\u0026#34; ]]; then echo \u0026#34;${line}\u0026#34; isTarget=1 fi done git add . git commit -m \u0026#34;增加 $1 对项目 $2 的权限……\u0026#34; git push 删除用户对指定代码仓库的读写权限 #!/bin/bash #删除指定用户对指定代码仓库的读写权限 #第1个参数为用户名，对应公钥pub文件的文件名 #第2个参数为repository名，对应/home/git/repositories下的文件夹名（不包含.git后缀） echo \u0026#34;删除 $1 对项目 $2 的权限……\u0026#34; cd /home/git/gitolite-admin isTarget=0 row=0 #找到要修改的行 cat conf/gitolite1.conf | while read line do row=`expr $row + 1` if [[ \u0026#34;$isTarget\u0026#34; == 1 ]]; then echo \u0026#34;$line\u0026#34; if [[ \u0026#34;$line\u0026#34; =~ \u0026#34;$1\u0026#34; ]]; then #删对应用户 reg=\u0026#34;${row}s/$1 //g\u0026#34; sed -i \u0026#34;${reg}\u0026#34; conf/gitolite1.conf break else echo \u0026#34;$1 原来就没有 $2 项目的读写权限\u0026#34; exit 0 fi isTarget=0 fi if [[ $line =~ \u0026#34;$2\u0026#34; ]]; then echo \u0026#34;${line}\u0026#34; isTarget=1 fi done git add . git commit -m \u0026#34;删除 $1 对项目 $2 的权限……\u0026#34; git push 新增一个代码仓库，并初始化其权限 #!/bin/bash #新增一个代码仓库并初始化权限管理文件 #第1个参数为该项目初始分配的用户名，对应公钥pub文件的文件名 #第2个参数为repository名，对应/home/git/repositories下的文件夹名（不包含.git后缀） #第3个参数为项目在权限配置文件中的注释 echo \u0026#34;新增项目 $2 并初始化……\u0026#34; cd /home/git/repositories if [ -d \u0026#34;$2.git\u0026#34; ]; then echo \u0026#34;项目 $2 已存在，请选择另一个项目名……\u0026#34; exit 0 fi echo \u0026#34;no exists\u0026#34; git init --bare \u0026#34;$2.git\u0026#34; #修改权限文件，增加默认管理员权限和初始化的用户权限 echo \u0026#34;## $3\u0026#34; \u0026gt;\u0026gt; /home/git/gitolite-admin/conf/gitolite1.conf echo \u0026#34;repo\t$2\u0026#34; \u0026gt;\u0026gt; /home/git/gitolite-admin/conf/gitolite1.conf echo \u0026#34;\tRW = $1 \u0026#34; \u0026gt;\u0026gt; /home/git/gitolite-admin/conf/gitolite1.conf echo \u0026#34;\tRW+ = @admin\u0026#34; \u0026gt;\u0026gt; /home/git/gitolite-admin/conf/gitolite1.conf git add . git commit -m \u0026#34;增加项目 $2 并初始化，同时增加用户 $1 对其的读写权限……\u0026#34; git push #初始化.gitignore文件 cd /home/git/gittmp git clone git@172.16.99.235:$2.git cd $2 echo \u0026#34;#filter class/binary file\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;/build/\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;/target/\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;/bin/\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;## filter eclipse file\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;*.classpath\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;*.project\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;/.settings/\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;## filter IntelliJ IDEA file\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;*.iml\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;/.idea/\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;#filter temp file\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;*.tmp\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;/~$*\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;.~*\u0026#34; \u0026gt;\u0026gt; .gitignore echo \u0026#34;*.log\u0026#34; \u0026gt;\u0026gt; .gitignore git add . git commit -m \u0026#34;初始化项目的.gitignore文件，忽略常见无用文件\u0026#34; git push echo \u0026#34;项目 $2 创建成功\u0026#34; 重命名项目并修改对应权限配置文件 #!/bin/bash #修改代码仓库的名字，同时更新权限配置文件 #第1个参数为原repository名，对应/home/git/repositories下的文件夹名（不包含.git后缀） #第2个参数为repository名，对应/home/git/repositories下的文件夹名（不包含.git后缀） echo \u0026#34;重命名项目 $1 为 $2……\u0026#34; cd /home/git/repositories if [ ! -d \u0026#34;$1.git\u0026#34; ]; then echo \u0026#34;项目 $1 不存在，请选择另一个项目名……\u0026#34; exit 0 fi #更改目录名 mv \u0026#34;$1.git\u0026#34; \u0026#34;$2.git\u0026#34; #修改权限文件，增加默认管理员权限和初始化的用户权限 sed -i \u0026#34;s/$1$/$2/g\u0026#34; /home/git/gitolite-admin/conf/gitolite.conf echo \u0026#34;新的项目URL为：git@172.30.16.235:$2.git\u0026#34; cd /home/git/gitolite-admin git add . git commit -m \u0026#34;重命名项目 $1 为 $2……\u0026#34; git push ","date":"2016-07-20T20:32:32+08:00","permalink":"https://leibnizhu.github.io/p/Gitolite%E7%AE%A1%E7%90%86%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%9A%84Shell%E8%84%9A%E6%9C%AC/","title":"Gitolite管理用户权限的Shell脚本"},{"content":"本文参照Github上的Run any Desktop Environment in WSL、结合本人实际安装操作而编写。\n安装Bash on Windows 首先安装Bash on Windows，可以参考 如何安装体验 Ubuntu on Windows 一文。按照这篇文章安装的bash可以直接在开始菜单输入“bash”启动，但是不支持GUI程序的，所以有了本文。\n安装VcXsrv并启动XLaunch 在 SourceForge: https://sourceforge.net/projects/vcxsrv/files/latest/download 下载安装程序并安装后，运行XLaunch，按下图配置后，一路点Next直至完成。\n安装 ubuntu-desktop, unity和ccsm并配置 打开bash，输入：\nsudo apt-get install ubuntu-desktop unity compizconfig-settings-manager 再打开ccsm:\nccsm 如果启动ccsm遇到以下的报错（感谢网友 Event 提供的解决方案）：\nAttributeError: \u0026#39;NoneType\u0026#39; object has no attribute \u0026#39;get_default_screen\u0026#39; 解决方法如下：\nexport DISPLAY=:0 dconf reset -f /org/compiz/ unity --reset-icons 按下图勾选需要的模块，然后按Close退出。\n配置~/.bashrc和/etc/dbus-1/session.conf 在bash输入以下命令：\necho \u0026#34;export DISPLAY=:0.0\u0026#34; \u0026gt;\u0026gt; ~/.bashrc sudo sed -i \u0026#39;s$\u0026lt;listen\u0026gt;.*\u0026lt;/listen\u0026gt;$\u0026lt;listen\u0026gt;tcp:host=localhost,port=0\u0026lt;/listen\u0026gt;$\u0026#39; /etc/dbus-1/session.conf 启动Unity桌面 在bash中输入：\ncompiz 等待十几秒之后即可显示Ubuntu桌面。Enjoy it！\n2016-07-19增加：对于XFCE，则是安装xubuntu-desktop或xfce4，与上面同样的步骤，启动的时候输入xfce4-session。 ","date":"2016-07-16T17:09:46+08:00","permalink":"https://leibnizhu.github.io/p/Bash-on-Windows%E5%BC%80%E5%90%AFUbuntu-unity%E5%92%8CXfce4%E6%A1%8C%E9%9D%A2%E7%9A%84%E6%96%B9%E6%B3%95/","title":"Bash on Windows开启Ubuntu unity和Xfce4桌面的方法"},{"content":"測試一下Gitlab+Hexo搭建靜態博客。\n感谢 viosey 提供的Material风格Hexo主题 hexo-theme-material。\n","date":"2016-07-12T22:27:03+08:00","permalink":"https://leibnizhu.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%B8%AC%E8%A9%A6/","title":"第一篇博客——測試"}]